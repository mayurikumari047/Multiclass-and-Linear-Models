{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Multiclass and Linear Models\n",
    "\n",
    "UIC CS 412, Spring 2018\n",
    "\n",
    "_If you have discussed this assignment with anyone, please state their name(s) here: [NAMES]. Keep in mind the expectations set in the Academic Honesty part of the syllabus._\n",
    "\n",
    "There are two parts to this project. The first is on multiclass reductions. The second is on linear models and gradient descent. There is also a third part which gives you an opportunity for extra credit.\n",
    "\n",
    "This assignment is adapted from the github materials for [A Course in Machine Learning](https://github.com/hal3/ciml).\n",
    "\n",
    "## Due Date\n",
    "\n",
    "This assignment is due at 11:59pm Thursday, March 15th. \n",
    "\n",
    "### Files You'll Edit\n",
    "\n",
    "``multiclass.py``: The multiclass classification implementation you need to complete.\n",
    "\n",
    "``gd.py``: The gradient descent file you need to edit.\n",
    "\n",
    "``quizbowl.py``: Multiclass evaluation of the quiz bowl dataset (optional).\n",
    "\n",
    "``predictions.txt``: This file is automatically generated as part of Part 3 (optional).\n",
    "\n",
    "### Files you might want to look at\n",
    "  \n",
    "``binary.py``: Our generic interface for binary classifiers (actually\n",
    "works for regression and other types of classification, too).\n",
    "\n",
    "``datasets.py``: Where a handful of test data sets are stored.\n",
    "\n",
    "``util.py``: A handful of useful utility functions: these will\n",
    "undoubtedly be helpful to you, so take a look!\n",
    "\n",
    "``runClassifier.py``: A few wrappers for doing useful things with\n",
    "classifiers, like training them, generating learning curves, etc.\n",
    "\n",
    "``mlGraphics.py``: A few useful plotting commands\n",
    "\n",
    "``data/*``: All of the datasets we'll use.\n",
    "\n",
    "### What to Submit\n",
    "\n",
    "You will hand in all of the python files listed above as a single zip file **h3.zip** on Gradescope under *Homework 3*.  The programming part constitutes 60% of the grade for this homework. You also need to answer the questions denoted by **WU#** (and a kitten) in this notebook which are the other 40% of your homework grade. When you are done, you should export **hw3.ipynb** with your answers as a PDF file **hw3WrittenPart.pdf**, upload the PDF file to Gradescope under *Homework 3 - Written Part*, and tag each question on Gradescope. \n",
    "\n",
    "Your entire homework will be considered late if any of these parts are submitted late. \n",
    "\n",
    "#### Autograding\n",
    "\n",
    "Your code will be autograded for technical correctness. Please **do\n",
    "not** change the names of any provided functions or classes within the\n",
    "code, or you will wreak havoc on the autograder. We have provided two simple test cases that you can try your code on, see ``run_tests_simple.py``. As usual, you should create more test cases to make sure your code runs correctly.\n",
    "\n",
    "# Part 1: Multiclass Classification *[30% impl, 20% writeup]*\n",
    "\n",
    "In this section, you will explore the differences between three\n",
    "multiclass-to-binary reductions: one-versus-all (OVA), all-versus-all\n",
    "(AVA), and a tree-based reduction (TREE).  The evaluation will be on different datasets from \n",
    "`datasets.py`.\n",
    "\n",
    "The classification task we'll work with is wine classification. The dataset was downloaded from allwines.com. Your job is to predict the type of wine, given the description of the wine. There are two tasks: WineData has 20 different wines, WineDataSmall is just the first five of those (sorted roughly by frequency). You can find the names of the wines both in WineData.labels as well as the file wines.names.\n",
    "\n",
    "To start out, let's import everything and train decision \"stumps\" (aka depth=1 decision trees) on the large data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training classifier for 0 versus rest\n",
      "training classifier for 1 versus rest\n",
      "training classifier for 2 versus rest\n",
      "training classifier for 3 versus rest\n",
      "training classifier for 4 versus rest\n",
      "training classifier for 5 versus rest\n",
      "training classifier for 6 versus rest\n",
      "training classifier for 7 versus rest\n",
      "training classifier for 8 versus rest\n",
      "training classifier for 9 versus rest\n",
      "training classifier for 10 versus rest\n",
      "training classifier for 11 versus rest\n",
      "training classifier for 12 versus rest\n",
      "training classifier for 13 versus rest\n",
      "training classifier for 14 versus rest\n",
      "training classifier for 15 versus rest\n",
      "training classifier for 16 versus rest\n",
      "training classifier for 17 versus rest\n",
      "training classifier for 18 versus rest\n",
      "training classifier for 19 versus rest\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.29499072356215211"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import multiclass\n",
    "import util\n",
    "from datasets import *\n",
    "import importlib\n",
    "importlib.reload(multiclass)\n",
    "\n",
    "h = multiclass.OVA(20, lambda: DecisionTreeClassifier(max_depth=1))\n",
    "h.train(WineData.X, WineData.Y)\n",
    "P = h.predictAll(WineData.Xte)\n",
    "mean(P == WineData.Yte)\n",
    "# 0.29499072356215211"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means 29% accuracy on this task. The most frequent class is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Cabernet-Sauvignon\n"
     ]
    }
   ],
   "source": [
    "print(mode(WineData.Y))\n",
    "# 1\n",
    "print(WineData.labels[1])\n",
    "# Cabernet-Sauvignon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you were to always predict label 1, you would get the following accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17254174397031541"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(WineData.Yte == 1)\n",
    "# 0.17254174397031541"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we're doing a bit (12%) better than that using decision stumps. \n",
    "\n",
    "The default implementation of OVA uses decision tree confidence (probability of prediction) to weigh the votes. You can switch to zero/one predictions to see the effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19109461966604824"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = h.predictAll(WineData.Xte, useZeroOne=True)\n",
    "mean(P == WineData.Yte)\n",
    "# 0.19109461966604824"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this is markedly worse.\n",
    "\n",
    "Switching to the smaller data set for a minute, we can train, say, depth 3 decision trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training classifier for 0 versus rest\n",
      "training classifier for 1 versus rest\n",
      "training classifier for 2 versus rest\n",
      "training classifier for 3 versus rest\n",
      "training classifier for 4 versus rest\n",
      "0.606126914661\n",
      "0.407002188184\n"
     ]
    }
   ],
   "source": [
    "h = multiclass.OVA(5, lambda: DecisionTreeClassifier(max_depth=3))\n",
    "h.train(WineDataSmall.X, WineDataSmall.Y)\n",
    "P = h.predictAll(WineDataSmall.Xte)\n",
    "print(mean(P == WineDataSmall.Yte))\n",
    "# 0.590809628009\n",
    "print(mean(WineDataSmall.Yte == 1))\n",
    "# 0.407002188184"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So using depth 3 trees we get an accuracy of about 60% (this number varies a bit), versus a baseline of 41%. That's not too terrible, but not great.\n",
    "\n",
    "We can look at what this classifier is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sauvignon-Blanc\n",
      "citrus?\n",
      "-N-> lime?\n",
      "|    -N-> gooseberry?\n",
      "|    |    -N-> class 0\t(356.0 for class 0, 10.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 4.0 for class 1)\n",
      "|    -Y-> apple?\n",
      "|    |    -N-> class 1\t(1.0 for class 0, 15.0 for class 1)\n",
      "|    |    -Y-> class 0\t(2.0 for class 0, 0.0 for class 1)\n",
      "-Y-> grapefruit?\n",
      "|    -N-> flavors?\n",
      "|    |    -N-> class 1\t(4.0 for class 0, 12.0 for class 1)\n",
      "|    |    -Y-> class 0\t(11.0 for class 0, 5.0 for class 1)\n",
      "|    -Y-> opens?\n",
      "|    |    -N-> class 1\t(0.0 for class 0, 14.0 for class 1)\n",
      "|    |    -Y-> class 0\t(1.0 for class 0, 0.0 for class 1)\n"
     ]
    }
   ],
   "source": [
    "print(WineDataSmall.labels[0])\n",
    "#'Sauvignon-Blanc'\n",
    "util.showTree(h.f[0], WineDataSmall.words)\n",
    "#citrus?\n",
    "#-N-> lime?\n",
    "#|    -N-> gooseberry?\n",
    "#|    |    -N-> class 0\t(356.0 for class 0, 10.0 for class 1)\n",
    "#|    |    -Y-> class 1\t(0.0 for class 0, 4.0 for class 1)\n",
    "#|    -Y-> apple?\n",
    "#|    |    -N-> class 1\t(1.0 for class 0, 15.0 for class 1)\n",
    "#|    |    -Y-> class 0\t(2.0 for class 0, 0.0 for class 1)\n",
    "#-Y-> grapefruit?\n",
    "#|    -N-> flavors?\n",
    "#|    |    -N-> class 1\t(4.0 for class 0, 12.0 for class 1)\n",
    "#|    |    -Y-> class 0\t(11.0 for class 0, 5.0 for class 1)\n",
    "#|    -Y-> opens?\n",
    "#|    |    -N-> class 1\t(0.0 for class 0, 14.0 for class 1)\n",
    "#|    |    -Y-> class 0\t(1.0 for class 0, 0.0 for class 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should show the tree that's associated with predicting label 0 (which is stored in h.f[0]). The 1s mean \"likely to be Sauvignon-Blanc\" and the 0s mean \"likely not to be\".\n",
    "\n",
    "Now, go in and complete the AVA implementation in `multiclass.py`. You should be able to train an AVA model on the small data set by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training classifier for 1 versus 0\n",
      "training classifier for 2 versus 0\n",
      "training classifier for 2 versus 1\n",
      "training classifier for 3 versus 0\n",
      "training classifier for 3 versus 1\n",
      "training classifier for 3 versus 2\n",
      "training classifier for 4 versus 0\n",
      "training classifier for 4 versus 1\n",
      "training classifier for 4 versus 2\n",
      "training classifier for 4 versus 3\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(multiclass)\n",
    "\n",
    "h = multiclass.AVA(5, lambda: DecisionTreeClassifier(max_depth=3))\n",
    "h.train(WineDataSmall.X, WineDataSmall.Y)\n",
    "P = h.predictAll(WineDataSmall.Xte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you must implement a \n",
    "tree-based reduction in `multiclass.py`. Most of train is given to you, but predict you\n",
    "must do all on your own. There is a tree class to help you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1] [2 [3 4]]]\n",
      "False\n",
      "[2 [3 4]]\n",
      "2\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "t = multiclass.makeBalancedTree(range(5))\n",
    "print(t)\n",
    "# [[0 1]] [2 [3 4]]]\n",
    "print(t.isLeaf)\n",
    "# False\n",
    "print(t.getRight())\n",
    "# [2 [3 4]]\n",
    "print(t.getRight().getLeft())\n",
    "# 2\n",
    "print(t.getRight().getLeft().isLeaf)\n",
    "# True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to train a MCTree model by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training classifier for [0, 1] versus [2, 3, 4]\n",
      "training classifier for [0] versus [1]\n",
      "training classifier for [2] versus [3, 4]\n",
      "training classifier for [3] versus [4]\n",
      "0.562363238512\n",
      "0.407002188184\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(multiclass)\n",
    "h = multiclass.MCTree(t, lambda: DecisionTreeClassifier(max_depth=3))\n",
    "h.train(WineDataSmall.X, WineDataSmall.Y)\n",
    "P = h.predictAll(WineDataSmall.Xte)\n",
    "print(mean(P == WineDataSmall.Yte))\n",
    "print(mean(1 == WineDataSmall.Yte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training classifier for [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] versus [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "training classifier for [0, 1, 2, 3, 4] versus [5, 6, 7, 8, 9]\n",
      "training classifier for [0, 1] versus [2, 3, 4]\n",
      "training classifier for [0] versus [1]\n",
      "training classifier for [2] versus [3, 4]\n",
      "training classifier for [3] versus [4]\n",
      "training classifier for [5, 6] versus [7, 8, 9]\n",
      "training classifier for [5] versus [6]\n",
      "training classifier for [7] versus [8, 9]\n",
      "training classifier for [8] versus [9]\n",
      "training classifier for [10, 11, 12, 13, 14] versus [15, 16, 17, 18, 19]\n",
      "training classifier for [10, 11] versus [12, 13, 14]\n",
      "training classifier for [10] versus [11]\n",
      "training classifier for [12] versus [13, 14]\n",
      "training classifier for [13] versus [14]\n",
      "training classifier for [15, 16] versus [17, 18, 19]\n",
      "training classifier for [15] versus [16]\n",
      "training classifier for [17] versus [18, 19]\n",
      "training classifier for [18] versus [19]\n",
      "0.308905380334\n",
      "0.17254174397\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(multiclass)\n",
    "t = multiclass.makeBalancedTree(range(20))\n",
    "h = multiclass.MCTree(t, lambda: DecisionTreeClassifier(max_depth=3))\n",
    "h.train(WineData.X, WineData.Y)\n",
    "P = h.predictAll(WineData.Xte)\n",
    "print(mean(P == WineData.Yte))\n",
    "print(mean(1 == WineData.Yte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/kitten.jpeg\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br>\n",
    "## WU1 (10%):\n",
    "Answer A, B, C for both OVA and AVA.\n",
    "\n",
    "(A) What words are most indicative of being Sauvignon-Blanc? Which words are most indicative of not being Sauvignon-Blanc? What about Pinot-Noir (label==2)?\n",
    "\n",
    "(B) Train depth 3 decision trees on the full WineData task (with 20 labels). What accuracy do you get? How long does this take (in seconds)? One of my least favorite wines is Viognier -- what words are indicative of this?\n",
    "\n",
    "(C) Compare the accuracy using zero-one predictions versus using confidence. How much difference does it make?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training classifier for 0 versus rest\n",
      "training classifier for 1 versus rest\n",
      "training classifier for 2 versus rest\n",
      "training classifier for 3 versus rest\n",
      "training classifier for 4 versus rest\n",
      "0.599562363239\n",
      "\n",
      "*********** OVA Classifiers' trees ***********\n",
      "\n",
      "Sauvignon-Blanc\n",
      "citrus?\n",
      "-N-> lime?\n",
      "|    -N-> gooseberry?\n",
      "|    |    -N-> class 0\t(356.0 for class 0, 10.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 4.0 for class 1)\n",
      "|    -Y-> hint?\n",
      "|    |    -N-> class 1\t(1.0 for class 0, 15.0 for class 1)\n",
      "|    |    -Y-> class 0\t(2.0 for class 0, 0.0 for class 1)\n",
      "-Y-> grapefruit?\n",
      "|    -N-> flavors?\n",
      "|    |    -N-> class 1\t(4.0 for class 0, 12.0 for class 1)\n",
      "|    |    -Y-> class 0\t(11.0 for class 0, 5.0 for class 1)\n",
      "|    -Y-> stone?\n",
      "|    |    -N-> class 1\t(0.0 for class 0, 14.0 for class 1)\n",
      "|    |    -Y-> class 0\t(1.0 for class 0, 0.0 for class 1)\n",
      "\n",
      "Cabernet-Sauvignon\n",
      "tannins?\n",
      "-N-> blackberry?\n",
      "|    -N-> black?\n",
      "|    |    -N-> class 0\t(173.0 for class 0, 30.0 for class 1)\n",
      "|    |    -Y-> class 1\t(24.0 for class 0, 28.0 for class 1)\n",
      "|    -Y-> strawberry?\n",
      "|    |    -N-> class 1\t(4.0 for class 0, 22.0 for class 1)\n",
      "|    |    -Y-> class 0\t(2.0 for class 0, 0.0 for class 1)\n",
      "-Y-> salmon?\n",
      "|    -N-> raspberries?\n",
      "|    |    -N-> class 1\t(29.0 for class 0, 106.0 for class 1)\n",
      "|    |    -Y-> class 0\t(8.0 for class 0, 1.0 for class 1)\n",
      "|    -Y-> class 0\t(8.0 for class 0, 0.0 for class 1)\n",
      "\n",
      "Pinot-Noir\n",
      "cherry?\n",
      "-N-> raspberries?\n",
      "|    -N-> strawberry?\n",
      "|    |    -N-> class 0\t(225.0 for class 0, 58.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 4.0 for class 1)\n",
      "|    -Y-> nice?\n",
      "|    |    -N-> class 1\t(0.0 for class 0, 12.0 for class 1)\n",
      "|    |    -Y-> class 0\t(1.0 for class 0, 0.0 for class 1)\n",
      "-Y-> cassis?\n",
      "|    -N-> verdot?\n",
      "|    |    -N-> class 1\t(36.0 for class 0, 68.0 for class 1)\n",
      "|    |    -Y-> class 0\t(8.0 for class 0, 0.0 for class 1)\n",
      "|    -Y-> allspice?\n",
      "|    |    -N-> class 0\t(21.0 for class 0, 0.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 2.0 for class 1)\n",
      "\n",
      "Pinot-Gris\n",
      "thai?\n",
      "-N-> quince?\n",
      "|    -N-> honey?\n",
      "|    |    -N-> class 0\t(415.0 for class 0, 6.0 for class 1)\n",
      "|    |    -Y-> class 0\t(3.0 for class 0, 2.0 for class 1)\n",
      "|    -Y-> honey?\n",
      "|    |    -N-> class 1\t(0.0 for class 0, 2.0 for class 1)\n",
      "|    |    -Y-> class 0\t(1.0 for class 0, 0.0 for class 1)\n",
      "-Y-> variety?\n",
      "|    -N-> class 1\t(0.0 for class 0, 5.0 for class 1)\n",
      "|    -Y-> class 0\t(1.0 for class 0, 0.0 for class 1)\n",
      "\n",
      "Pinot-Grigio\n",
      "apple?\n",
      "-N-> paired?\n",
      "|    -N-> friends?\n",
      "|    |    -N-> class 0\t(395.0 for class 0, 13.0 for class 1)\n",
      "|    |    -Y-> class 1\t(1.0 for class 0, 2.0 for class 1)\n",
      "|    -Y-> red?\n",
      "|    |    -N-> class 1\t(0.0 for class 0, 3.0 for class 1)\n",
      "|    |    -Y-> class 0\t(2.0 for class 0, 0.0 for class 1)\n",
      "-Y-> straw?\n",
      "|    -N-> peach?\n",
      "|    |    -N-> class 0\t(7.0 for class 0, 2.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 2.0 for class 1)\n",
      "|    -Y-> gooseberry?\n",
      "|    |    -N-> class 1\t(0.0 for class 0, 7.0 for class 1)\n",
      "|    |    -Y-> class 0\t(1.0 for class 0, 0.0 for class 1)\n",
      "\n",
      "training classifier for 1 versus 0\n",
      "training classifier for 2 versus 0\n",
      "training classifier for 2 versus 1\n",
      "training classifier for 3 versus 0\n",
      "training classifier for 3 versus 1\n",
      "training classifier for 3 versus 2\n",
      "training classifier for 4 versus 0\n",
      "training classifier for 4 versus 1\n",
      "training classifier for 4 versus 2\n",
      "training classifier for 4 versus 3\n",
      "0.352297592998\n",
      "\n",
      "*********** AVA Classifiers' trees ***********\n",
      "\n",
      "thai?\n",
      "-N-> very?\n",
      "|    -N-> touch?\n",
      "|    |    -N-> class 1\t(3.0 for class 0, 28.0 for class 1)\n",
      "|    |    -Y-> class 0\t(2.0 for class 0, 0.0 for class 1)\n",
      "|    -Y-> conditions?\n",
      "|    |    -N-> class 0\t(5.0 for class 0, 0.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "-Y-> class 0\t(5.0 for class 0, 0.0 for class 1)\n",
      "\n",
      "straw?\n",
      "-N-> crisp?\n",
      "|    -N-> example?\n",
      "|    |    -N-> class 0\t(142.0 for class 0, 8.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 2.0 for class 1)\n",
      "|    -Y-> red?\n",
      "|    |    -N-> class 1\t(0.0 for class 0, 7.0 for class 1)\n",
      "|    |    -Y-> class 0\t(2.0 for class 0, 0.0 for class 1)\n",
      "-Y-> class 1\t(0.0 for class 0, 12.0 for class 1)\n",
      "\n",
      "straw?\n",
      "-N-> crisp?\n",
      "|    -N-> goes?\n",
      "|    |    -N-> class 0\t(187.0 for class 0, 8.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 2.0 for class 1)\n",
      "|    -Y-> class 1\t(0.0 for class 0, 7.0 for class 1)\n",
      "-Y-> class 1\t(0.0 for class 0, 12.0 for class 1)\n",
      "\n",
      "apple?\n",
      "-N-> pasta?\n",
      "|    -N-> quite?\n",
      "|    |    -N-> class 0\t(56.0 for class 0, 11.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 3.0 for class 1)\n",
      "|    -Y-> class 1\t(0.0 for class 0, 4.0 for class 1)\n",
      "-Y-> bright?\n",
      "|    -N-> class 1\t(0.0 for class 0, 10.0 for class 1)\n",
      "|    -Y-> particularly?\n",
      "|    |    -N-> class 0\t(4.0 for class 0, 0.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "\n",
      "crisp?\n",
      "-N-> peach?\n",
      "|    -N-> pear?\n",
      "|    |    -N-> class 0\t(142.0 for class 0, 3.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 2.0 for class 1)\n",
      "|    -Y-> class 1\t(0.0 for class 0, 3.0 for class 1)\n",
      "-Y-> red?\n",
      "|    -N-> class 1\t(0.0 for class 0, 7.0 for class 1)\n",
      "|    -Y-> class 0\t(2.0 for class 0, 0.0 for class 1)\n",
      "\n",
      "crisp?\n",
      "-N-> white?\n",
      "|    -N-> pear?\n",
      "|    |    -N-> class 0\t(186.0 for class 0, 2.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 2.0 for class 1)\n",
      "|    -Y-> )?\n",
      "|    |    -N-> class 1\t(0.0 for class 0, 4.0 for class 1)\n",
      "|    |    -Y-> class 0\t(1.0 for class 0, 0.0 for class 1)\n",
      "-Y-> class 1\t(0.0 for class 0, 7.0 for class 1)\n",
      "\n",
      "thai?\n",
      "-N-> very?\n",
      "|    -N-> acid?\n",
      "|    |    -N-> class 0\t(56.0 for class 0, 4.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "|    -Y-> ripe?\n",
      "|    |    -N-> class 0\t(4.0 for class 0, 1.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 4.0 for class 1)\n",
      "-Y-> class 1\t(0.0 for class 0, 5.0 for class 1)\n",
      "\n",
      "cassis?\n",
      "-N-> acidity?\n",
      "|    -N-> salmon?\n",
      "|    |    -N-> class 0\t(129.0 for class 0, 92.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 11.0 for class 1)\n",
      "|    -Y-> tannins?\n",
      "|    |    -N-> class 1\t(0.0 for class 0, 22.0 for class 1)\n",
      "|    |    -Y-> class 1\t(11.0 for class 0, 15.0 for class 1)\n",
      "-Y-> tea?\n",
      "|    -N-> 100?\n",
      "|    |    -N-> class 0\t(47.0 for class 0, 1.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "|    -Y-> class 1\t(0.0 for class 0, 2.0 for class 1)\n",
      "\n",
      "crisp?\n",
      "-N-> lime?\n",
      "|    -N-> lemon?\n",
      "|    |    -N-> class 1\t(9.0 for class 0, 141.0 for class 1)\n",
      "|    |    -Y-> class 0\t(8.0 for class 0, 0.0 for class 1)\n",
      "|    -Y-> grilled?\n",
      "|    |    -N-> class 0\t(13.0 for class 0, 0.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "-Y-> red?\n",
      "|    -N-> class 0\t(30.0 for class 0, 0.0 for class 1)\n",
      "|    -Y-> class 1\t(0.0 for class 0, 2.0 for class 1)\n",
      "\n",
      "citrus?\n",
      "-N-> lime?\n",
      "|    -N-> refreshing?\n",
      "|    |    -N-> class 1\t(9.0 for class 0, 187.0 for class 1)\n",
      "|    |    -Y-> class 0\t(5.0 for class 0, 0.0 for class 1)\n",
      "|    -Y-> class 0\t(15.0 for class 0, 0.0 for class 1)\n",
      "-Y-> class 0\t(31.0 for class 0, 0.0 for class 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#(A) What words are most indicative of being Sauvignon-Blanc? \n",
    "#Which words are most indicative of not being Sauvignon-Blanc? What about Pinot-Noir (label==2)?\n",
    "\n",
    "# ********* OVA Training **********\n",
    "\n",
    "h = multiclass.OVA(5, lambda: DecisionTreeClassifier(max_depth=3))\n",
    "h.train(WineDataSmall.X, WineDataSmall.Y)\n",
    "P = h.predictAll(WineDataSmall.Xte)\n",
    "print(mean(P == WineDataSmall.Yte))\n",
    "print(\"\")\n",
    "print(\"*********** OVA Classifiers' trees ***********\")\n",
    "print(\"\")\n",
    "print(WineDataSmall.labels[0])\n",
    "util.showTree(h.f[0], WineDataSmall.words)\n",
    "print(\"\")\n",
    "\n",
    "print(WineDataSmall.labels[1])\n",
    "util.showTree(h.f[1], WineDataSmall.words)\n",
    "print(\"\")\n",
    "\n",
    "print(WineDataSmall.labels[2])\n",
    "util.showTree(h.f[2], WineDataSmall.words)\n",
    "print(\"\")\n",
    "\n",
    "print(WineDataSmall.labels[3])\n",
    "util.showTree(h.f[3], WineDataSmall.words)\n",
    "print(\"\")\n",
    "\n",
    "print(WineDataSmall.labels[4])\n",
    "util.showTree(h.f[4], WineDataSmall.words)\n",
    "print(\"\")\n",
    "\n",
    "# ********* AVA Training **********\n",
    "\n",
    "h = multiclass.AVA(5, lambda: DecisionTreeClassifier(max_depth=3))\n",
    "h.train(WineDataSmall.X, WineDataSmall.Y)\n",
    "P = h.predictAll(WineDataSmall.Xte)\n",
    "print(mean(P == WineDataSmall.Yte))\n",
    "print(\"\")\n",
    "print(\"*********** AVA Classifiers' trees ***********\")\n",
    "print(\"\")\n",
    "util.showTree(h.f[4][3], WineDataSmall.words)\n",
    "print(\"\")\n",
    "util.showTree(h.f[4][2], WineDataSmall.words)\n",
    "print(\"\")\n",
    "util.showTree(h.f[4][1], WineDataSmall.words)\n",
    "print(\"\")\n",
    "util.showTree(h.f[4][0], WineDataSmall.words)\n",
    "print(\"\")\n",
    "util.showTree(h.f[3][2], WineDataSmall.words)\n",
    "print(\"\")\n",
    "util.showTree(h.f[3][1], WineDataSmall.words)\n",
    "print(\"\")\n",
    "util.showTree(h.f[3][0], WineDataSmall.words)\n",
    "print(\"\")\n",
    "util.showTree(h.f[2][1], WineDataSmall.words)\n",
    "print(\"\")\n",
    "util.showTree(h.f[2][0], WineDataSmall.words)\n",
    "print(\"\")\n",
    "util.showTree(h.f[1][0], WineDataSmall.words)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training depth 3 decision trees on the full WineData task (with 20 labels) for OVA\n",
      "training classifier for 0 versus rest\n",
      "training classifier for 1 versus rest\n",
      "training classifier for 2 versus rest\n",
      "training classifier for 3 versus rest\n",
      "training classifier for 4 versus rest\n",
      "training classifier for 5 versus rest\n",
      "training classifier for 6 versus rest\n",
      "training classifier for 7 versus rest\n",
      "training classifier for 8 versus rest\n",
      "training classifier for 9 versus rest\n",
      "training classifier for 10 versus rest\n",
      "training classifier for 11 versus rest\n",
      "training classifier for 12 versus rest\n",
      "training classifier for 13 versus rest\n",
      "training classifier for 14 versus rest\n",
      "training classifier for 15 versus rest\n",
      "training classifier for 16 versus rest\n",
      "training classifier for 17 versus rest\n",
      "training classifier for 18 versus rest\n",
      "training classifier for 19 versus rest\n",
      "\n",
      "OVA Accuracy on the full WineData task with 20 labels: 0.37012987013\n",
      "Label 1 prediction accuracy: 0.17254174397\n",
      "Execution time for OVA: 2.455502510070801\n",
      "\n",
      "Viognier\n",
      "peaches?\n",
      "-N-> nectarine?\n",
      "|    -N-> chilled?\n",
      "|    |    -N-> class 0\t(1036.0 for class 0, 1.0 for class 1)\n",
      "|    |    -Y-> class 0\t(6.0 for class 0, 1.0 for class 1)\n",
      "|    -Y-> fruits?\n",
      "|    |    -N-> class 0\t(13.0 for class 0, 1.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "-Y-> milk?\n",
      "|    -N-> fully?\n",
      "|    |    -N-> class 0\t(14.0 for class 0, 0.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "|    -Y-> class 1\t(0.0 for class 0, 3.0 for class 1)\n",
      "\n",
      "Training depth 3 decision trees on the full WineData task (with 20 labels) for AVA\n",
      "training classifier for 1 versus 0\n",
      "training classifier for 2 versus 0\n",
      "training classifier for 2 versus 1\n",
      "training classifier for 3 versus 0\n",
      "training classifier for 3 versus 1\n",
      "training classifier for 3 versus 2\n",
      "training classifier for 4 versus 0\n",
      "training classifier for 4 versus 1\n",
      "training classifier for 4 versus 2\n",
      "training classifier for 4 versus 3\n",
      "training classifier for 5 versus 0\n",
      "training classifier for 5 versus 1\n",
      "training classifier for 5 versus 2\n",
      "training classifier for 5 versus 3\n",
      "training classifier for 5 versus 4\n",
      "training classifier for 6 versus 0\n",
      "training classifier for 6 versus 1\n",
      "training classifier for 6 versus 2\n",
      "training classifier for 6 versus 3\n",
      "training classifier for 6 versus 4\n",
      "training classifier for 6 versus 5\n",
      "training classifier for 7 versus 0\n",
      "training classifier for 7 versus 1\n",
      "training classifier for 7 versus 2\n",
      "training classifier for 7 versus 3\n",
      "training classifier for 7 versus 4\n",
      "training classifier for 7 versus 5\n",
      "training classifier for 7 versus 6\n",
      "training classifier for 8 versus 0\n",
      "training classifier for 8 versus 1\n",
      "training classifier for 8 versus 2\n",
      "training classifier for 8 versus 3\n",
      "training classifier for 8 versus 4\n",
      "training classifier for 8 versus 5\n",
      "training classifier for 8 versus 6\n",
      "training classifier for 8 versus 7\n",
      "training classifier for 9 versus 0\n",
      "training classifier for 9 versus 1\n",
      "training classifier for 9 versus 2\n",
      "training classifier for 9 versus 3\n",
      "training classifier for 9 versus 4\n",
      "training classifier for 9 versus 5\n",
      "training classifier for 9 versus 6\n",
      "training classifier for 9 versus 7\n",
      "training classifier for 9 versus 8\n",
      "training classifier for 10 versus 0\n",
      "training classifier for 10 versus 1\n",
      "training classifier for 10 versus 2\n",
      "training classifier for 10 versus 3\n",
      "training classifier for 10 versus 4\n",
      "training classifier for 10 versus 5\n",
      "training classifier for 10 versus 6\n",
      "training classifier for 10 versus 7\n",
      "training classifier for 10 versus 8\n",
      "training classifier for 10 versus 9\n",
      "training classifier for 11 versus 0\n",
      "training classifier for 11 versus 1\n",
      "training classifier for 11 versus 2\n",
      "training classifier for 11 versus 3\n",
      "training classifier for 11 versus 4\n",
      "training classifier for 11 versus 5\n",
      "training classifier for 11 versus 6\n",
      "training classifier for 11 versus 7\n",
      "training classifier for 11 versus 8\n",
      "training classifier for 11 versus 9\n",
      "training classifier for 11 versus 10\n",
      "training classifier for 12 versus 0\n",
      "training classifier for 12 versus 1\n",
      "training classifier for 12 versus 2\n",
      "training classifier for 12 versus 3\n",
      "training classifier for 12 versus 4\n",
      "training classifier for 12 versus 5\n",
      "training classifier for 12 versus 6\n",
      "training classifier for 12 versus 7\n",
      "training classifier for 12 versus 8\n",
      "training classifier for 12 versus 9\n",
      "training classifier for 12 versus 10\n",
      "training classifier for 12 versus 11\n",
      "training classifier for 13 versus 0\n",
      "training classifier for 13 versus 1\n",
      "training classifier for 13 versus 2\n",
      "training classifier for 13 versus 3\n",
      "training classifier for 13 versus 4\n",
      "training classifier for 13 versus 5\n",
      "training classifier for 13 versus 6\n",
      "training classifier for 13 versus 7\n",
      "training classifier for 13 versus 8\n",
      "training classifier for 13 versus 9\n",
      "training classifier for 13 versus 10\n",
      "training classifier for 13 versus 11\n",
      "training classifier for 13 versus 12\n",
      "training classifier for 14 versus 0\n",
      "training classifier for 14 versus 1\n",
      "training classifier for 14 versus 2\n",
      "training classifier for 14 versus 3\n",
      "training classifier for 14 versus 4\n",
      "training classifier for 14 versus 5\n",
      "training classifier for 14 versus 6\n",
      "training classifier for 14 versus 7\n",
      "training classifier for 14 versus 8\n",
      "training classifier for 14 versus 9\n",
      "training classifier for 14 versus 10\n",
      "training classifier for 14 versus 11\n",
      "training classifier for 14 versus 12\n",
      "training classifier for 14 versus 13\n",
      "training classifier for 15 versus 0\n",
      "training classifier for 15 versus 1\n",
      "training classifier for 15 versus 2\n",
      "training classifier for 15 versus 3\n",
      "training classifier for 15 versus 4\n",
      "training classifier for 15 versus 5\n",
      "training classifier for 15 versus 6\n",
      "training classifier for 15 versus 7\n",
      "training classifier for 15 versus 8\n",
      "training classifier for 15 versus 9\n",
      "training classifier for 15 versus 10\n",
      "training classifier for 15 versus 11\n",
      "training classifier for 15 versus 12\n",
      "training classifier for 15 versus 13\n",
      "training classifier for 15 versus 14\n",
      "training classifier for 16 versus 0\n",
      "training classifier for 16 versus 1\n",
      "training classifier for 16 versus 2\n",
      "training classifier for 16 versus 3\n",
      "training classifier for 16 versus 4\n",
      "training classifier for 16 versus 5\n",
      "training classifier for 16 versus 6\n",
      "training classifier for 16 versus 7\n",
      "training classifier for 16 versus 8\n",
      "training classifier for 16 versus 9\n",
      "training classifier for 16 versus 10\n",
      "training classifier for 16 versus 11\n",
      "training classifier for 16 versus 12\n",
      "training classifier for 16 versus 13\n",
      "training classifier for 16 versus 14\n",
      "training classifier for 16 versus 15\n",
      "training classifier for 17 versus 0\n",
      "training classifier for 17 versus 1\n",
      "training classifier for 17 versus 2\n",
      "training classifier for 17 versus 3\n",
      "training classifier for 17 versus 4\n",
      "training classifier for 17 versus 5\n",
      "training classifier for 17 versus 6\n",
      "training classifier for 17 versus 7\n",
      "training classifier for 17 versus 8\n",
      "training classifier for 17 versus 9\n",
      "training classifier for 17 versus 10\n",
      "training classifier for 17 versus 11\n",
      "training classifier for 17 versus 12\n",
      "training classifier for 17 versus 13\n",
      "training classifier for 17 versus 14\n",
      "training classifier for 17 versus 15\n",
      "training classifier for 17 versus 16\n",
      "training classifier for 18 versus 0\n",
      "training classifier for 18 versus 1\n",
      "training classifier for 18 versus 2\n",
      "training classifier for 18 versus 3\n",
      "training classifier for 18 versus 4\n",
      "training classifier for 18 versus 5\n",
      "training classifier for 18 versus 6\n",
      "training classifier for 18 versus 7\n",
      "training classifier for 18 versus 8\n",
      "training classifier for 18 versus 9\n",
      "training classifier for 18 versus 10\n",
      "training classifier for 18 versus 11\n",
      "training classifier for 18 versus 12\n",
      "training classifier for 18 versus 13\n",
      "training classifier for 18 versus 14\n",
      "training classifier for 18 versus 15\n",
      "training classifier for 18 versus 16\n",
      "training classifier for 18 versus 17\n",
      "training classifier for 19 versus 0\n",
      "training classifier for 19 versus 1\n",
      "training classifier for 19 versus 2\n",
      "training classifier for 19 versus 3\n",
      "training classifier for 19 versus 4\n",
      "training classifier for 19 versus 5\n",
      "training classifier for 19 versus 6\n",
      "training classifier for 19 versus 7\n",
      "training classifier for 19 versus 8\n",
      "training classifier for 19 versus 9\n",
      "training classifier for 19 versus 10\n",
      "training classifier for 19 versus 11\n",
      "training classifier for 19 versus 12\n",
      "training classifier for 19 versus 13\n",
      "training classifier for 19 versus 14\n",
      "training classifier for 19 versus 15\n",
      "training classifier for 19 versus 16\n",
      "training classifier for 19 versus 17\n",
      "training classifier for 19 versus 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVA Accuracy on the full WineData task with 20 labels: 0.0593692022263\n",
      "Label 1 prediction accuracy: 0.17254174397\n",
      "Execution time for AVA: 14.578674077987671\n",
      "\n",
      "Viognier\n",
      "floral?\n",
      "-N-> harmonious?\n",
      "|    -N-> signature?\n",
      "|    |    -N-> class 0\t(59.0 for class 0, 2.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "|    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "-Y-> finish?\n",
      "|    -N-> class 1\t(0.0 for class 0, 4.0 for class 1)\n",
      "|    -Y-> class 0\t(1.0 for class 0, 0.0 for class 1)\n",
      "\n",
      "peach?\n",
      "-N-> peaches?\n",
      "|    -N-> pear?\n",
      "|    |    -N-> class 0\t(187.0 for class 0, 0.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "|    -Y-> class 1\t(0.0 for class 0, 3.0 for class 1)\n",
      "-Y-> class 1\t(0.0 for class 0, 4.0 for class 1)\n",
      "\n",
      "peaches?\n",
      "-N-> peach?\n",
      "|    -N-> apple?\n",
      "|    |    -N-> class 0\t(144.0 for class 0, 0.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "|    -Y-> class 1\t(0.0 for class 0, 3.0 for class 1)\n",
      "-Y-> class 1\t(0.0 for class 0, 4.0 for class 1)\n",
      "\n",
      "peaches?\n",
      "-N-> nectarine?\n",
      "|    -N-> serve?\n",
      "|    |    -N-> class 0\t(15.0 for class 0, 1.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "|    -Y-> class 1\t(0.0 for class 0, 2.0 for class 1)\n",
      "-Y-> class 1\t(0.0 for class 0, 4.0 for class 1)\n",
      "\n",
      "peaches?\n",
      "-N-> lovely?\n",
      "|    -N-> also?\n",
      "|    |    -N-> class 0\t(29.0 for class 0, 1.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "|    -Y-> class 1\t(0.0 for class 0, 2.0 for class 1)\n",
      "-Y-> class 1\t(0.0 for class 0, 4.0 for class 1)\n",
      "\n",
      "milk?\n",
      "-N-> edge?\n",
      "|    -N-> also?\n",
      "|    |    -N-> class 0\t(147.0 for class 0, 3.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "|    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "-Y-> class 1\t(0.0 for class 0, 3.0 for class 1)\n",
      "\n",
      "peaches?\n",
      "-N-> apricot?\n",
      "|    -N-> 5?\n",
      "|    |    -N-> class 0\t(61.0 for class 0, 1.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "|    -Y-> class 1\t(0.0 for class 0, 2.0 for class 1)\n",
      "-Y-> class 1\t(0.0 for class 0, 4.0 for class 1)\n",
      "\n",
      "peaches?\n",
      "-N-> peach?\n",
      "|    -N-> apple?\n",
      "|    |    -N-> class 0\t(56.0 for class 0, 0.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "|    -Y-> /?\n",
      "|    |    -N-> class 1\t(0.0 for class 0, 3.0 for class 1)\n",
      "|    |    -Y-> class 0\t(1.0 for class 0, 0.0 for class 1)\n",
      "-Y-> class 1\t(0.0 for class 0, 4.0 for class 1)\n",
      "\n",
      "peach?\n",
      "-N-> peaches?\n",
      "|    -N-> apple?\n",
      "|    |    -N-> class 0\t(47.0 for class 0, 0.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "|    -Y-> class 1\t(0.0 for class 0, 3.0 for class 1)\n",
      "-Y-> class 1\t(0.0 for class 0, 4.0 for class 1)\n",
      "\n",
      "peaches?\n",
      "-N-> peach?\n",
      "|    -N-> fragrant?\n",
      "|    |    -N-> class 0\t(49.0 for class 0, 0.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "|    -Y-> class 1\t(0.0 for class 0, 3.0 for class 1)\n",
      "-Y-> class 1\t(0.0 for class 0, 4.0 for class 1)\n",
      "\n",
      "peaches?\n",
      "-N-> peach?\n",
      "|    -N-> pear?\n",
      "|    |    -N-> class 0\t(48.0 for class 0, 0.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "|    -Y-> class 1\t(0.0 for class 0, 3.0 for class 1)\n",
      "-Y-> class 1\t(0.0 for class 0, 4.0 for class 1)\n",
      "\n",
      "fruits?\n",
      "-N-> peach?\n",
      "|    -N-> fragrant?\n",
      "|    |    -N-> class 0\t(31.0 for class 0, 0.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "|    -Y-> class 1\t(0.0 for class 0, 3.0 for class 1)\n",
      "-Y-> class 1\t(0.0 for class 0, 4.0 for class 1)\n",
      "\n",
      "milk?\n",
      "-N-> straw?\n",
      "|    -N-> fragrant?\n",
      "|    |    -N-> class 0\t(34.0 for class 0, 2.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "|    -Y-> provides?\n",
      "|    |    -N-> class 1\t(0.0 for class 0, 2.0 for class 1)\n",
      "|    |    -Y-> class 0\t(1.0 for class 0, 0.0 for class 1)\n",
      "-Y-> class 1\t(0.0 for class 0, 3.0 for class 1)\n",
      "\n",
      "peach?\n",
      "-N-> peaches?\n",
      "|    -N-> apple?\n",
      "|    |    -N-> class 0\t(45.0 for class 0, 0.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "|    -Y-> class 1\t(0.0 for class 0, 3.0 for class 1)\n",
      "-Y-> class 1\t(0.0 for class 0, 4.0 for class 1)\n",
      "\n",
      "peach?\n",
      "-N-> freshness?\n",
      "|    -N-> lovely?\n",
      "|    |    -N-> class 0\t(35.0 for class 0, 0.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "|    -Y-> class 1\t(0.0 for class 0, 3.0 for class 1)\n",
      "-Y-> class 1\t(0.0 for class 0, 4.0 for class 1)\n",
      "\n",
      "freshness?\n",
      "-N-> apricot?\n",
      "|    -N-> lychee?\n",
      "|    |    -N-> class 0\t(31.0 for class 0, 1.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "|    -Y-> even?\n",
      "|    |    -N-> class 1\t(0.0 for class 0, 3.0 for class 1)\n",
      "|    |    -Y-> class 0\t(1.0 for class 0, 0.0 for class 1)\n",
      "-Y-> class 1\t(0.0 for class 0, 3.0 for class 1)\n",
      "\n",
      "floral?\n",
      "-N-> very?\n",
      "|    -N-> light?\n",
      "|    |    -N-> class 0\t(16.0 for class 0, 1.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "|    -Y-> class 1\t(0.0 for class 0, 2.0 for class 1)\n",
      "-Y-> class 1\t(0.0 for class 0, 4.0 for class 1)\n",
      "\n",
      "floral?\n",
      "-N-> straw?\n",
      "|    -N-> pear?\n",
      "|    |    -N-> class 1\t(0.0 for class 0, 15.0 for class 1)\n",
      "|    |    -Y-> class 0\t(2.0 for class 0, 0.0 for class 1)\n",
      "|    -Y-> class 0\t(2.0 for class 0, 0.0 for class 1)\n",
      "-Y-> class 0\t(4.0 for class 0, 0.0 for class 1)\n",
      "\n",
      "fruits?\n",
      "-N-> white?\n",
      "|    -N-> spice?\n",
      "|    |    -N-> class 1\t(0.0 for class 0, 15.0 for class 1)\n",
      "|    |    -Y-> class 0\t(1.0 for class 0, 0.0 for class 1)\n",
      "|    -Y-> nose?\n",
      "|    |    -N-> class 0\t(3.0 for class 0, 0.0 for class 1)\n",
      "|    |    -Y-> class 1\t(0.0 for class 0, 1.0 for class 1)\n",
      "-Y-> class 0\t(4.0 for class 0, 0.0 for class 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#(B) Train depth 3 decision trees on the full WineData task (with 20 labels). \n",
    "#What accuracy do you get? How long does this take (in seconds)? \n",
    "#One of my least favorite wines is Viognier -- what words are indicative of this?\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"Training depth 3 decision trees on the full WineData task (with 20 labels) for OVA\")\n",
    "start_time = time.time()\n",
    "ova = multiclass.OVA(20, lambda: DecisionTreeClassifier(max_depth=3))\n",
    "ova.train(WineData.X, WineData.Y)\n",
    "P = ova.predictAll(WineData.Xte)\n",
    "end_time = time.time()\n",
    "print(\"\")\n",
    "print(\"OVA Accuracy on the full WineData task with 20 labels:\",mean(P == WineData.Yte))\n",
    "print(\"Label 1 prediction accuracy:\",mean(1 == WineData.Yte))\n",
    "print(\"Execution time for OVA:\",end_time - start_time)\n",
    "print(\"\")\n",
    "# ******Label Wine Viognier******\n",
    "print(WineData.labels[17])\n",
    "util.showTree(ova.f[17], WineData.words)\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training depth 3 decision trees on the full WineData task (with 20 labels) for AVA\")\n",
    "start_time = time.time()\n",
    "ava = multiclass.AVA(20, lambda: DecisionTreeClassifier(max_depth=3))\n",
    "ava.train(WineData.X, WineData.Y)\n",
    "P = ava.predictAll(WineData.Xte)\n",
    "end_time = time.time()\n",
    "print(\"AVA Accuracy on the full WineData task with 20 labels:\",mean(P == WineData.Yte))\n",
    "print(\"Label 1 prediction accuracy:\",mean(1 == WineData.Yte))\n",
    "print(\"Execution time for AVA:\",end_time - start_time)\n",
    "print(\"\")\n",
    "# ****** Viognier******\n",
    "print(WineData.labels[17])\n",
    "util.showTree(ava.f[17][0], WineData.words)\n",
    "print(\"\")\n",
    "util.showTree(ava.f[17][1], WineData.words)\n",
    "print(\"\")\n",
    "util.showTree(ava.f[17][2], WineData.words)\n",
    "print(\"\")\n",
    "util.showTree(ava.f[17][3], WineData.words)\n",
    "print(\"\")\n",
    "util.showTree(ava.f[17][4], WineData.words)\n",
    "print(\"\")\n",
    "util.showTree(ava.f[17][5], WineData.words)\n",
    "print(\"\")\n",
    "util.showTree(ava.f[17][6], WineData.words)\n",
    "print(\"\")\n",
    "util.showTree(ava.f[17][7], WineData.words)\n",
    "print(\"\")\n",
    "util.showTree(ava.f[17][8], WineData.words)\n",
    "print(\"\")\n",
    "util.showTree(ava.f[17][9], WineData.words)\n",
    "print(\"\")\n",
    "util.showTree(ava.f[17][10], WineData.words)\n",
    "print(\"\")\n",
    "util.showTree(ava.f[17][11], WineData.words)\n",
    "print(\"\")\n",
    "util.showTree(ava.f[17][12], WineData.words)\n",
    "print(\"\")\n",
    "util.showTree(ava.f[17][13], WineData.words)\n",
    "print(\"\")\n",
    "util.showTree(ava.f[17][14], WineData.words)\n",
    "print(\"\")\n",
    "util.showTree(ava.f[17][15], WineData.words)\n",
    "print(\"\")\n",
    "util.showTree(ava.f[17][16], WineData.words)\n",
    "print(\"\")\n",
    "util.showTree(ava.f[18][17], WineData.words)\n",
    "print(\"\")\n",
    "util.showTree(ava.f[19][17], WineData.words)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training classifier for 0 versus rest\n",
      "training classifier for 1 versus rest\n",
      "training classifier for 2 versus rest\n",
      "training classifier for 3 versus rest\n",
      "training classifier for 4 versus rest\n",
      "training classifier for 5 versus rest\n",
      "training classifier for 6 versus rest\n",
      "training classifier for 7 versus rest\n",
      "training classifier for 8 versus rest\n",
      "training classifier for 9 versus rest\n",
      "training classifier for 10 versus rest\n",
      "training classifier for 11 versus rest\n",
      "training classifier for 12 versus rest\n",
      "training classifier for 13 versus rest\n",
      "training classifier for 14 versus rest\n",
      "training classifier for 15 versus rest\n",
      "training classifier for 16 versus rest\n",
      "training classifier for 17 versus rest\n",
      "training classifier for 18 versus rest\n",
      "training classifier for 19 versus rest\n",
      "\n",
      "OVA Accuracy using confidence:  0.373840445269\n",
      "\n",
      "OVA Accuracy using zero-one predictions:  0.251391465677\n",
      "\n",
      "OVA Accuracy difference on switching from confidence to zero-one predictions: -0.122448979592\n",
      "\n",
      "training classifier for 1 versus 0\n",
      "training classifier for 2 versus 0\n",
      "training classifier for 2 versus 1\n",
      "training classifier for 3 versus 0\n",
      "training classifier for 3 versus 1\n",
      "training classifier for 3 versus 2\n",
      "training classifier for 4 versus 0\n",
      "training classifier for 4 versus 1\n",
      "training classifier for 4 versus 2\n",
      "training classifier for 4 versus 3\n",
      "training classifier for 5 versus 0\n",
      "training classifier for 5 versus 1\n",
      "training classifier for 5 versus 2\n",
      "training classifier for 5 versus 3\n",
      "training classifier for 5 versus 4\n",
      "training classifier for 6 versus 0\n",
      "training classifier for 6 versus 1\n",
      "training classifier for 6 versus 2\n",
      "training classifier for 6 versus 3\n",
      "training classifier for 6 versus 4\n",
      "training classifier for 6 versus 5\n",
      "training classifier for 7 versus 0\n",
      "training classifier for 7 versus 1\n",
      "training classifier for 7 versus 2\n",
      "training classifier for 7 versus 3\n",
      "training classifier for 7 versus 4\n",
      "training classifier for 7 versus 5\n",
      "training classifier for 7 versus 6\n",
      "training classifier for 8 versus 0\n",
      "training classifier for 8 versus 1\n",
      "training classifier for 8 versus 2\n",
      "training classifier for 8 versus 3\n",
      "training classifier for 8 versus 4\n",
      "training classifier for 8 versus 5\n",
      "training classifier for 8 versus 6\n",
      "training classifier for 8 versus 7\n",
      "training classifier for 9 versus 0\n",
      "training classifier for 9 versus 1\n",
      "training classifier for 9 versus 2\n",
      "training classifier for 9 versus 3\n",
      "training classifier for 9 versus 4\n",
      "training classifier for 9 versus 5\n",
      "training classifier for 9 versus 6\n",
      "training classifier for 9 versus 7\n",
      "training classifier for 9 versus 8\n",
      "training classifier for 10 versus 0\n",
      "training classifier for 10 versus 1\n",
      "training classifier for 10 versus 2\n",
      "training classifier for 10 versus 3\n",
      "training classifier for 10 versus 4\n",
      "training classifier for 10 versus 5\n",
      "training classifier for 10 versus 6\n",
      "training classifier for 10 versus 7\n",
      "training classifier for 10 versus 8\n",
      "training classifier for 10 versus 9\n",
      "training classifier for 11 versus 0\n",
      "training classifier for 11 versus 1\n",
      "training classifier for 11 versus 2\n",
      "training classifier for 11 versus 3\n",
      "training classifier for 11 versus 4\n",
      "training classifier for 11 versus 5\n",
      "training classifier for 11 versus 6\n",
      "training classifier for 11 versus 7\n",
      "training classifier for 11 versus 8\n",
      "training classifier for 11 versus 9\n",
      "training classifier for 11 versus 10\n",
      "training classifier for 12 versus 0\n",
      "training classifier for 12 versus 1\n",
      "training classifier for 12 versus 2\n",
      "training classifier for 12 versus 3\n",
      "training classifier for 12 versus 4\n",
      "training classifier for 12 versus 5\n",
      "training classifier for 12 versus 6\n",
      "training classifier for 12 versus 7\n",
      "training classifier for 12 versus 8\n",
      "training classifier for 12 versus 9\n",
      "training classifier for 12 versus 10\n",
      "training classifier for 12 versus 11\n",
      "training classifier for 13 versus 0\n",
      "training classifier for 13 versus 1\n",
      "training classifier for 13 versus 2\n",
      "training classifier for 13 versus 3\n",
      "training classifier for 13 versus 4\n",
      "training classifier for 13 versus 5\n",
      "training classifier for 13 versus 6\n",
      "training classifier for 13 versus 7\n",
      "training classifier for 13 versus 8\n",
      "training classifier for 13 versus 9\n",
      "training classifier for 13 versus 10\n",
      "training classifier for 13 versus 11\n",
      "training classifier for 13 versus 12\n",
      "training classifier for 14 versus 0\n",
      "training classifier for 14 versus 1\n",
      "training classifier for 14 versus 2\n",
      "training classifier for 14 versus 3\n",
      "training classifier for 14 versus 4\n",
      "training classifier for 14 versus 5\n",
      "training classifier for 14 versus 6\n",
      "training classifier for 14 versus 7\n",
      "training classifier for 14 versus 8\n",
      "training classifier for 14 versus 9\n",
      "training classifier for 14 versus 10\n",
      "training classifier for 14 versus 11\n",
      "training classifier for 14 versus 12\n",
      "training classifier for 14 versus 13\n",
      "training classifier for 15 versus 0\n",
      "training classifier for 15 versus 1\n",
      "training classifier for 15 versus 2\n",
      "training classifier for 15 versus 3\n",
      "training classifier for 15 versus 4\n",
      "training classifier for 15 versus 5\n",
      "training classifier for 15 versus 6\n",
      "training classifier for 15 versus 7\n",
      "training classifier for 15 versus 8\n",
      "training classifier for 15 versus 9\n",
      "training classifier for 15 versus 10\n",
      "training classifier for 15 versus 11\n",
      "training classifier for 15 versus 12\n",
      "training classifier for 15 versus 13\n",
      "training classifier for 15 versus 14\n",
      "training classifier for 16 versus 0\n",
      "training classifier for 16 versus 1\n",
      "training classifier for 16 versus 2\n",
      "training classifier for 16 versus 3\n",
      "training classifier for 16 versus 4\n",
      "training classifier for 16 versus 5\n",
      "training classifier for 16 versus 6\n",
      "training classifier for 16 versus 7\n",
      "training classifier for 16 versus 8\n",
      "training classifier for 16 versus 9\n",
      "training classifier for 16 versus 10\n",
      "training classifier for 16 versus 11\n",
      "training classifier for 16 versus 12\n",
      "training classifier for 16 versus 13\n",
      "training classifier for 16 versus 14\n",
      "training classifier for 16 versus 15\n",
      "training classifier for 17 versus 0\n",
      "training classifier for 17 versus 1\n",
      "training classifier for 17 versus 2\n",
      "training classifier for 17 versus 3\n",
      "training classifier for 17 versus 4\n",
      "training classifier for 17 versus 5\n",
      "training classifier for 17 versus 6\n",
      "training classifier for 17 versus 7\n",
      "training classifier for 17 versus 8\n",
      "training classifier for 17 versus 9\n",
      "training classifier for 17 versus 10\n",
      "training classifier for 17 versus 11\n",
      "training classifier for 17 versus 12\n",
      "training classifier for 17 versus 13\n",
      "training classifier for 17 versus 14\n",
      "training classifier for 17 versus 15\n",
      "training classifier for 17 versus 16\n",
      "training classifier for 18 versus 0\n",
      "training classifier for 18 versus 1\n",
      "training classifier for 18 versus 2\n",
      "training classifier for 18 versus 3\n",
      "training classifier for 18 versus 4\n",
      "training classifier for 18 versus 5\n",
      "training classifier for 18 versus 6\n",
      "training classifier for 18 versus 7\n",
      "training classifier for 18 versus 8\n",
      "training classifier for 18 versus 9\n",
      "training classifier for 18 versus 10\n",
      "training classifier for 18 versus 11\n",
      "training classifier for 18 versus 12\n",
      "training classifier for 18 versus 13\n",
      "training classifier for 18 versus 14\n",
      "training classifier for 18 versus 15\n",
      "training classifier for 18 versus 16\n",
      "training classifier for 18 versus 17\n",
      "training classifier for 19 versus 0\n",
      "training classifier for 19 versus 1\n",
      "training classifier for 19 versus 2\n",
      "training classifier for 19 versus 3\n",
      "training classifier for 19 versus 4\n",
      "training classifier for 19 versus 5\n",
      "training classifier for 19 versus 6\n",
      "training classifier for 19 versus 7\n",
      "training classifier for 19 versus 8\n",
      "training classifier for 19 versus 9\n",
      "training classifier for 19 versus 10\n",
      "training classifier for 19 versus 11\n",
      "training classifier for 19 versus 12\n",
      "training classifier for 19 versus 13\n",
      "training classifier for 19 versus 14\n",
      "training classifier for 19 versus 15\n",
      "training classifier for 19 versus 16\n",
      "training classifier for 19 versus 17\n",
      "training classifier for 19 versus 18\n",
      "\n",
      "AVA Accuracy using confidence:  0.0584415584416\n",
      "\n",
      "AVA Accuracy using zero-one predictions:  0.251391465677\n",
      "\n",
      "AVA Accuracy difference on switching from confidence to zero-one predictions: 0.192949907236\n"
     ]
    }
   ],
   "source": [
    "# (C) Compare the accuracy using zero-one predictions versus using confidence. How much difference does it make?\n",
    "\n",
    "ova = multiclass.OVA(20, lambda: DecisionTreeClassifier(max_depth=3))\n",
    "ova.train(WineData.X, WineData.Y)\n",
    "P_confidence = ova.predictAll(WineData.Xte)  #by default useZeroOne=False\n",
    "accuracy_confidence = mean(P_confidence == WineData.Yte)\n",
    "print(\"\")\n",
    "print(\"OVA Accuracy using confidence: \",accuracy_confidence)\n",
    "\n",
    "print(\"\")\n",
    "P_zero_one = ova.predictAll(WineData.Xte, useZeroOne=True)\n",
    "accuracy_zero_one = mean(P_zero_one == WineData.Yte)\n",
    "print(\"OVA Accuracy using zero-one predictions: \",accuracy_zero_one)\n",
    "print(\"\")\n",
    "print(\"OVA Accuracy difference on switching from confidence to zero-one predictions:\",accuracy_zero_one - accuracy_confidence)\n",
    "print(\"\")\n",
    "ava = multiclass.AVA(20, lambda: DecisionTreeClassifier(max_depth=3))\n",
    "ava.train(WineData.X, WineData.Y)\n",
    "P_confidence = ava.predictAll(WineData.Xte)  #by default useZeroOne=False\n",
    "print(\"\")\n",
    "accuracy_confidence = mean(P_confidence == WineData.Yte)\n",
    "print(\"AVA Accuracy using confidence: \",accuracy_confidence)\n",
    "\n",
    "print(\"\")\n",
    "P_zero_one = ova.predictAll(WineData.Xte, useZeroOne=True)\n",
    "accuracy_zero_one = mean(P_zero_one == WineData.Yte)\n",
    "print(\"AVA Accuracy using zero-one predictions: \",accuracy_zero_one)\n",
    "print(\"\")\n",
    "print(\"AVA Accuracy difference on switching from confidence to zero-one predictions:\",accuracy_zero_one - accuracy_confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[WU1 Answer] : \n",
    "\n",
    "(A) What words are most indicative of being Sauvignon-Blanc? Which words are most indicative of not being Sauvignon-Blanc? What about Pinot-Noir (label==2)?\n",
    "\n",
    "Citrus, Lime and Gooseberry are the most indicative words of not being Sauvignon-Blanc as these combinations predicts 356 actual non Sauvignon-Blanc wine as not being Sauvignon-Blanc which is a huge number and only 10 actual Sauvignon-Blanc wine as not being Sauvignon-Blanc.\n",
    "\n",
    "Citrus, Grapefruit and opens are the most indicative words of being Sauvignon-Blanc as these combinations predicts 14 actual  Sauvignon-Blanc wine as being Sauvignon-Blanc and 0 actual non Sauvignon-Blanc wine as being Sauvignon-Blanc.\n",
    "\n",
    "Cherry, Raspberries and Strawberries are the most indicative words of not being Pinot-Noir as these combination of words predicts 225 actual non Pinot-Noir as not being Pinot Noir which is a huge number.\n",
    "\n",
    "Cherry, Raspberries and round are the most indicative words of being Pinot-Noir as these combination of words predicts 12 actual  Pinot-Noir as being Pinot Noir and 0 actual non Pinot-Noir as Pinot-Noir.\n",
    "\n",
    "\n",
    "(B) Train depth 3 decision trees on the full WineData task (with 20 labels). What accuracy do you get? How long does this take (in seconds)? One of my least favorite wines is Viognier -- what words are indicative of this?\n",
    "\n",
    "OVA Accuracy on the full WineData task with 20 labels: 0.369202226345\n",
    "Execution time for OVA: 2.4106128215789795\n",
    "AVA Accuracy on the full WineData task with 20 labels: 0.0556586270872\n",
    "Execution time for AVA: 13.042717933654785\n",
    "\n",
    "peaches, nectarine and chilled are the most indicative words of not being Viognier wine as these combinations predicts 1036 actual non Viognier wine as not being Viognier wine which is very huge number to make these words indicative for this wine.\n",
    "\n",
    "peaches and milk are the most indicative words of being Viognier wine as these combinations predicts 3 actual Viognier wine as being Viognier wine and 0 non Viognier as being Viognier wine.\n",
    "\n",
    "(C) Compare the accuracy using zero-one predictions versus using confidence. How much difference does it make?\n",
    "\n",
    "OVA Accuracy is getting decreased by 0.12 on using zero-one prediction instead of confidence. But AVA accuracy got increased by 0.18 on using zero-one prediction instead of confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/kitten.jpeg\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br>\n",
    "## WU2 (10%):\n",
    "Using decision trees of constant depth for each\n",
    "classifier (but you choose it as well as you can!), train AVA, OVA and\n",
    "Tree (using balanced trees) for the wine data. Which does best and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training classifier for 0 versus rest\n",
      "training classifier for 1 versus rest\n",
      "training classifier for 2 versus rest\n",
      "training classifier for 3 versus rest\n",
      "training classifier for 4 versus rest\n",
      "training classifier for 5 versus rest\n",
      "training classifier for 6 versus rest\n",
      "training classifier for 7 versus rest\n",
      "training classifier for 8 versus rest\n",
      "training classifier for 9 versus rest\n",
      "training classifier for 10 versus rest\n",
      "training classifier for 11 versus rest\n",
      "training classifier for 12 versus rest\n",
      "training classifier for 13 versus rest\n",
      "training classifier for 14 versus rest\n",
      "training classifier for 15 versus rest\n",
      "training classifier for 16 versus rest\n",
      "training classifier for 17 versus rest\n",
      "training classifier for 18 versus rest\n",
      "training classifier for 19 versus rest\n",
      "\n",
      "OVA Accuracy using max_depth 6:  0.364564007421\n",
      "\n",
      "training classifier for 1 versus 0\n",
      "training classifier for 2 versus 0\n",
      "training classifier for 2 versus 1\n",
      "training classifier for 3 versus 0\n",
      "training classifier for 3 versus 1\n",
      "training classifier for 3 versus 2\n",
      "training classifier for 4 versus 0\n",
      "training classifier for 4 versus 1\n",
      "training classifier for 4 versus 2\n",
      "training classifier for 4 versus 3\n",
      "training classifier for 5 versus 0\n",
      "training classifier for 5 versus 1\n",
      "training classifier for 5 versus 2\n",
      "training classifier for 5 versus 3\n",
      "training classifier for 5 versus 4\n",
      "training classifier for 6 versus 0\n",
      "training classifier for 6 versus 1\n",
      "training classifier for 6 versus 2\n",
      "training classifier for 6 versus 3\n",
      "training classifier for 6 versus 4\n",
      "training classifier for 6 versus 5\n",
      "training classifier for 7 versus 0\n",
      "training classifier for 7 versus 1\n",
      "training classifier for 7 versus 2\n",
      "training classifier for 7 versus 3\n",
      "training classifier for 7 versus 4\n",
      "training classifier for 7 versus 5\n",
      "training classifier for 7 versus 6\n",
      "training classifier for 8 versus 0\n",
      "training classifier for 8 versus 1\n",
      "training classifier for 8 versus 2\n",
      "training classifier for 8 versus 3\n",
      "training classifier for 8 versus 4\n",
      "training classifier for 8 versus 5\n",
      "training classifier for 8 versus 6\n",
      "training classifier for 8 versus 7\n",
      "training classifier for 9 versus 0\n",
      "training classifier for 9 versus 1\n",
      "training classifier for 9 versus 2\n",
      "training classifier for 9 versus 3\n",
      "training classifier for 9 versus 4\n",
      "training classifier for 9 versus 5\n",
      "training classifier for 9 versus 6\n",
      "training classifier for 9 versus 7\n",
      "training classifier for 9 versus 8\n",
      "training classifier for 10 versus 0\n",
      "training classifier for 10 versus 1\n",
      "training classifier for 10 versus 2\n",
      "training classifier for 10 versus 3\n",
      "training classifier for 10 versus 4\n",
      "training classifier for 10 versus 5\n",
      "training classifier for 10 versus 6\n",
      "training classifier for 10 versus 7\n",
      "training classifier for 10 versus 8\n",
      "training classifier for 10 versus 9\n",
      "training classifier for 11 versus 0\n",
      "training classifier for 11 versus 1\n",
      "training classifier for 11 versus 2\n",
      "training classifier for 11 versus 3\n",
      "training classifier for 11 versus 4\n",
      "training classifier for 11 versus 5\n",
      "training classifier for 11 versus 6\n",
      "training classifier for 11 versus 7\n",
      "training classifier for 11 versus 8\n",
      "training classifier for 11 versus 9\n",
      "training classifier for 11 versus 10\n",
      "training classifier for 12 versus 0\n",
      "training classifier for 12 versus 1\n",
      "training classifier for 12 versus 2\n",
      "training classifier for 12 versus 3\n",
      "training classifier for 12 versus 4\n",
      "training classifier for 12 versus 5\n",
      "training classifier for 12 versus 6\n",
      "training classifier for 12 versus 7\n",
      "training classifier for 12 versus 8\n",
      "training classifier for 12 versus 9\n",
      "training classifier for 12 versus 10\n",
      "training classifier for 12 versus 11\n",
      "training classifier for 13 versus 0\n",
      "training classifier for 13 versus 1\n",
      "training classifier for 13 versus 2\n",
      "training classifier for 13 versus 3\n",
      "training classifier for 13 versus 4\n",
      "training classifier for 13 versus 5\n",
      "training classifier for 13 versus 6\n",
      "training classifier for 13 versus 7\n",
      "training classifier for 13 versus 8\n",
      "training classifier for 13 versus 9\n",
      "training classifier for 13 versus 10\n",
      "training classifier for 13 versus 11\n",
      "training classifier for 13 versus 12\n",
      "training classifier for 14 versus 0\n",
      "training classifier for 14 versus 1\n",
      "training classifier for 14 versus 2\n",
      "training classifier for 14 versus 3\n",
      "training classifier for 14 versus 4\n",
      "training classifier for 14 versus 5\n",
      "training classifier for 14 versus 6\n",
      "training classifier for 14 versus 7\n",
      "training classifier for 14 versus 8\n",
      "training classifier for 14 versus 9\n",
      "training classifier for 14 versus 10\n",
      "training classifier for 14 versus 11\n",
      "training classifier for 14 versus 12\n",
      "training classifier for 14 versus 13\n",
      "training classifier for 15 versus 0\n",
      "training classifier for 15 versus 1\n",
      "training classifier for 15 versus 2\n",
      "training classifier for 15 versus 3\n",
      "training classifier for 15 versus 4\n",
      "training classifier for 15 versus 5\n",
      "training classifier for 15 versus 6\n",
      "training classifier for 15 versus 7\n",
      "training classifier for 15 versus 8\n",
      "training classifier for 15 versus 9\n",
      "training classifier for 15 versus 10\n",
      "training classifier for 15 versus 11\n",
      "training classifier for 15 versus 12\n",
      "training classifier for 15 versus 13\n",
      "training classifier for 15 versus 14\n",
      "training classifier for 16 versus 0\n",
      "training classifier for 16 versus 1\n",
      "training classifier for 16 versus 2\n",
      "training classifier for 16 versus 3\n",
      "training classifier for 16 versus 4\n",
      "training classifier for 16 versus 5\n",
      "training classifier for 16 versus 6\n",
      "training classifier for 16 versus 7\n",
      "training classifier for 16 versus 8\n",
      "training classifier for 16 versus 9\n",
      "training classifier for 16 versus 10\n",
      "training classifier for 16 versus 11\n",
      "training classifier for 16 versus 12\n",
      "training classifier for 16 versus 13\n",
      "training classifier for 16 versus 14\n",
      "training classifier for 16 versus 15\n",
      "training classifier for 17 versus 0\n",
      "training classifier for 17 versus 1\n",
      "training classifier for 17 versus 2\n",
      "training classifier for 17 versus 3\n",
      "training classifier for 17 versus 4\n",
      "training classifier for 17 versus 5\n",
      "training classifier for 17 versus 6\n",
      "training classifier for 17 versus 7\n",
      "training classifier for 17 versus 8\n",
      "training classifier for 17 versus 9\n",
      "training classifier for 17 versus 10\n",
      "training classifier for 17 versus 11\n",
      "training classifier for 17 versus 12\n",
      "training classifier for 17 versus 13\n",
      "training classifier for 17 versus 14\n",
      "training classifier for 17 versus 15\n",
      "training classifier for 17 versus 16\n",
      "training classifier for 18 versus 0\n",
      "training classifier for 18 versus 1\n",
      "training classifier for 18 versus 2\n",
      "training classifier for 18 versus 3\n",
      "training classifier for 18 versus 4\n",
      "training classifier for 18 versus 5\n",
      "training classifier for 18 versus 6\n",
      "training classifier for 18 versus 7\n",
      "training classifier for 18 versus 8\n",
      "training classifier for 18 versus 9\n",
      "training classifier for 18 versus 10\n",
      "training classifier for 18 versus 11\n",
      "training classifier for 18 versus 12\n",
      "training classifier for 18 versus 13\n",
      "training classifier for 18 versus 14\n",
      "training classifier for 18 versus 15\n",
      "training classifier for 18 versus 16\n",
      "training classifier for 18 versus 17\n",
      "training classifier for 19 versus 0\n",
      "training classifier for 19 versus 1\n",
      "training classifier for 19 versus 2\n",
      "training classifier for 19 versus 3\n",
      "training classifier for 19 versus 4\n",
      "training classifier for 19 versus 5\n",
      "training classifier for 19 versus 6\n",
      "training classifier for 19 versus 7\n",
      "training classifier for 19 versus 8\n",
      "training classifier for 19 versus 9\n",
      "training classifier for 19 versus 10\n",
      "training classifier for 19 versus 11\n",
      "training classifier for 19 versus 12\n",
      "training classifier for 19 versus 13\n",
      "training classifier for 19 versus 14\n",
      "training classifier for 19 versus 15\n",
      "training classifier for 19 versus 16\n",
      "training classifier for 19 versus 17\n",
      "training classifier for 19 versus 18\n",
      "\n",
      "AVA Accuracy using max_depth 6:  0.0677179962894\n",
      "training classifier for [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] versus [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "training classifier for [0, 1, 2, 3, 4] versus [5, 6, 7, 8, 9]\n",
      "training classifier for [0, 1] versus [2, 3, 4]\n",
      "training classifier for [0] versus [1]\n",
      "training classifier for [2] versus [3, 4]\n",
      "training classifier for [3] versus [4]\n",
      "training classifier for [5, 6] versus [7, 8, 9]\n",
      "training classifier for [5] versus [6]\n",
      "training classifier for [7] versus [8, 9]\n",
      "training classifier for [8] versus [9]\n",
      "training classifier for [10, 11, 12, 13, 14] versus [15, 16, 17, 18, 19]\n",
      "training classifier for [10, 11] versus [12, 13, 14]\n",
      "training classifier for [10] versus [11]\n",
      "training classifier for [12] versus [13, 14]\n",
      "training classifier for [13] versus [14]\n",
      "training classifier for [15, 16] versus [17, 18, 19]\n",
      "training classifier for [15] versus [16]\n",
      "training classifier for [17] versus [18, 19]\n",
      "training classifier for [18] versus [19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tree Accuracy using max_depth 6:  0.30241187384\n"
     ]
    }
   ],
   "source": [
    "# WU2 CODE HERE\n",
    "ova = multiclass.OVA(20, lambda: DecisionTreeClassifier(max_depth=6))\n",
    "ova.train(WineData.X, WineData.Y)\n",
    "P = ova.predictAll(WineData.Xte)  \n",
    "accuracy = mean(P == WineData.Yte)\n",
    "print(\"\")\n",
    "print(\"OVA Accuracy using max_depth 6: \",accuracy)\n",
    "\n",
    "print(\"\")\n",
    "ava = multiclass.AVA(20, lambda: DecisionTreeClassifier(max_depth=6))\n",
    "ava.train(WineData.X, WineData.Y)\n",
    "P = ava.predictAll(WineData.Xte)  \n",
    "print(\"\")\n",
    "accuracy = mean(P == WineData.Yte)\n",
    "print(\"AVA Accuracy using max_depth 6: \",accuracy)\n",
    "\n",
    "h = multiclass.MCTree(t, lambda: DecisionTreeClassifier(max_depth=6))\n",
    "h.train(WineData.X, WineData.Y)\n",
    "P = h.predictAll(WineData.Xte)\n",
    "print(\"\")\n",
    "accuracy = mean(P == WineData.Yte)\n",
    "print(\"Tree Accuracy using max_depth 6: \",accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[WU2 ANSWER HERE] \n",
    "Using decision trees of constant depth for each classifier (but you choose it as well as you can!), train AVA, OVA and Tree (using balanced trees) for the wine data. Which does best and why?\n",
    "\n",
    "OVA Accuracy using max_depth 4:  0.349721706865\n",
    "OVA Accuracy using max_depth 5:  0.356215213358\n",
    "OVA Accuracy using max_depth 6:  0.360853432282\n",
    "OVA Accuracy using max_depth 7:  0.34879406308\n",
    "\n",
    "AVA Accuracy using max_depth 4:  0.0677179962894\n",
    "AVA Accuracy using max_depth 5:  0.0621521335807\n",
    "AVA Accuracy using max_depth 6:  0.0640074211503\n",
    "AVA Accuracy using max_depth 7:  0.0658627087199\n",
    "\n",
    "Tree Accuracy using max_depth 4:  0.233766233766\n",
    "Tree Accuracy using max_depth 5:  0.237476808905\n",
    "Tree Accuracy using max_depth 6:  0.233766233766\n",
    "Tree Accuracy using max_depth 7:  0.23654\n",
    "\n",
    "On training OVA, AVA and Tree (using balanced trees) for the wine data for different depths, we can see that OVA is doing better as compared to AVA and Tree and with decision tree of depth 6, its accuracy is highest that is 0.360853432282. Hence OVA does best for this classification problem over winedata. \n",
    "\n",
    "The best multiclass classification method to use when generalizing binary to multiclass is highly specific to the problem. Since our wine dataset is small having only 2156 smaples and to train the model better, we need good amount of data. Thus OVA will do better in this case. In OVA, we use the entire dataset to build each OVA classifier. To build each ith OVA classifier, training examples corresponding to class i is considered as positive and rest all examples are considered as negative and on this we run binary classifier, hence using entire dataset for each classifier to train the model better. But in case of AVA and Tree, we use only subset of dataset to build each classifier. Since our dataset is small, we should use as much of data as we can to train our model to get best accuracy and OVA does this for given problem and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/kitten.jpeg\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br>\n",
    "## WU-EC1 ExtraCredit (10%):\n",
    "Build a better tree (any way you want) other\n",
    "than the balanced binary tree. Fill in your code for this in\n",
    "`getMyTreeForWine`, which defaults to a balanced tree. It should get\n",
    "at least 5% lower absolute error to get the extra credit. Describe what you\n",
    "did.\n",
    "\n",
    "[YOUR WU-EC1 ANSWER HERE] \n",
    "To build a better tree for more accuracy as compared to balanced binary tree, I changed the splitting of the tree from as in balanced binary tree. In my implementation, at each node I am splitting the tree as first item in left tree and rest in right tree and so on. This implementation gives 5% lower absolute error which is shown below in execution of the both trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********Balanced Tree***********\n",
      "[[[0 [1 2]] [[3 4] [5 6]]] [[[7 8] [9 10]] [[11 12] [13 14]]]]\n",
      "\n",
      "training classifier for [0, 1, 2, 3, 4, 5, 6] versus [7, 8, 9, 10, 11, 12, 13, 14]\n",
      "training classifier for [0, 1, 2] versus [3, 4, 5, 6]\n",
      "training classifier for [0] versus [1, 2]\n",
      "training classifier for [1] versus [2]\n",
      "training classifier for [3, 4] versus [5, 6]\n",
      "training classifier for [3] versus [4]\n",
      "training classifier for [5] versus [6]\n",
      "training classifier for [7, 8, 9, 10] versus [11, 12, 13, 14]\n",
      "training classifier for [7, 8] versus [9, 10]\n",
      "training classifier for [7] versus [8]\n",
      "training classifier for [9] versus [10]\n",
      "training classifier for [11, 12] versus [13, 14]\n",
      "training classifier for [11] versus [12]\n",
      "training classifier for [13] versus [14]\n",
      "Accuracy with balanced tree:  0.272727272727\n",
      "\n",
      "*********My Tree***********\n",
      "[[[[[[[0 [1 2]] [3 4]] [5 6]] [7 8]] [9 10]] [11 12]] [13 14]]\n",
      "\n",
      "training classifier for [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] versus [13, 14]\n",
      "training classifier for [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] versus [11, 12]\n",
      "training classifier for [0, 1, 2, 3, 4, 5, 6, 7, 8] versus [9, 10]\n",
      "training classifier for [0, 1, 2, 3, 4, 5, 6] versus [7, 8]\n",
      "training classifier for [0, 1, 2, 3, 4] versus [5, 6]\n",
      "training classifier for [0, 1, 2] versus [3, 4]\n",
      "training classifier for [0] versus [1, 2]\n",
      "training classifier for [1] versus [2]\n",
      "training classifier for [3] versus [4]\n",
      "training classifier for [5] versus [6]\n",
      "training classifier for [7] versus [8]\n",
      "training classifier for [9] versus [10]\n",
      "training classifier for [11] versus [12]\n",
      "training classifier for [13] versus [14]\n",
      "Accuracy with my tree:  0.32560296846\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(multiclass)\n",
    "\n",
    "balanced_tree = multiclass.makeBalancedTree(range(15))\n",
    "print(\"*********Balanced Tree***********\")\n",
    "print(balanced_tree)\n",
    "print(\"\")\n",
    "h = multiclass.MCTree(balanced_tree, lambda: DecisionTreeClassifier(max_depth=3))\n",
    "h.train(WineData.X, WineData.Y)\n",
    "P = h.predictAll(WineData.Xte)\n",
    "print(\"Accuracy with balanced tree: \",mean(P == WineData.Yte))\n",
    "print(\"\")\n",
    "my_tree = multiclass.getMyTreeForWine(range(15))\n",
    "print(\"*********My Tree***********\")\n",
    "print(my_tree)\n",
    "print(\"\")\n",
    "h = multiclass.MCTree(my_tree, lambda: DecisionTreeClassifier(max_depth=3))\n",
    "h.train(WineData.X, WineData.Y)\n",
    "P = h.predictAll(WineData.Xte)\n",
    "print(\"Accuracy with my tree: \",mean(P == WineData.Yte))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Gradient Descent and Linear Classification *[30% impl, 20% writeup]*\n",
    "\n",
    "To get started with linear models, we will implement a generic\n",
    "gradient descent method.  This should go in `gd.py`, which\n",
    "contains a single (short) function: `gd`. This takes five\n",
    "parameters: the function we're optimizing, it's gradient, an initial\n",
    "position, a number of iterations to run, and an initial step size.\n",
    "\n",
    "In each iteration of gradient descent, we will compute the gradient\n",
    "and take a step in that direction, with step size `eta`.  We\n",
    "will have an *adaptive* step size, where `eta` is computed\n",
    "as `stepSize` divided by the square root of the iteration\n",
    "number (counting from one).\n",
    "\n",
    "Once you have an implementation running, we can check it on a simple\n",
    "example of minimizing the function `x^2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0034641051795872,\n",
       " array([ 100.        ,   36.        ,   18.5153247 ,   10.95094653,\n",
       "           7.00860578,    4.72540613,    3.30810578,    2.38344246,\n",
       "           1.75697198,    1.31968118,    1.00694021]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imports\n",
    "from numpy import *\n",
    "from pylab import *\n",
    "from util import *\n",
    "import datasets,binary,gd,linear,mlGraphics,multiclass,runClassifier\n",
    "import importlib\n",
    "importlib.reload(linear)\n",
    "importlib.reload(gd)\n",
    "\n",
    "gd.gd(lambda x: x**2, lambda x: 2*x, 10, 10, 0.2)\n",
    "#(1.0034641051795872, array([ 100.        ,   36.        ,   18.5153247 ,   10.95094653,\n",
    "#          7.00860578,    4.72540613,    3.30810578,    2.38344246,\n",
    "#          1.75697198,    1.31968118,    1.00694021]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the \"solution\" found is about 1, which is not great\n",
    "(it should be zero!), but it's better than the initial value of ten!\n",
    "If yours is going up rather than going down, you probably have a sign\n",
    "error somewhere!\n",
    "\n",
    "We can let it run longer and plot the trajectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003645900464603937\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFehJREFUeJzt3X2QXfV93/H39967d4Uk0OMCsgQRJCoJMbFxFwebNPWA\naW3yAO3YtT1uonGZ0UzGre0404S0M3U7nenYM66fOhlPGXCidKhNiplCXDc1lXHcdiYKku2IB2GE\niQ0CgRaDBAihffr2j3tWrKR7d5e9u7p7f/t+jXbuPWfP3fs9OprP/vQ95/xuZCaSpHLVel2AJGlx\nGfSSVDiDXpIKZ9BLUuEMekkqnEEvSYUz6CWpcAa9JBXOoJekwjV6XQDAxo0bc+vWrb0uQ5L6yt69\ne5/PzKHZtlsSQb9161b27NnT6zIkqa9ExE/msp2tG0kqnEEvSYUz6CWpcAa9JBXOoJekws0a9BHx\nlYg4HBEPTVu3PiLui4gD1eO6an1ExJci4vGI2BcRb1vM4iVJs5vLiP5PgPectu4WYFdmbgN2VcsA\n7wW2VV87gC8vTJmSpPmaNegz87vAC6etvhHYWT3fCdw0bf2fZstfAWsjYtNCFXu6B378Av/xWz9k\nfGJysd5CkvrefHv0F2TmIYDq8fxq/WbgqWnbHazWnSEidkTEnojYMzIyMq8ivv/ki/ynbz/OiXGD\nXpI6WeiTsdFmXdtPH8/MWzNzODOHh4ZmvYO3rYF6q/wxR/SS1NF8g/65qZZM9Xi4Wn8QuGjadluA\nZ+Zf3symgn7UoJekjuYb9PcC26vn24F7pq3/7erqm6uBo1MtnsXQPDmib/ufBkkSc5jULCK+CrwL\n2BgRB4FPAZ8G/iwibgaeBN5fbf5N4AbgceBV4COLUPNJA41Wp2jUHr0kdTRr0Gfmhzp867o22ybw\n0W6Lmit79JI0u76+M/Zkj94RvSR11NdB32w4opek2fR30HsyVpJm1ddBb49ekmbX50HvVTeSNJs+\nD3pvmJKk2fR10HsyVpJm199Bb49ekmbV10E/MDWiH/eqG0nqpL+DfupkrCN6Seqor4Pe1o0kza6v\ng94pECRpdkUEvSN6Seqsz4N+qkfvyVhJ6qSvgz4iaNZrjuglaQZ9HfTQGtWP2aOXpI76P+gbjugl\naSb9H/T1mtfRS9IM+j7om/Uao94ZK0kd9X3QD9TD1o0kzaDvg75pj16SZtT3QT/g5ZWSNKMigt4b\npiSps74P+tbJ2IlelyFJS1bfB/1AIxhzRC9JHfV/0Nujl6QZ9X3Qt1o3Br0kddL3Qe8UCJI0s74P\n+tbslfboJamTvg/6gXrYupGkGXQV9BHxuxHxcEQ8FBFfjYgVEXFJROyOiAMRcWdENBeq2HY8GStJ\nM5t30EfEZuBjwHBmvhmoAx8EPgN8PjO3AS8CNy9EoZ04e6Ukzazb1k0DOCciGsBK4BBwLXBX9f2d\nwE1dvseMBj0ZK0kzmnfQZ+bTwGeBJ2kF/FFgL3AkM8erzQ4Cm7stciYDnoyVpBl107pZB9wIXAK8\nCVgFvLfNpm1TOCJ2RMSeiNgzMjIy3zIYqNeYmEwmJg17SWqnm9bNu4G/zcyRzBwD7gbeCaytWjkA\nW4Bn2r04M2/NzOHMHB4aGpp3EQONALB9I0kddBP0TwJXR8TKiAjgOuAR4H7gfdU224F7uitxZs16\naxc8IStJ7XXTo99N66Tr94AHq591K/AHwCcj4nFgA3D7AtTZ0UAV9GNeSy9JbTVm36SzzPwU8KnT\nVj8BvL2bn/tGNBtV0HtCVpLaKuDO2Kmgd0QvSe0UEPStk7H26CWpvb4P+qYjekmaUd8H/VTrxonN\nJKm9/g/6hiN6SZpJ/wf9VI9+3KtuJKmdvg/6QUf0kjSjvg96L6+UpJkZ9JJUuGKC/oRX3UhSW30f\n9K9fR+/JWElqp++D3mmKJWlmfR/03hkrSTPr+6CfumHKO2Mlqb2+D3p79JI0s74Peue6kaSZ9X3Q\n12tBLezRS1InfR/00BrVG/SS1F4RQd9s1PzgEUnqoIygd0QvSR0VEfQD9RpjTlMsSW2VEfSNsHUj\nSR2UEfR1e/SS1EkRQd+s1xjzOnpJaquMoG94MlaSOiki6FvX0XsyVpLaKSToPRkrSZ0UEvQ157qR\npA6KCHpvmJKkzooIeue6kaTOygj6hidjJamTroI+ItZGxF0R8WhE7I+Id0TE+oi4LyIOVI/rFqrY\nTpr26CWpo25H9F8E/iIzfx54C7AfuAXYlZnbgF3V8qJqNsLWjSR1MO+gj4jzgF8FbgfIzNHMPALc\nCOysNtsJ3NRtkbNxCgRJ6qybEf2lwAjwxxHx/Yi4LSJWARdk5iGA6vH8di+OiB0RsSci9oyMjHRR\nxtTslQa9JLXTTdA3gLcBX87MK4FjvIE2TWbempnDmTk8NDTURRneGStJM+km6A8CBzNzd7V8F63g\nfy4iNgFUj4e7K3F2zerO2EzDXpJON++gz8xngaci4rJq1XXAI8C9wPZq3Xbgnq4qnINmo7Ub45MG\nvSSdrtHl6/8FcEdENIEngI/Q+uXxZxFxM/Ak8P4u32NWA/VW0I9NTJ58Lklq6SroM/MHwHCbb13X\nzc99o04G/XhC82y+syQtfUUMfweq1s2JiYkeVyJJS08RQd+sB4BX3khSG0UE/eutG6+ll6TTFRH0\nU1fdOA2CJJ2piKCfGtE7DYIknamIoG+evLzSHr0kna6IoD85ordHL0lnKCTop666Megl6XRlBH3D\nHr0kdVJE0De9vFKSOioj6BuejJWkTooI+umTmkmSTlVI0LdOxnrVjSSdqYigb3rDlCR1VETQ27qR\npM7KCHrnupGkjooIeqdAkKTOigh6T8ZKUmdFBH1EMFAPT8ZKUhtFBD20Tsh6Z6wknamsoHdEL0ln\nKCroRz0ZK0lnKCboBxuO6CWpnWKCfqAeBr0ktVFQ0Ne8vFKS2igq6B3RS9KZygn6hidjJamdYoK+\nWQ+vo5ekNsoJeq+6kaS2igl6e/SS1F7XQR8R9Yj4fkR8o1q+JCJ2R8SBiLgzIprdlzm7gXqNE7Zu\nJOkMCzGi/ziwf9ryZ4DPZ+Y24EXg5gV4j1k1HdFLUltdBX1EbAF+DbitWg7gWuCuapOdwE3dvMdc\ntW6Y8qobSTpdtyP6LwC/D0wNpTcARzJzvFo+CGzu8j3mxB69JLU376CPiF8HDmfm3umr22zadpgd\nETsiYk9E7BkZGZlvGScNeNWNJLXVzYj+GuA3I+LHwNdotWy+AKyNiEa1zRbgmXYvzsxbM3M4M4eH\nhoa6KKOl6RQIktTWvIM+M/8wM7dk5lbgg8C3M/PDwP3A+6rNtgP3dF3lHLSuo7dHL0mnW4zr6P8A\n+GREPE6rZ3/7IrzHGfwoQUlqrzH7JrPLzO8A36mePwG8fSF+7hsxUK8xMZlMTCb1WrtTBZK0PBV1\nZyzgCVlJOk0xQd806CWprXKCvtHaFadBkKRTFRP0a84ZAODIq2M9rkSSlpZign7j6kEAfvrKiR5X\nIklLSzFBv2F1a5LM518Z7XElkrS0FBP0J0f0xxzRS9J0xQT9upUDRMDzLxv0kjRdMUHfqNdYv7LJ\n88ds3UjSdMUEPbT69I7oJelURQX9xtWD/NQRvSSdorigf97LKyXpFEUFva0bSTpTUUG/cfUgx0Yn\nOD460etSJGnJKCzop26aclQvSVMKC/qpm6Y8IStJU4oK+g1V0Nunl6TXFRX0U60bp0GQpNcVFvTV\niN6JzSTppKKCfsVAndWDDU/GStI0RQU9VNfSO6KXpJOKC/qNqwc9GStJ0xQY9E1PxkrSNMUF/YbV\ng7ZuJGma4oJ+4+pBXnx1lPGJyV6XIklLQoFB3yQTXnjVUb0kQZFBX02DYPtGkoACg37DKic2k6Tp\nigv6jec6opek6coL+lVT0yA4opckKDDozzunQbNeY8SglySgi6CPiIsi4v6I2B8RD0fEx6v16yPi\nvog4UD2uW7hy51QXG1Y3bd1IUqWbEf048HuZ+QvA1cBHI+Jy4BZgV2ZuA3ZVy2dVa74bR/SSBF0E\nfWYeyszvVc9fBvYDm4EbgZ3VZjuBm7ot8o3auHrQEb0kVRakRx8RW4Ergd3ABZl5CFq/DIDzO7xm\nR0TsiYg9IyMjC1HGSRtWDTqil6RK10EfEauBrwOfyMyX5vq6zLw1M4czc3hoaKjbMk6x8dxWjz4z\nF/TnSlI/6iroI2KAVsjfkZl3V6ufi4hN1fc3AYe7K/GN27hqkNGJSV56bfxsv7UkLTndXHUTwO3A\n/sz83LRv3Qtsr55vB+6Zf3nzc+GaFQAcfPHVs/3WkrTkdDOivwb4LeDaiPhB9XUD8Gng+og4AFxf\nLZ9Vv/im8wB46OmjZ/utJWnJacz3hZn5f4Ho8O3r5vtzF8LWDas4d7DBvoNH+cBVvaxEknqvuDtj\nAWq14Iota3jQEb0klRn0AFdsWcP+Qy9xYnyi16VIUk8VG/S/tHktYxPJY8++0utSJKmnyg36LWsA\n2Pf0kR5XIkm9VWzQb1l3DmtXDvDgQfv0kpa3YoM+Irhi8xr2GfSSlrligx5a7ZvHnnuZ18Y8IStp\n+So66K/YvJbxyWT/oTlPwSNJxSk66KdOyHo9vaTlrOig37RmBRtXN/mbpwx6SctX0UEfEfzSlrU8\n6CWWkpaxooMe4IrNa3j88CscO+GUxZKWp+KD/u/+zDomE/7PgYX9FCtJ6hfFB/07f3YDF563gjsf\neKrXpUhSTxQf9I16jfcPb+EvHxvhmSPHe12OJJ11xQc9wD8ZvojJhLv2Hux1KZJ01i2LoL9o/Up+\n5ec2cucDTzE56QeGS1pelkXQA3zgqot4+shx/t+Pnu91KZJ0Vi2boP8Hv3gBa1cO8DVPykpaZpZN\n0A826vzjK7fwrYef5fDLr/W6HEk6a5ZN0AP806svJgj+7b0Pk2mvXtLysKyC/tKh1Xzi+m1888Fn\n+fN9h3pdjiSdFcsq6AF2/L1LeetFa/k39zxkC0fSsrDsgr5Rr/HZ97+F46MT/Ku7H7SFI6l4yy7o\nAX7u/NX8y394Gf97/2E+/T8fNewlFa3R6wJ65Z9dcwk//ukx/vN3n+DFV0f5D//oChr1Zfl7T1Lh\nlm3Q12rBv7/xzaxfNciXdh3gyKtjfP4Db2XV4LL9K5FUqGU9hI0IPnn93+FTv3E533rkOd79ub/k\nf+w7ZCtHUlGWddBP+cg1l/D133kH61Y2+eh//R6/dftfs/uJnxr4kooQSyHMhoeHc8+ePb0ug/GJ\nSe7Y/SSfu+8xjh4fY9v5q/nwL1/MDVds4vzzVvS6PEk6RUTszczhWbdbjKCPiPcAXwTqwG2Z+emZ\ntl8qQT/l+OgEf77vGe7Y/SR/81Tr82Yv33Qef/+yIa7auo4rNq9l6NzBHlcpabnrWdBHRB14DLge\nOAg8AHwoMx/p9JqlFvTT/fDZl/n2o4f5zg8Ps/cnLzJeTXO8ac0Ktl1wLpduXMXWDSvZsm4lF65Z\nwaY1K1i3skmtFj2uXFLp5hr0i3GJyduBxzPziaqQrwE3Ah2Dfim77MJzuezCc/mdd/0sx06M88ih\nl9h38CgPHjzCj0aOcddPXuSV0z54vF4L1q1ssmFVk7UrBzjvnAHWnDPA6sEGqwbrrBpssHKgzoqB\nOuc06ww26gwO1Bis12g2agycfAwatRqNejBQr1GvBY1aUKsF9QjqtaB28rF1clmSTrcYQb8ZmD4X\n8EHglxfhfc66VYMNrtq6nqu2rj+5LjMZeeUEzxx5jWePHufQ0dd4/pUTvHBslOdfGeXo8TGeeuFV\nHj4+xssnxjl2YpzF+uyTCFrBH0HE68tB65HWH2q11rqIqUeA6jXVcus7nFwHZ/4imb54ynOiw/rp\n20fb9TPv4IJs0vm1/qJsy7+VxfWx67bxG29506K+x2IEfbt/F2dEW0TsAHYAXHzxxYtQxtkREZx/\n7grOP3cFXLR21u0zkxPjkxwfneD42ASvjk5wYnyC0fFJToxPMjo+ydhE62t0IhmfmGR8IhmbnGRy\nMhmfTCamvjKZnEwmEyYmk8mc+oLJySSr95tMyITJqk13ch1JJtV2wNRydbSmvt96/vr6qe9NW2j3\n9JSrlk5d33772f7eZt1mjj9r4V9crvQvZtGtOWdg0d9jMYL+IHDRtOUtwDOnb5SZtwK3QqtHvwh1\nLEkRwYqqbbOu18VIWhYW4zr6B4BtEXFJRDSBDwL3LsL7SJLmYMFH9Jk5HhH/HPhftC6v/EpmPrzQ\n7yNJmptFmdglM78JfHMxfrYk6Y1xCgRJKpxBL0mFM+glqXAGvSQVzqCXpMItiWmKI2IE+Mk8X74R\neH4By+kH7vPy4D4vD93s889k5tBsGy2JoO9GROyZy+xtJXGflwf3eXk4G/ts60aSCmfQS1LhSgj6\nW3tdQA+4z8uD+7w8LPo+932PXpI0sxJG9JKkGfR10EfEeyLihxHxeETc0ut6FkNEXBQR90fE/oh4\nOCI+Xq1fHxH3RcSB6rGo6e0joh4R34+Ib1TLl0TE7mp/76ymwC5GRKyNiLsi4tHqWL9jGRzj363+\nTT8UEV+NiBWlHeeI+EpEHI6Ih6ata3tco+VLVZ7ti4i3LVQdfRv01YeQ/xHwXuBy4EMRcXlvq1oU\n48DvZeYvAFcDH6328xZgV2ZuA3ZVyyX5OLB/2vJngM9X+/sicHNPqlo8XwT+IjN/HngLrX0v9hhH\nxGbgY8BwZr6Z1pTmH6S84/wnwHtOW9fpuL4X2FZ97QC+vFBF9G3QM+1DyDNzFJj6EPKiZOahzPxe\n9fxlWgGwmda+7qw22wnc1JsKF15EbAF+DbitWg7gWuCuapPS9vc84FeB2wEyczQzj1DwMa40gHMi\nogGsBA5R2HHOzO8CL5y2utNxvRH402z5K2BtRGxaiDr6OejbfQj55h7VclZExFbgSmA3cEFmHoLW\nLwPg/N5VtuC+APw+MFktbwCOZOZ4tVzasb4UGAH+uGpX3RYRqyj4GGfm08BngSdpBfxRYC9lH+cp\nnY7romVaPwf9nD6EvBQRsRr4OvCJzHyp1/Usloj4deBwZu6dvrrNpiUd6wbwNuDLmXklcIyC2jTt\nVH3pG4FLgDcBq2i1Lk5X0nGezaL9O+/noJ/Th5CXICIGaIX8HZl5d7X6uan/1lWPh3tV3wK7BvjN\niPgxrXbctbRG+Gur/+JDecf6IHAwM3dXy3fRCv5SjzHAu4G/zcyRzBwD7gbeSdnHeUqn47pomdbP\nQb8sPoS86k/fDuzPzM9N+9a9wPbq+XbgnrNd22LIzD/MzC2ZuZXWMf12Zn4YuB94X7VZMfsLkJnP\nAk9FxGXVquuARyj0GFeeBK6OiJXVv/GpfS72OE/T6bjeC/x2dfXN1cDRqRZP1zKzb7+AG4DHgB8B\n/7rX9SzSPv4Krf++7QN+UH3dQKtvvQs4UD2u73Wti7Dv7wK+UT2/FPhr4HHgvwGDva5vgff1rcCe\n6jj/d2Bd6ccY+HfAo8BDwH8BBks7zsBXaZ2DGKM1Yr+503Gl1br5oyrPHqR1RdKC1OGdsZJUuH5u\n3UiS5sCgl6TCGfSSVDiDXpIKZ9BLUuEMekkqnEEvSYUz6CWpcP8fBqzMcpO/9bIAAAAASUVORK5C\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2638c6a81d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Step size = 0.2\n",
    "x, trajectory = gd.gd(lambda x: x**2, lambda x: 2*x, 10, 100, 0.2)\n",
    "print(x)\n",
    "# 0.003645900464603937\n",
    "plot(trajectory)\n",
    "show(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21734427526984595\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+QDPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2638c6980f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Step size = 0.1\n",
    "x, trajectory = gd.gd(lambda x: x**2, lambda x: 2*x, 10, 100, 0.1)\n",
    "print(x)\n",
    "plot(trajectory)\n",
    "show(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2638c6a88d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Step size = 0.5\n",
    "x, trajectory = gd.gd(lambda x: x**2, lambda x: 2*x, 10, 100, 0.5)\n",
    "print(x)\n",
    "plot(trajectory)\n",
    "show(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB4MSjAMVc5hvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2638c6dc128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Step size = 1\n",
    "x, trajectory = gd.gd(lambda x: x**2, lambda x: 2*x, 10, 100, 1)\n",
    "print(x)\n",
    "plot(trajectory)\n",
    "show(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0570252338952513e-21\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2638c710048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Step size = 1.05\n",
    "x, trajectory = gd.gd(lambda x: x**2, lambda x: 2*x, 10, 100, 1.05)\n",
    "print(x)\n",
    "plot(trajectory)\n",
    "show(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.970721293692096e-23\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wHqqhbNV5WOdQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2638c771470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Step size = 1.1\n",
    "x, trajectory = gd.gd(lambda x: x**2, lambda x: 2*x, 10, 100, 1.1)\n",
    "print(x)\n",
    "plot(trajectory)\n",
    "show(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.107243088758273e-11\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEDCAYAAAAVyO4LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njIJbRKTM/H8ya2L2xoSiQQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2638db79cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Step size = 5\n",
    "x, trajectory = gd.gd(lambda x: x**2, lambda x: 2*x, 10, 100, 6)\n",
    "print(x)\n",
    "plot(trajectory)\n",
    "show(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It's now found a value close to zero and you can see that the\n",
    "objective is decreasing by looking at the plot.\n",
    "\n",
    "<img src=\"data/kitten.jpeg\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br>\n",
    "## WU3 (5%):\n",
    "Find a few values of step size where it converges and\n",
    "a few values where it diverges.  Where does the threshold seem to\n",
    "be?\n",
    "\n",
    "[WU3 Answer: As from the above graphs with different step size values, it can be inferred that if step size is less than or equal to 1 the objective decreases continuously i.e; we get pure convergence. But on increasing the step size value from 1, in the start objective increases and then decreases. So step size more than 1 introduces divergence as we can see in the graph of step sizes 1.05 and 1.1. \n",
    "So, few values of step size where it converges can be 0.2, 0.3, 0.4, 0.5 and few values of step size where it diverges can be 1.1, 1.2, 4, 6. And step size = 1  would become threshold here, step size more than this introduces divergence.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/kitten.jpeg\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br>\n",
    "## WU4 (10%):\n",
    "Come up with a *non-convex* univariate\n",
    "optimization problem.  Plot the function you're trying to minimize and\n",
    "show two runs of `gd`, one where it gets caught in a local\n",
    "minimum and one where it manages to make it to a global minimum.  (Use\n",
    "different starting points to accomplish this.)\n",
    "\n",
    "If you implemented it well, this should work in multiple dimensions,\n",
    "too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2638c6c7a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_random_numbers_in_range(n, r_start, r_end):\n",
    "    random_numbers = list()\n",
    "    for i in range(n):\n",
    "        random_numbers.append(random.uniform(r_start, r_end))\n",
    "    return random_numbers\n",
    "\n",
    "X = generate_random_numbers_in_range(200, -2, 2) \n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "f = lambda x: sin(4* x**2) + 2 * x**2 - x\n",
    "Y = []\n",
    "for i in range(len(X)): \n",
    "    Y.append(f(X[i]))\n",
    "\n",
    "pylab.xlabel('x')\n",
    "pylab.ylabel('func(x): sin(4* x**2) + 2 * x**2 - x')\n",
    "pylab.plot(X, Y, 'bo')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.26680553559e+18\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEDCAYAAAAVyO4LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHBxJREFUeJzt3X2UXHd93/HPd+7MPmitR2tlg2UhP2FD3AM2W8dP8QED\njSHEbnvgHChJaY8bNSUh0NJSOEl7StPTnnDy3OOQ6gAJNARqHExcJzjhgH0c+4BhhS0jWVKwjY1V\nP2iFvJKsXe3cuffbP+be2dnZO7ujmblXmtH7dc4eaVZ3d3+j2f3oq+/v97s/c3cBAAZH6XQPAABw\naghuABgwBDcADBiCGwAGDMENAAOG4AaAAZNbcJvZ58zskJntOYWPebeZuZlNJY+vMbPHkrfdZvZP\n8hovAAwKy2sdt5ndJOkVSV9w9ys7uH6tpL+SNCLpV9192szWSKq6e83MXiVpt6RXu3stl0EDwADI\nreJ29wclHWl+n5ldYmb3mdkuM/s7M7ui6Y9/U9KnJJ1s+hxzTSE9JondQgDOekX3uHdK+pC7v0nS\nv5f0R5JkZldJutDd7239ADP7aTPbK+kHkn6ZahvA2a5c1Bcys3MkXS/pK2aWvnvUzEqSfk/Sv8j6\nOHd/RNJPmdnrJH3ezL7u7iezrgWAs0Fhwa16dT/r7m9sfqeZrZd0paQHkkA/X9I9Znaru0+n17n7\nPjM7kVw7LQA4SxXWKnH3Y5J+ZGbvkSSre4O7H3X3ze6+3d23S/qOpFuTycmLzKycXP8aSZdLeqao\nMQPAmSjP5YBfkvRtSZeb2UEzu13S+yXdbma7Je2VdNsqn+ZGSbvN7DFJd0v6oLsfzmvMADAIclsO\nCADIBzsnAWDA5DI5uXnzZt++fXsenxoAhtKuXbsOu/tkJ9fmEtzbt2/X9DQLPwCgU2b2bKfX0ioB\ngAFDcAPAgCG4AWDAENwAMGAIbgAYMAQ3AAwYghsABgzBDRSoWot15/Rz4lYT6AXBDRTo4ScP62N3\nPa69zx873UPBACO4gQKdDCNJ0kItPs0jwSDrKLjNbIOZ3WVm+81sn5ldl/fAgGEUxvUWSS0iuNG9\nTu9V8geS7nP3d5vZiKQ1OY4JGFphUmlHMT1udG/V4DazdZJuUnImpLtXJVXzHRYwnMKk0q4R3OhB\nJ62SiyXNSPoTM3vUzD5jZhOtF5nZDjObNrPpmZmZvg8UGAaNVklMqwTd6yS4y5KulvRpd79K0glJ\nH2+9yN13uvuUu09NTnZ0S1ngrJO2SmoRFTe610lwH5R00N0fSR7fpXqQAzhFaaVNjxu9WDW43f1F\nSc+Z2eXJu94q6YlcRwUMqTCptEOCGz3odFXJhyR9MVlR8rSkf5nfkIDhlU5ORvS40YOOgtvdH5M0\nlfNYgKGXBndIjxs9YOckUKA0sOlxoxcEN1Ag1nGjHwhuoECN4GbLO3pAcAMFqtEqQR8Q3ECBqrRK\n0AcEN1CgdHKSVgl6QXADBapRcaMPCG6gQIuTkwQ3ukdwAwVqtEqouNEDghsoEFve0Q8EN1CgtEXC\nlnf0guAGClSNuK0rekdwAwViyzv6geAGClRjHTf6gOAGChTSKkEfENxAgcJkNQkn4KAXBDdQoLCW\n3mSKVgm6R3ADBUoPC2bnJHpBcAMFqtZYVYLeEdxAgdjyjn4guIECpa0SetzoBcENFMTdGxU3W97R\nC4IbKEhze4R13OhFuZOLzOwZScclRZJq7j6V56CAYRQ27ZZk5yR60VFwJ97i7odzGwkw5NI13BKT\nk+gNrRKgIGHThCStEvSi0+B2SX9rZrvMbEfWBWa2w8ymzWx6ZmamfyMEhkRzqySkVYIedBrcN7j7\n1ZLeIelXzOym1gvcfae7T7n71OTkZF8HCQyD5t2SVNzoRUfB7e7PJ78eknS3pGvyHBQwjNJDFCqB\n0eNGT1YNbjObMLO16e8l/SNJe/IeGDBs0op7rBJwrxL0pJNVJedJutvM0uv/3N3vy3VUwBBK+9rj\nlYCKGz1ZNbjd/WlJbyhgLMBQS1sl4yOBjs6Hp3k0GGQsBwQKkrZHxiuBIlol6AHBDRQkbZWM0SpB\njwhuoCBLe9ys40b3CG6gIOkdAcdHqLjRG4IbKEitqeJ2l2LCG10iuIGCVJt63NLSe5cAp4LgBgqy\n2Cqp/9ix7R3dIriBgjS3SiROwUH3CG6gIGFLcFNxo1sEN1CQtMIeTYKbJYHoFsENFKS14uZGU+gW\nwQ0UJGy6V4lEqwTdI7iBgoSN27rWf+zYhINuEdxAQcIoViUwVYIkuDm+DF0iuIGC1GJXuVRSuWSN\nx0A3CG6gINVaveIOSmnFTXCjOwQ3UJBaHKsSlFQOrPEY6AbBDRQkrHk9uJNWCatK0C2CGyhIGMUq\nB6YgCW62vKNbBDdQkDB2jQSlxqoSKm50i+AGChLW6j3uoESPG70huIGC1OJ6q6TCqhL0iOAGClKN\nvKXiJrjRnY6D28wCM3vUzO7Nc0DAsAqTddwsB0SvTqXi/rCkfXkNBBh2jXXcLAdEjzoKbjPbKunn\nJH0m3+EAw6saucpBSWV63OhRpxX370v6mKS2/7czsx1mNm1m0zMzM30ZHDBMalGskcAU0CpBj1YN\nbjN7l6RD7r5rpevcfae7T7n71OTkZN8GCAyL+t0BS6owOYkedVJx3yDpVjN7RtKXJd1sZn+W66iA\nIVRLWiUBPW70aNXgdvdPuPtWd98u6b2SvuXuv5D7yIAhU43SVSX1Hzu2vKNbrOMGChJGsSql5lUl\n9LjRnfKpXOzuD0h6IJeRAEOuFrkqZW4yhd5RcQMFqUaxyiVuMoXeEdxAQWqRa6RcUlJws6oEXSO4\ngYKkhwWbmcol47BgdI3gBgrg7o3DgiWpHBitEnSN4AYKkE5EjpST4C6VaJWgawQ3UIAwaYukSwHL\nAa0SdI/gBgqQ3lAqXVFSLhkVN7pGcAMFqCbVdSW5wVRQMu4OiK4R3EAB0jsBLlbc9LjRPYIbKEBY\na2mVBMaWd3SN4AYKkLZKyk2tkpCKG10iuIECpK2SkaTirpRKiuhxo0sEN1CAtFWS3tI1YFUJekBw\nAwUI46WrSiqBcXQZukZwAwUIa0tXlQQltryjewQ3UIC0LdK8HDBk5yS6RHADBWjdgMNNptALghso\nQFarhMlJdIvgBgqwvFXClnd0j+AGChC2bMApB2x5R/cIbqAAjftxN1XcbHlHtwhuoACZFTetEnSJ\n4AYKUIta7w7I5CS6t2pwm9mYmX3XzHab2V4z+2QRAwOGSbXlIIWAw4LRg3IH1yxIutndXzGziqSH\nzOzr7v6dnMcGDI0wytryTsWN7qwa3O7ukl5JHlaSN77jgFPQ2iphyzt60VGP28wCM3tM0iFJ33D3\nR/IdFjBc0lZJ47BgtryjBx0Ft7tH7v5GSVslXWNmV7ZeY2Y7zGzazKZnZmb6PU5goNWiWJXAZJYG\nNxU3undKq0rcfVbSA5Juyfizne4+5e5Tk5OTfRoeMBzCKFa5tPjjFtDjRg86WVUyaWYbkt+PS3qb\npP15DwwYJmHkjYlJqX4CDsGNbnWyquRVkj5vZoHqQX+nu9+b77CA4RJGsUbKTRV30ipx90b7BOhU\nJ6tKHpd0VQFjAYZWa6sknaSsxUsrcUmKY9cfP/iUfuHa12jdWKXQcWIwsHMSKEAtclXKiwGdnj2Z\nNUF54KXj+tR9B/TAASb5kY3gBgpQjWJV2lTcrU4s1CRJJ6tRMYPDwCG4gQLUIm9svpEWbzaVte19\nLgnshRrBjWwEN1CAMIobYS2tXHHPVZOKO2SDDrIR3EABqlG8pOIOSu173GnFfTKk4kY2ghsoQC3y\nxiEK0mKrJGvb+4lGq4SKG9kIbqAA7VolmRV3OjlJxY02CG6gAGHsLa2SlXrcVNxYGcENFCCsxUu3\nvCchnnV82eLkJBU3shHcQAFqcevkpDXe36oxOUnFjTYIbqAAYeSN3ZLS4kk42RV30iqh4kYbBDdQ\ngGpLqyRdDrjiOm4qbrRBcAMFqMXx0uWAK60qoeLGKghuoAD1VknGzsmsddwLVNxYGcENFCBs2TnZ\nuFcJFTe6QHADBWgN7k62vLOOG+0Q3EABWo8uS1slWVveuVcJVkNwAzmLY1cU+9ITcIKVJifZgIOV\nEdxAzsJkk03zmZNpiIctwR3HrvmQVglWRnADOUs32WS1SqKWnZMna5Hc69eeDCO5cxI8liO4gZyl\nfezmVkljy3vLzskTC/Vqe9PEiGLPXnUCENxAzsK04i43b3nP3jk5n0xMblwzIok+N7IR3EDO0oq7\nUmre8p69jvtEMjG5aSINbvrcWG7V4DazC83sfjPbZ2Z7zezDRQwMGBaN4M7a8t6yHDBdCrgxCW4O\nDEaWcgfX1CR91N2/b2ZrJe0ys2+4+xM5jw0YCmmrZMmW9zY7J9OlgJvWUHGjvVUrbnd/wd2/n/z+\nuKR9ki7Ie2DAsEgr7qU3mcrucVNxoxOn1OM2s+2SrpL0SB6DAYbR4nLAjHuVLGuVpBV3RRIVN7J1\nHNxmdo6kv5D0EXc/lvHnO8xs2symZ2Zm+jlGYKBV0+WAzffjtnatkpaKm1UlyNBRcJtZRfXQ/qK7\nfzXrGnff6e5T7j41OTnZzzECAy2rVVIqmUq2fMv7XNM6bondk8jWyaoSk/RZSfvc/XfzHxIwXGqN\nycmlP27loNSYuEylywFZx42VdFJx3yDpFyXdbGaPJW/vzHlcwNBYXA5oS95fLtmyLe/z1Uij5ZIm\nRusLvk4yOYkMqy4HdPeHJNlq1wHIlrWOW6pvwsnagDMxWtZosstygclJZGDnJJCzMGNVSfq49V4l\nc9VI45VAY5VAEq0SZCO4gZylS/zGkzBOZVXccwuRJkaDxYqbyUlkILiBnB2dDyVJGyYqS95fKdny\nddxhpPGRclPFTXBjOYIbyNnLc1UFJdPa0aVTSkFgGcsBa5oYCRSUrH5PbiYnkYHgBnI2Oxdq/XhF\nZq2rSkoZk5OR1ozUA360HDA5iUwEN5Cz2flQG8Yry95fLplqy5YD1rRmpN4mGauUqLiRieAGcnZ0\nLtT6NcuDOyjZ8hNwqvXJSYmKG+0R3EDOZuerjZ2QzcoZPe75aqTxStIqoeJGGwQ3kLPZuXatktKS\nU97dPdmAk7RKygE3mUImghvIWbtWSeuW94VaLHdpPOlxj1ZKrONGJoIbyFEYxTq+UNOG8exWSfNN\npk4s1DfqTCSrSsbKATsnkYngBnJ0LN18k1lxl5b0uNN7cS9ZVcLkJDIQ3ECOZlcI7tYt74vB3bSO\nm8lJZCC4gRzNztWDe33G5GQlWLrlPb0X95pRKm6sjOAGcjQ7V5UkbchYDhiUli4HnE8r7krTOm4q\nbmQguIEcpRV3u+WAza2SxuRkck8TKm60Q3ADOVqpx11uaZXMJytIxhuTk6wqQTaCG8jR0bmqzKS1\nY6tPTp5IDgqeaExOlpK13b7sY3F2I7iBHM3O1+8MGJSWn/5XKS09AWeuZXJyNOl1swkHrQhuIEft\ntrtL9ftxZy4HbExOcgoOshHcQI5m50Otz1hRIi3f8n6iWtNIuaRycjZlegoO9ytBK4IbyNHRuWrb\nirvc0iqZr0aNXZOSOL4MbRHcQI5m58PMFSVSsqqkZXIynZiUmlslVNxYiuAGcrRSj7vcugEnrDWW\nAkpU3Ghv1eA2s8+Z2SEz21PEgIBhEcWuYydX7nGHzT3uhUgTS4K7/uPJYQpo1UnF/aeSbsl5HMDQ\nOTYfyj1716QkBaWS3KU4qbrnq9GSinu0nE5OUnFjqVWD290flHSkgLEAQ2WlXZNSvcctqVF1n6jW\nlvS4GxU3q0rQom89bjPbYWbTZjY9MzPTr08LDKzFG0y173FLavS556qR1ow2T06yAQfZ+hbc7r7T\n3afcfWpycrJfnxYYWIsVd3aPO91NWWsEd62x+Uai4kZ7rCoBcnJ0hTsDSlIl2WiTruWeW4ga292l\nplUlTE6iBcEN5GSle3FLzRV3/UZSc+HSDTiNddxMTqJFJ8sBvyTp25IuN7ODZnZ7/sMCBl/aKlk3\nVs7880qw2ONeqMWKYm8cWyZRcaO97O+oJu7+viIGAgyb2blQa8fKjXuPtApKi62S+ZaDgiVpJKDi\nRjZaJUBOjq6w3V1aXFVSi71x3mTzcsBSyTRSLlFxY5lVK24A3Xl5rqoN49n9bWlxHXctipUuHGme\nnJSksXKJihvLENxATmbnOq+4H37ysCRp08TSoB+tcGAwlqNVAuTkaHL6TTtpj/t7zxzRb923X297\n3RZde9G5S67hwGBkIbiBnMzOVVeuuJNWyX+7d5/OXz+m33nPG1VqOeJstEzFjeUIbiAHcez1ycmV\netxpSJv06fe/SeszQp6KG1kIbiAHxxdqir39fUok6dyJUUnSf731p3TlBeszrxkrB2x5xzJMTgI5\naGx3b7NrUpJe/+p1+v5/evuyCclmo1TcyEDFDeRgdj7Z7r7C5KS0fBVJKypuZCG4gRzMzq18L+5O\njVUIbixHcAM5eHmVe3F3arRc4n7cWIbgBnLwxPPHVAlMr1o/3tPnGa0E9LixDMEN5OD+A4f0D7dv\n0sRob/P/9YqbVgmWIriBPjv48pz+/qVXdPMVW3r+XGOVgHuVYBmCG+iz+w/Uz1x98+X9CO6SqlHc\nOJcSkAhuoO8e2H9I2zat0SWTEz1/rvTA4CoTlGhCcAN9dDKM9PBTh/WWyydlZqt/wCo4MBhZCG6g\nj77z9E90Moz1lj70t6XFipslgWhGcAN99MCBGY1VSrr24nNXv7gDnVTctSjWb977hHY9e6QvXxNn\nPoIb6BN317f2H9INl2xuHPTbq04ODP7Ct5/VZx/6kX7ja3vlziTm2YDgBvrk6cMn9OMjc3pzn9ok\nUn0dt9T+wOAXj57U7/ztAW0+Z0T7Xjimv/vh4b59bZy5CG6gD2pRrN/+mwOSpLdcPtm3z9uouNu0\nSj75f/eqFru+vOM6nbduVP/rwaf69rVx5iK4gR5Fsevf3blbX9/zon79na/T1o1r+va504r7ZMbk\n5Lf2v6Sv73lRv/bWy3TplnN0+40X6eEnf6IfHDzat6+PM1NHwW1mt5jZATN70sw+nveggEERxa7/\n8JXdumf38/qPt1yhX7rp4r5+/rTiXkgq7jCK9d0fHdGn7tuvj931A1265Rz90s/Uv+b7rtmmtaNl\n/fEpVt1x7Es2+FRrse5+9KBuu+Nh/dM/elhPPH+sT88G/bLqjRTMLJB0h6S3Szoo6Xtmdo+7P5H3\n4IAzyXw10jM/OaGnZ07owIvH9Ohzs3rsx7M6vlDTR9/+Wv2bN1/S96+Zriq59/EXdOf0QT3y9E90\nfKGmoGR607aN+s8//3qNJFX52rGK3n/ta7Tzwae09/mjuuL8dQpazrAMo1iHX1nQS8cWtPu5WT30\n5OFkCWOkrRvXaOvGcR148bgOHV/QpVvO0dH5ULfd8ZA+8rbX6l/fdLHKQf//kx7HrpO1SAthLDPJ\nZAoC08RI0Je18MPIVpuFNrPrJP0Xd//Z5PEnJMnd/0e7j5mamvLp6elTHszP/8+H2GiAFTV/t670\nvZv+wKfXtLuyORbSa9zrlXQUu2pxrGot1kIt1lx18XuzZNIV56/TVds26Gcum9QtV55/6k+mA4eO\nndQ1//2bkqRtm9bohkvP1U2XTer6SzdnniB/6NhJ3fip+1WtxSpZ/aCGSlBSGNWfwysLNTX/tV24\naVw3XrpZ68dH9NyROT175IS2rB3TB67frpsu26zZuVC/8Zd79FePv6ANayoaKwdK/y2I3BUlHZxy\nyRQkb/XwrWsO3vTvNHZXGLnCqP53O9/mZz4omTaMV7RuvLLkH6B+RHlea282rRnRnb98XVcfa2a7\n3H2qk2s7uXXZBZKea3p8UNJPZ3zRHZJ2SNK2bds6+drLXDI5oWrERgOszJp/dLN+ilt/Km3JL9mX\nef0CUz1sApNKJVO5ZBqrBBqrBFo7WtZFkxO6aHP9bc1I/if/bVk3prs/eL02nzOqCzet3jvfsm5M\nX/vgDZp+9ogOH1/QzCsLimLXSLmkcqmk9eMVnbduTFvWjuq1563VtnNX/pwbJ0Z0xz+7Wu/6By/o\nwR/OKI6l2F0uKTBrnEofx65a7IrieMk/gKn69fW/08BMlXJJI0FJlcA0PlLWeCVo/O8i9vpk77GT\noWbnQh2dDxufy/sYudbDPwEuz/z4tWPFnAbZScX9Hkk/6+7/Knn8i5KucfcPtfuYbituADhbnUrF\n3UnD6qCkC5seb5X0fDcDAwD0rpPg/p6ky8zsIjMbkfReSffkOywAQDurNmTcvWZmvyrpbyQFkj7n\n7ntzHxkAIFNHnXR3/2tJf53zWAAAHWDnJAAMGIIbAAYMwQ0AA4bgBoABs+oGnK4+qdmMpGe7/PDN\nks62mwrznIff2fZ8JZ7zqXqNu3d0T+BcgrsXZjbd6e6hYcFzHn5n2/OVeM55olUCAAOG4AaAAXMm\nBvfO0z2A04DnPPzOtucr8Zxzc8b1uAEAKzsTK24AwAoIbgAYMGdMcJ8NBxKb2YVmdr+Z7TOzvWb2\n4eT9m8zsG2b2w+TXjad7rP1mZoGZPWpm9yaPLzKzR5Ln/H+SWwYPDTPbYGZ3mdn+5PW+bthfZzP7\nt8n39R4z+5KZjQ3b62xmnzOzQ2a2p+l9ma+r1f1hkmmPm9nV/RrHGRHcTQcSv0PS6yW9z8xef3pH\nlYuapI+6++skXSvpV5Ln+XFJ33T3yyR9M3k8bD4saV/T49+S9HvJc35Z0u2nZVT5+QNJ97n7FZLe\noPpzH9rX2cwukPRrkqbc/UrVbwH9Xg3f6/ynkm5peV+71/Udki5L3nZI+nS/BnFGBLekayQ96e5P\nu3tV0pcl3Xaax9R37v6Cu38/+f1x1X+YL1D9uX4+uezzkv7x6RlhPsxsq6Sfk/SZ5LFJulnSXckl\nQ/WczWydpJskfVaS3L3q7rMa8tdZ9dtEj5tZWdIaSS9oyF5nd39Q0pGWd7d7XW+T9AWv+46kDWb2\nqn6M40wJ7qwDiS84TWMphJltl3SVpEcknefuL0j1cJe05fSNLBe/L+ljktKToM+VNOvuteTxsL3e\nF0uakfQnSXvoM2Y2oSF+nd39/0n6bUk/Vj2wj0rapeF+nVPtXtfccu1MCe5OzuoeGmZ2jqS/kPQR\ndz92useTJzN7l6RD7r6r+d0Zlw7T612WdLWkT7v7VZJOaIjaIlmSvu5tki6S9GpJE6q3CloN0+u8\nmty+z8+U4D5rDiQ2s4rqof1Fd/9q8u6X0v9CJb8eOl3jy8ENkm41s2dUb4HdrHoFviH5L7U0fK/3\nQUkH3f2R5PFdqgf5ML/Ob5P0I3efcfdQ0lclXa/hfp1T7V7X3HLtTAnus+JA4qS3+1lJ+9z9d5v+\n6B5JH0h+/wFJf1n02PLi7p9w963uvl311/Vb7v5+SfdLendy2bA95xclPWdmlyfvequkJzTEr7Pq\nLZJrzWxN8n2ePuehfZ2btHtd75H0z5PVJddKOpq2VHrm7mfEm6R3Svp7SU9J+vXTPZ6cnuONqv9X\n6XFJjyVv71S95/tNST9Mft10usea0/N/s6R7k99fLOm7kp6U9BVJo6d7fH1+rm+UNJ281l+TtHHY\nX2dJn5S0X9IeSf9b0uiwvc6SvqR6Dz9UvaK+vd3rqnqr5I4k036g+oqbvoyDLe8AMGDOlFYJAKBD\nBDcADBiCGwAGDMENAAOG4AaAAUNwA8CAIbgBYMD8f7gdCdG0oQ2eAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2638dbb8ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, trajectory = gd.gd(lambda x: sin(4* x**2) + 2 * x**2 - x, lambda x: cos(4* x**2) * 8 * x + 4 *x - 1, 10, 100, 2)\n",
    "print(x)\n",
    "plot(trajectory)\n",
    "show()\n",
    "\n",
    "# gets caught in local minima with step size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0833547930341\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2638f8b3fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, trajectory = gd.gd(lambda x: sin(4* x**2) + 2 * x**2 - x, lambda x: cos(4* x**2) * 8 * x + 4 *x - 1, 10, 100, 0.2)\n",
    "print(x)\n",
    "plot(trajectory)\n",
    "show()\n",
    "\n",
    "# Manages to reach global minima with step size 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0036459   0.00182295]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2638f6282e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, trajectory = gd.gd(lambda x: linalg.norm(x)**2, lambda x: 2*x, array([10,5]), 100, 0.2)\n",
    "print(x)\n",
    "# array([ 0.0036459 ,  0.00182295])\n",
    "plot(trajectory)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our generic linear classifier implementation is\n",
    "in `linear.py`.  The way this works is as follows.  We have an\n",
    "interface `LossFunction` that we want to minimize.  This must\n",
    "be able to compute the loss for a pair `Y` and `Yhat`\n",
    "where, the former is the truth and the latter are the predictions.  It\n",
    "must also be able to compute a gradient when additionally given the\n",
    "data `X`.  This should be all you need for these.\n",
    "\n",
    "There are three loss function stubs: `SquaredLoss` (which is\n",
    "implemented for you!), `LogisticLoss` and `HingeLoss`\n",
    "(both of which you'll have to implement.  My suggestion is to hold off\n",
    "implementing the other two until you have the linear classifier\n",
    "working.\n",
    "\n",
    "The `LinearClassifier` class is a stub implemention of a\n",
    "generic linear classifier with an l2 regularizer.  It\n",
    "is *unbiased* so all you have to take care of are the weights.\n",
    "Your implementation should go in `train`, which has a handful\n",
    "of stubs.  The idea is to just pass appropriate functions\n",
    "to `gd` and have it do all the work.  See the comments inline\n",
    "in the code for more information.\n",
    " \n",
    "Once you've implemented the function evaluation and gradient, we can\n",
    "test this.  We'll begin with a very simple 2D example data set so that\n",
    "we can plot the solutions.  We'll also start with *no\n",
    "regularizer* to help you figure out where errors might be if you\n",
    "have them.  (You'll have to import `mlGraphics` to make this\n",
    "work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 0.91, test accuracy 0.86\n",
      "w=array([ 2.73466371, -0.29563932])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nir8xxkSQFX9jjImg/w/F/tYSyYqhiAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2638f452390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(linear)\n",
    "importlib.reload(gd)\n",
    "\n",
    "f = linear.LinearClassifier({'lossFunction': linear.SquaredLoss(), 'lambda': 0, 'numIter': 100, 'stepSize': 0.5})\n",
    "runClassifier.trainTestSet(f, datasets.TwoDAxisAligned)\n",
    "# Training accuracy 0.91, test accuracy 0.86\n",
    "print(f)\n",
    "# w=array([ 2.73466371, -0.29563932])\n",
    "mlGraphics.plotLinearClassifier(f, datasets.TwoDAxisAligned.X, datasets.TwoDAxisAligned.Y)\n",
    "show(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even though this data is clearly linearly separable,\n",
    "the *unbiased* classifier is unable to perfectly separate it.\n",
    "\n",
    "If we change the regularizer, we'll get a slightly different\n",
    "solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 0.9, test accuracy 0.86\n",
      "w=array([ 1.30221546, -0.06764756])\n"
     ]
    }
   ],
   "source": [
    "f = linear.LinearClassifier({'lossFunction': linear.SquaredLoss(), 'lambda': 10, 'numIter': 100, 'stepSize': 0.5})\n",
    "runClassifier.trainTestSet(f, datasets.TwoDAxisAligned)\n",
    "# Training accuracy 0.9, test accuracy 0.86\n",
    "print(f)\n",
    "# w=array([ 1.30221546, -0.06764756])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the weights are *smaller*.\n",
    "\n",
    "Now, we can try different loss functions.  Implement logistic loss and\n",
    "hinge loss.  Here are some simple test cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 0.99, test accuracy 0.86\n",
      "w=array([ 0.29809083,  1.01287561])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Mayuri_local\\UIC\\2018_Spring\\IML\\IML_HW3\\hw3\\linear.py:67: RuntimeWarning: overflow encountered in exp\n",
      "  return sum(log(1 + exp(-dot(Y, Yhat))))\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(linear)\n",
    "\n",
    "f = linear.LinearClassifier({'lossFunction': linear.LogisticLoss(), 'lambda': 10, 'numIter': 100, 'stepSize': 0.5})\n",
    "runClassifier.trainTestSet(f, datasets.TwoDDiagonal)\n",
    "# Training accuracy 0.99, test accuracy 0.86\n",
    "print(f)\n",
    "# w=array([ 0.29809083,  1.01287561])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 0.98, test accuracy 0.86\n",
      "w=array([ 1.17110065,  4.67288657])\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(linear)\n",
    "\n",
    "f = linear.LinearClassifier({'lossFunction': linear.HingeLoss(), 'lambda': 1, 'numIter': 100, 'stepSize': 0.5})\n",
    "runClassifier.trainTestSet(f, datasets.TwoDDiagonal)\n",
    "# Training accuracy 0.98, test accuracy 0.86\n",
    "print(f)\n",
    "# w=array([ 1.17110065,  4.67288657])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/kitten.jpeg\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br>\n",
    "## WU5 (5%):\n",
    "For each of the loss functions, train a model on the\n",
    "binary version of the wine data (called WineDataBinary) and evaluate\n",
    "it on the test data. You should use lambda=1 in all cases. Which works\n",
    "best? For that best model, look at the learned weights. Find\n",
    "the *words* corresponding to the weights with the greatest\n",
    "positive value and those with the greatest negative value (this is\n",
    "like LAB3). Hint: look at WineDataBinary.words to get the id-to-word\n",
    "mapping. List the top 5 positive and top 5 negative and explain.\n",
    "\n",
    "[Your WU5 answer here]\n",
    "On training model on the binary version of the wine data using SquaredLoss, LogisticLoss and HingeLoss functions, we can see that LogisticLoss function works best. \n",
    "Training and test accuracy for all three functions are as below:\n",
    "\n",
    "SquaredLoss:\n",
    "Training accuracy 0.242914979757085, test accuracy 0.31365313653136534\n",
    "\n",
    "LogisticLoss:\n",
    "Training accuracy 0.9959514170040485, test accuracy 0.974169741697417\n",
    "\n",
    "HingeLoss:\n",
    "Training accuracy 0.9959514170040485, test accuracy 0.9520295202952029\n",
    "\n",
    "Top positive and top negative weights and words corresponding to those for the best model(using LogisticLoss in this case):\n",
    "\n",
    "top_positive_w:  [ 80  84 130 135 351]\n",
    "top_negative_w:  [ 21  10   2  88 114]\n",
    "top_positive_weights_words:  ['tropical', 'acidity', 'lime', 'crisp', 'citrus']\n",
    "top_negative_weights_words:  ['tannins', 'black', 'dark', 'cherry', 'blackberry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model on the binary version of the wine data using SquaredLoss function \n",
      "\n",
      "Training accuracy 0.242914979757085, test accuracy 0.31365313653136534\n",
      "w=array([  1.51368853e+213,   6.09268891e+213,   3.21507475e+213,\n",
      "         3.16553665e+212,   1.25506088e+213,   3.04907925e+212,\n",
      "         3.17890462e+213,   7.39235111e+212,   1.64690181e+213,\n",
      "         5.76530765e+213,   4.69717950e+213,   1.18776512e+213,\n",
      "         1.06912711e+213,   2.19280051e+213,   2.96145823e+212,\n",
      "         2.26609195e+213,   1.96344768e+212,   5.33402757e+213,\n",
      "         3.50341430e+213,   1.05124361e+213,   7.75154964e+212,\n",
      "         5.43561659e+213,   7.63175716e+213,   6.78868801e+212,\n",
      "         5.88615607e+212,   8.15251160e+213,   7.25647780e+213,\n",
      "         1.57985718e+213,   5.86471628e+213,   1.01752257e+213,\n",
      "         4.13412669e+212,   9.47652114e+212,   3.09401465e+213,\n",
      "         4.65762666e+213,   6.45655930e+213,   1.20015388e+213,\n",
      "         6.39828798e+213,   4.02110473e+212,   7.73754483e+212,\n",
      "         7.07102150e+212,   4.76729706e+212,   2.09114282e+213,\n",
      "         2.43843333e+212,   1.92631210e+213,   2.41782264e+212,\n",
      "         2.48052212e+212,   2.70349443e+213,   5.13240195e+212,\n",
      "         6.53824720e+212,   4.14404234e+213,   2.13488663e+213,\n",
      "         1.33282909e+213,   3.56926778e+211,   8.59529488e+212,\n",
      "         2.49125530e+212,   3.80108435e+212,   3.86838124e+212,\n",
      "         6.05406621e+213,   1.78314556e+212,   2.89121813e+213,\n",
      "         3.30429081e+212,   1.05084101e+213,   7.27914821e+211,\n",
      "         1.68858846e+212,   2.52346021e+213,   6.92297551e+212,\n",
      "         2.30398753e+211,   2.24422783e+212,   1.21316035e+212,\n",
      "         1.76772521e+213,   5.97473007e+212,   1.16524006e+213,\n",
      "         2.01930873e+212,   2.34014595e+211,   7.59272196e+212,\n",
      "         6.07289466e+212,   3.39999121e+212,   9.20727162e+212,\n",
      "         1.46600562e+213,   2.56787778e+212,   9.31348627e+212,\n",
      "         7.07701987e+212,   2.00989442e+213,   5.58871013e+212,\n",
      "         2.18484665e+213,   9.78489442e+212,   7.00167000e+212,\n",
      "         5.16146776e+211,   3.19798450e+213,   3.97747717e+212,\n",
      "         1.59463888e+213,   2.65175789e+212,   4.17831406e-007,\n",
      "         1.11379688e+213,   1.25040686e+213,   1.27146937e+213,\n",
      "         1.31823441e+213,   1.56723181e+213,   4.53193777e+212,\n",
      "         1.29292543e+213,   3.01946637e+213,   5.08950978e+212,\n",
      "         2.28221179e+212,   8.84861130e+212,   1.74527583e+212,\n",
      "         1.43319137e+213,   1.96203041e+213,   3.31273695e+212,\n",
      "         2.87591095e+212,   3.52825442e+213,   3.62028055e+212,\n",
      "         3.65222836e+211,   9.98993902e+212,   4.11758252e+212,\n",
      "         2.40653669e+213,   7.92636293e+212,   6.82605242e+212,\n",
      "         4.72677773e+212,   7.29509376e+212,   3.24831058e+212,\n",
      "         1.92080266e+213,   1.96360436e+212,   5.03492246e+212,\n",
      "         1.35107949e+212,   2.22166733e+212,   1.96319340e+212,\n",
      "         3.46866463e+212,   2.51081463e+212,   6.64848436e+212,\n",
      "         1.75899114e+212,   9.22784590e+212,   3.40469101e+212,\n",
      "         6.50716493e+212,   2.92369084e+212,   1.21543770e+213,\n",
      "         1.15765180e+213,   6.81625372e+211,   2.22452464e+212,\n",
      "         3.41902583e+213,   1.80553502e+212,   4.02765405e+212,\n",
      "         2.89269568e+212,   4.10751984e+212,   5.70165710e+212,\n",
      "         7.70707157e+212,   2.72317080e+213,   8.43706830e+212,\n",
      "         9.20376772e+212,   2.73100249e+212,   5.85691695e+211,\n",
      "         4.11579268e+212,   1.75366781e+213,   3.54755845e+212,\n",
      "         6.77168667e+211,   8.05559024e+212,   1.07119740e+213,\n",
      "         6.78731957e+212,   1.72334996e+212,   2.13722213e+212,\n",
      "         6.36170968e+212,   2.25415794e+212,   2.12855232e+212,\n",
      "         1.39344272e+213,   1.99831054e+212,   4.87979725e+212,\n",
      "         2.77899104e+212,   1.84873481e+212,   1.87719349e+213,\n",
      "         1.71249181e+213,   1.16413643e+212,   1.99712029e+212,\n",
      "         2.49823954e+212,   6.25871488e+212,   1.28164703e+213,\n",
      "         3.06158218e+212,   6.00962610e+212,   7.72192185e+212,\n",
      "         2.64554314e+212,   3.25004272e+213,   3.47459400e+213,\n",
      "         3.40990835e+213,   3.40990835e+213,   3.73687123e+213,\n",
      "         2.67368099e+213,   2.83497843e+212,   9.70037843e+212,\n",
      "         7.27914821e+211,   1.14440864e+213,   1.44683267e+212,\n",
      "         2.93509319e+213,   3.62676466e+212,   4.55221820e+212,\n",
      "         4.31801569e+212,   2.54899141e+212,   1.11713953e+213,\n",
      "         1.21909735e+212,   4.89219100e+212,   4.45713834e+212,\n",
      "         3.27313091e+212,   2.98196915e+212,   3.42234171e+213,\n",
      "         1.50000018e+212,   5.76752289e+212,   2.61381585e+212,\n",
      "         1.46817287e+212,   2.26663212e+212,   2.08710245e+213,\n",
      "         7.02598403e+212,   4.17831406e-007,   6.31326675e+212,\n",
      "         1.34197315e+212,   2.18652674e+213,   1.41553613e+213,\n",
      "         7.29156368e+212,   7.11463094e+212,   8.12593663e+212,\n",
      "         2.03556873e+212,   1.27485451e+212,   4.17831406e-007,\n",
      "         5.39821961e+211,   2.04487801e+213,   2.92389254e+212,\n",
      "         6.35377725e+212,   5.10802451e+212,   5.87940928e+212,\n",
      "         3.23097920e+212,   6.86340298e+212,   9.69946796e+212,\n",
      "         5.17833470e+212,   6.75484838e+212,   3.83377323e+212,\n",
      "         2.43054146e+213,   3.05582979e+213,   9.05063362e+211,\n",
      "         6.33464607e+212,   6.03475567e+212,   1.45973592e+212,\n",
      "         8.03778711e+212,   2.50965067e+212,   5.07364290e+211,\n",
      "         2.29512856e+212,   7.67875704e+212,   1.10468648e+213,\n",
      "         1.14343643e+212,   9.25868154e+211,   5.01566619e+212,\n",
      "         6.21473584e+212,   3.70679874e+212,   3.60690836e+212,\n",
      "         1.93619231e+212,   4.69356649e+212,   8.35274486e+212,\n",
      "         3.36685696e+212,   3.38365424e+212,   2.72045554e+213,\n",
      "         4.61390823e+212,   5.90229625e+212,   3.79969790e+211,\n",
      "         6.48638926e+211,   1.90131740e+212,   4.17831406e-007,\n",
      "         6.21907443e+212,   1.35533544e+213,   1.22852697e+212,\n",
      "         1.21699063e+212,   2.55261938e+212,   2.14826541e+213,\n",
      "         7.27914821e+211,   4.76628813e+212,   9.66824736e+212,\n",
      "         2.65727145e+212,   5.76597162e+212,   2.70691017e+212,\n",
      "         1.97679899e+212,   1.36789623e+212,   2.33689067e+212,\n",
      "         2.88307044e+212,   3.64163730e+212,   4.17284105e+212,\n",
      "         2.09604990e+212,   9.71930426e+212,   5.51678817e+212,\n",
      "         2.87884066e+212,   3.04960401e+213,   4.17831406e-007,\n",
      "         2.95603993e+212,   5.50522994e+212,   1.20119287e+212,\n",
      "         1.29181195e+212,   2.74543693e+212,   1.09862551e+213,\n",
      "         3.18779488e+212,   1.42462879e+212,   5.07364290e+211,\n",
      "         7.99220791e+212,   2.40047626e+212,   2.59585867e+212,\n",
      "         4.17831406e-007,   3.68159899e+212,   4.86800336e+211,\n",
      "         5.00442792e+212,   3.18615226e+211,   1.06474177e+212,\n",
      "         2.99631965e+212,   1.97182587e+212,   1.05037898e+213,\n",
      "         1.53833871e+212,   5.18972574e+212,   4.61823095e+212,\n",
      "         4.57267966e+212,   1.18861174e+212,   4.16395054e+211,\n",
      "         7.55722382e+212,   1.68399170e+212,   1.51128265e+212,\n",
      "         1.61834379e+212,   5.24623658e+212,   6.23140452e+212,\n",
      "         1.38134929e+212,   4.41150869e+211,   7.92854052e+211,\n",
      "         4.19524568e+211,   4.28821811e+211,   2.77670725e+212,\n",
      "         3.75508395e+212,   4.04031487e+212,   2.17073988e+212,\n",
      "         6.31524024e+212,   4.29359518e+212,   6.31524024e+212,\n",
      "         1.01752179e+212,   4.17831406e-007,   1.37417165e+212,\n",
      "         2.40084682e+212,   1.46238787e+212,   5.58413571e+211,\n",
      "         4.17831406e-007,   5.30288483e+211,   1.52255994e+212,\n",
      "         2.12908706e+212,   5.21413039e+212,   4.96718574e+212,\n",
      "         3.48251963e+212,   2.67517523e+212,   1.96009425e+212,\n",
      "         3.21985378e+212,   1.73206863e+212,   6.61076322e+211,\n",
      "         4.22072331e+212,   1.28184011e+213,   9.24019795e+212,\n",
      "         1.39452732e+213,   2.77497474e+212,   6.30720475e+212,\n",
      "         2.55792194e+212,   2.04434705e+213,   3.48790222e+212,\n",
      "         1.72732552e+213,   4.18181949e+212,   1.25372881e+213,\n",
      "         2.69145181e+212,   1.27136049e+212,   1.85232084e+213,\n",
      "         8.98453432e+212,   2.19175195e+212,   5.80133956e+212,\n",
      "         5.67227916e+212,   2.03612866e+212,   2.67625451e+212,\n",
      "         3.73932222e+212,   1.83109708e+212,   3.68056719e+212,\n",
      "         1.97615763e+212,   7.34175023e+212,   2.07871406e+212,\n",
      "         1.50533772e+212,   1.05783008e+213,   5.64780134e+212,\n",
      "         3.55488940e+211,   5.90337590e+212,   4.57149090e+212,\n",
      "         1.26027519e+212,   1.30958913e+212,   2.48717509e+212,\n",
      "         9.77952575e+211,   1.20278552e+212,   3.33493747e+211,\n",
      "         3.50199314e+212,   1.86588615e+213,   2.61447096e+212,\n",
      "         1.14502838e+212,   1.07259588e+212,   1.06474177e+212,\n",
      "         1.06474177e+212,   2.31593982e+212,   2.62303873e+211,\n",
      "         3.55488940e+211,   7.00065598e+212,   2.11504553e+212,\n",
      "         4.82147200e+212,   5.24856629e+212,   7.09053963e+212,\n",
      "         9.26795697e+212,   7.06935812e+212,   1.04902151e+212,\n",
      "         6.14803973e+212,   3.40050774e+212,   5.54146591e+211,\n",
      "         6.92660218e+211,   7.26320545e+212,   1.39785642e+212,\n",
      "         4.17831406e-007,   2.16023523e+212,   2.37739079e+212,\n",
      "         5.98950001e+211,   8.23932650e+211,   5.72377879e+212,\n",
      "         1.00030126e+212,   2.18531593e+212,   5.85591043e+212,\n",
      "         1.35875177e+212,   3.30598128e+212,   1.25022033e+212,\n",
      "         8.20619903e+211,   2.67647325e+212,   3.39339643e+212,\n",
      "         1.29266106e+212,   2.24485899e+212,   3.55175159e+212,\n",
      "         4.04836087e+212,   1.63566865e+212,   2.50832861e+212,\n",
      "         1.00301147e+212,   1.54471272e+212,   2.15582523e+212,\n",
      "         2.01324766e+212,   3.82769624e+212,   4.04365461e+212,\n",
      "         3.20195071e+212,   2.29534750e+211,   3.67037478e+212,\n",
      "         4.41969444e+212,   4.17831406e-007,   8.21503612e+212,\n",
      "         6.03324590e+212,   1.94062355e+212,   1.35586134e+212,\n",
      "         2.18185818e+212,   2.80380307e+212,   5.65955767e+211,\n",
      "         2.03829396e+212,   1.47950282e+212,   5.52042076e+212,\n",
      "         1.02460019e+213,   2.63628349e+212,   3.84997824e+212,\n",
      "         4.23160767e+211,   2.29534750e+211,   7.84697219e+212,\n",
      "         1.64645411e+212,   6.25228205e+211,   1.78058007e+212,\n",
      "         4.24618365e+211,   2.27304068e+212,   2.08923429e+212,\n",
      "         9.23897034e+212,   1.64271127e+212,   4.17831406e-007,\n",
      "         5.25567432e+212,   1.59185434e+212,   2.76756714e+212,\n",
      "         2.30934110e+212,   9.56420826e+212,   4.80331015e+212,\n",
      "         2.08202875e+212,   4.21989579e+212,   6.79076375e+211,\n",
      "         1.02966156e+212,   2.50501448e+212,   8.09156139e+211,\n",
      "         1.17472378e+212,   3.94387040e+212,   3.40333139e+212,\n",
      "         9.40058694e+211,   1.12342635e+213,   3.13421555e+212,\n",
      "         2.96238702e+212,   2.11366392e+212,   4.45500860e+212,\n",
      "         1.48672140e+212,   7.00407801e+212,   4.01421485e+212,\n",
      "         4.10051283e+212,   1.45307081e+212,   1.34277131e+212,\n",
      "         8.55889596e+212,   1.29963271e+212,   4.83562270e+212,\n",
      "         4.67599134e+212,   1.57166838e+212,   4.75110410e+212,\n",
      "         3.48387514e+212,   3.52614227e+212,   4.63734530e+212,\n",
      "         4.16395054e+211,   9.63642780e+211,   6.82673952e+212,\n",
      "         3.83283449e+212,   3.99565389e+212,   1.86324870e+213,\n",
      "         3.44740107e+212,   2.03036781e+212,   8.07347540e+211,\n",
      "         7.05823868e+212,   1.85404510e+212,   1.34874482e+212,\n",
      "         1.40063750e+212,   4.45853101e+211,   7.39436309e+212,\n",
      "         1.34780718e+212,   3.06632684e+212,   3.53487916e+212,\n",
      "         7.16459487e+212,   7.27556124e+212,   5.58784032e+212,\n",
      "         1.70390831e+212,   5.45621121e+211,   6.46597829e+212,\n",
      "         2.66520427e+212,   7.96412857e+211,   5.37684401e+212,\n",
      "         2.46130185e+212,   4.18447193e+212,   4.17831406e-007,\n",
      "         6.79076375e+211,   6.81400843e+212,   1.82832232e+212,\n",
      "         2.85535199e+211,   4.17831406e-007,   1.26678608e+212,\n",
      "         4.17831406e-007,   2.45682010e+212,   3.47704556e+212,\n",
      "         1.05108768e+213,   7.79200465e+211,   2.48446473e+212,\n",
      "         6.93063641e+212,   3.11534845e+212,   2.08244125e+212,\n",
      "         1.07209912e+213,   9.00121877e+212,   4.17831406e-007,\n",
      "         5.76704847e+211,   5.29279867e+212,   2.21825636e+212,\n",
      "         1.27349998e+212,   1.48464339e+212,   1.58083874e+212,\n",
      "         1.65956187e+212,   2.13936506e+212,   1.41085928e+212,\n",
      "         4.17831406e-007,   6.15712516e+212,   4.17831406e-007,\n",
      "         1.09045627e+213,   2.28706588e+212,   1.00047545e+212,\n",
      "         2.63257443e+212,   1.20424139e+212,   1.21517233e+212,\n",
      "         9.46621566e+211,   2.58736627e+212,   1.32020462e+212,\n",
      "         2.56487076e+212,   3.67458432e+212,   4.37623278e+212,\n",
      "         1.90501799e+212,   7.28005673e+211,   2.53277193e+212,\n",
      "         6.94075086e+212,   2.95413368e+212,   4.63018246e+212,\n",
      "         7.53366499e+211,   4.17831406e-007,   5.07364290e+211,\n",
      "         2.69347634e+212,   4.31977190e+212,   2.92309427e+211,\n",
      "         6.77174977e+211,   4.33443353e+212,   1.82994231e+212,\n",
      "         4.15287530e+212,   4.42354772e+212,   2.06410748e+212,\n",
      "         1.13310364e+212,   1.29778083e+212,   4.61078551e+211,\n",
      "         4.02376915e+212,   3.42011168e+212,   3.46375204e+212,\n",
      "         4.82623803e+211,   2.39202026e+212,   1.75591143e+212,\n",
      "         7.41855870e+211,   4.17831406e-007,   3.20941616e+212,\n",
      "         1.06811697e+212,   4.17831406e-007,   2.95086148e+212,\n",
      "         4.17831406e-007,   7.29271132e+211,   4.14434225e+211,\n",
      "         3.46648469e+212,   3.56243668e+211,   1.00327360e+212,\n",
      "         5.60099557e+211,   3.47721159e+212,   1.39286431e+212,\n",
      "         1.58694450e+212,   2.19998967e+212,   3.36307415e+212,\n",
      "         3.97767873e+212,   1.79366731e+212,   3.94373576e+212,\n",
      "         1.05873961e+212,   7.29271132e+211,   1.61577729e+212,\n",
      "         2.88805481e+212,   1.82092527e+212,   5.56109167e+211,\n",
      "         1.27324551e+212,   1.11909790e+212,   2.89782079e+212,\n",
      "         2.32950992e+212,   1.22889317e+212,   5.54146591e+211,\n",
      "         3.27355711e+211,   1.39209748e+212,   3.54870237e+212,\n",
      "         2.19964914e+212,   4.24140019e+212,   1.32323834e+213,\n",
      "         1.51341939e+212,   6.18981584e+212,   1.96780187e+212,\n",
      "         6.17945174e+212,   5.07034293e+211,   2.30220557e+212,\n",
      "         5.68062628e+212,   1.35828241e+212,   3.59086291e+212,\n",
      "         1.14435358e+212,   1.58140880e+212,   4.07711158e+212,\n",
      "         2.58896698e+212,   2.62113047e+212,   3.92815980e+212,\n",
      "         1.19290258e+212,   1.36547167e+212,   4.59467028e+212,\n",
      "         1.55172164e+212,   2.61589716e+212,   2.61062262e+212,\n",
      "         1.19011426e+212,   4.61078551e+211,   1.80796529e+212,\n",
      "         1.97347973e+212,   4.50317822e+212,   1.33559650e+212,\n",
      "         3.07386655e+212,   5.14502287e+212,   4.82095911e+211,\n",
      "         1.70102185e+212,   1.46334167e+212,   7.99788331e+211,\n",
      "         1.02538247e+212,   2.43315776e+212,   1.64081790e+212,\n",
      "         8.64307420e+211,   1.46886494e+211,   1.46663974e+212,\n",
      "         2.77948784e+212,   2.83892932e+212,   3.95248062e+212,\n",
      "         3.07464811e+211,   1.59154370e+212,   5.12625105e+211,\n",
      "         1.50272948e+212,   2.60448704e+212,   1.22146695e+212,\n",
      "         1.09645609e+212,   4.31355978e+212,   2.21080462e+212,\n",
      "         8.74613534e+211,   4.38958176e+212,   4.38958176e+212,\n",
      "         5.30288483e+211,   2.54282191e+212,   1.06064418e+212,\n",
      "         1.63176732e+212,   4.17831406e-007,   4.17831406e-007,\n",
      "         2.29534750e+211,   2.30795168e+212,   9.38446618e+211,\n",
      "         3.54888824e+212,   2.15871629e+212,   6.77168667e+211,\n",
      "         2.53336441e+212,   2.18151744e+212,   2.19220861e+212,\n",
      "         1.44973084e+212,   1.97847515e+212,   1.97847515e+212,\n",
      "         1.56501818e+212,   1.93352757e+212,   1.65667913e+212,\n",
      "         1.46359932e+212,   6.60950205e+211,   9.17780100e+212,\n",
      "         2.09841790e+212,   8.15226445e+211,   1.84084415e+212,\n",
      "         6.16418881e+211,   1.22657400e+212,   4.28572355e+211,\n",
      "         3.07464811e+211,   3.39804478e+212,   6.15811777e+211,\n",
      "         2.28550306e+212,   3.91543913e+211,   2.29327210e+212,\n",
      "         4.59053443e+211,   4.17831406e-007,   1.32402517e+212,\n",
      "         1.05873961e+212,   1.36167903e+212,   9.41149355e+211,\n",
      "         4.17831406e-007,   4.05923845e+211,   1.70254152e+212,\n",
      "         6.62788752e+211,   3.22208215e+212,   4.19524568e+211,\n",
      "         8.32604369e+211,   8.89667248e+212,   2.35781634e+212,\n",
      "         5.11784180e+211,   4.17831406e-007,   2.46428552e+212,\n",
      "         8.57317638e+211,   4.16395054e+211,   8.55068006e+211,\n",
      "         3.87809750e+212,   4.55363274e+212,   4.05923845e+211,\n",
      "         2.06770548e+212,   2.01117602e+212,   2.04451786e+212,\n",
      "         1.88596484e+212,   4.18807632e+212,   1.22155545e+212,\n",
      "         1.79887548e+212,   1.01441495e+213,   3.06648359e+212,\n",
      "         9.81166685e+211,   5.87431582e+211,   3.25388373e+212,\n",
      "         8.30052018e+211,   4.17831406e-007,   6.70803447e+211,\n",
      "         3.43020423e+212,   2.46801251e+212,   8.85026997e+211,\n",
      "         1.80842872e+212,   2.92309427e+211,   4.66160830e+211,\n",
      "         2.40496188e+212,   1.08416237e+212,   2.34671922e+212,\n",
      "         4.17831406e-007,   3.91472405e+212,   1.06096052e+212,\n",
      "         4.56940955e+211,   2.27257289e+212,   9.43174463e+211,\n",
      "         2.04602154e+212,   4.17831406e-007,   4.23160767e+211,\n",
      "         1.20084153e+212,   1.00493878e+212,   1.57069127e+212,\n",
      "         1.41379539e+212,   2.85371855e+212,   4.17831406e-007,\n",
      "         4.17831406e-007,   1.56565621e+212,   4.17831406e-007,\n",
      "         1.21497500e+212,   3.09385582e+212,   8.72190041e+211,\n",
      "         1.50696031e+212,   1.87769692e+212,   1.49553773e+212,\n",
      "         1.65863256e+212,   4.17831406e-007,   1.47850944e+212,\n",
      "         2.09784012e+212,   4.66970336e+211,   1.74749711e+212,\n",
      "         3.05387689e+211,   1.58527471e+212,   7.26156190e+211,\n",
      "         4.90676863e+212,   4.17831406e-007,   3.79274685e+212,\n",
      "         3.01710100e+212,   1.40119723e+212,   6.32937252e+211])\n",
      "\n",
      "Training model on the binary version of the wine data using LogisticLoss function \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 0.9959514170040485, test accuracy 0.974169741697417\n",
      "w=array([ -1.98266249e-01,   1.31616904e-01,  -6.84741830e-01,\n",
      "         1.00691389e-02,   1.78779194e-02,  -3.05188612e-02,\n",
      "        -1.89976924e-02,   9.00183167e-02,  -3.39901236e-02,\n",
      "        -3.33016936e-01,  -7.72612095e-01,  -2.26272888e-01,\n",
      "        -1.27385579e-02,  -1.34272230e-01,  -3.19749239e-02,\n",
      "        -3.01115740e-01,  -2.89261751e-02,   2.26508880e-01,\n",
      "         9.46066625e-02,  -6.30563105e-02,  -1.79127039e-01,\n",
      "        -1.17217518e+00,   1.34817537e-01,  -2.17478723e-01,\n",
      "        -9.71414088e-02,  -1.61448810e-01,  -2.63977047e-01,\n",
      "        -3.96837791e-01,  -1.10080950e-01,  -1.15079371e-01,\n",
      "         4.00516151e-03,  -1.01758948e-01,  -2.10447399e-01,\n",
      "        -1.17453720e-01,  -3.80541147e-02,   1.30637262e-01,\n",
      "        -1.73913057e-01,  -1.30428096e-01,  -1.34036968e-01,\n",
      "        -1.11851824e-01,  -1.20101346e-01,  -1.20086805e-01,\n",
      "        -8.69852461e-02,   1.95872206e-01,   6.14042701e-02,\n",
      "        -2.69871023e-02,   8.72206423e-02,  -5.75638368e-04,\n",
      "         3.51500339e-01,   2.06948378e-02,  -3.67236687e-01,\n",
      "        -1.33132497e-01,   9.06207414e-02,   6.25255394e-03,\n",
      "         2.69179504e-01,  -1.28055655e-02,   1.00711301e-01,\n",
      "        -1.80672743e-01,   7.03535151e-02,  -4.74164003e-01,\n",
      "        -9.23745585e-02,  -5.01406641e-02,  -2.71616946e-03,\n",
      "        -7.18417375e-02,  -3.47789437e-01,  -1.08823345e-01,\n",
      "         4.26612909e-03,  -1.15673761e-02,  -1.37442816e-02,\n",
      "        -1.46197255e-01,  -1.54904489e-01,  -2.26201585e-01,\n",
      "        -6.73011211e-02,  -3.30462195e-02,  -2.01051707e-01,\n",
      "         4.79837059e-01,   1.69093088e-01,   9.78850212e-02,\n",
      "        -1.89719266e-02,   2.92131642e-01,   6.07572878e-01,\n",
      "        -1.55875431e-01,   4.32066617e-01,   5.85715194e-01,\n",
      "         6.92389173e-01,   2.61892395e-01,  -1.85406448e-02,\n",
      "        -1.60452617e-02,  -6.34521636e-01,  -4.28948173e-02,\n",
      "        -4.15215658e-01,  -2.20207340e-02,   4.17831406e-07,\n",
      "        -8.32942389e-02,  -2.03837689e-01,  -2.63022105e-01,\n",
      "        -1.89597827e-01,  -5.47568612e-03,   1.74633272e-03,\n",
      "        -4.66621916e-02,   1.44388911e-01,   9.66496956e-03,\n",
      "        -6.04256927e-02,  -1.58396843e-01,  -5.21121719e-02,\n",
      "        -1.69897299e-01,  -2.45907186e-01,   8.65604505e-02,\n",
      "         3.76899636e-02,  -3.29088525e-02,  -4.31478323e-02,\n",
      "        -4.21966079e-03,  -1.27675074e-01,  -3.97988857e-02,\n",
      "        -5.32339755e-01,   1.18681162e-01,  -1.05828236e-01,\n",
      "        -2.33818689e-01,  -2.25744047e-01,  -4.56059333e-02,\n",
      "        -7.57373209e-02,   2.85229150e-02,  -6.79522263e-02,\n",
      "        -2.42234920e-02,   2.90767580e-03,  -2.31264185e-03,\n",
      "         3.12203756e-01,   1.11189694e-01,   5.12128039e-01,\n",
      "        -2.49130105e-02,   7.08736400e-01,  -5.97674456e-02,\n",
      "         3.29256872e-01,   8.66717888e-02,   2.41517596e-01,\n",
      "         7.69741623e-01,  -3.28623633e-02,  -1.34619458e-02,\n",
      "        -2.45277397e-01,  -4.21646747e-02,   4.38698461e-02,\n",
      "         1.48963913e-01,  -6.44203312e-02,  -7.25962208e-02,\n",
      "        -1.30035654e-01,   2.93280504e-02,  -1.18298948e-01,\n",
      "        -1.98834166e-01,  -2.31138565e-02,   9.49614087e-02,\n",
      "        -5.81090265e-02,  -1.87783536e-02,  -1.52349828e-01,\n",
      "        -3.52354062e-03,  -6.55304619e-02,   5.47491738e-02,\n",
      "         2.37526884e-02,  -5.61916370e-02,   4.79579066e-03,\n",
      "        -5.54062104e-03,  -2.96926619e-02,  -1.16440679e-02,\n",
      "        -1.61740110e-01,  -2.39714065e-02,  -9.57069158e-02,\n",
      "        -2.60439593e-02,  -1.46190546e-02,  -3.62962797e-01,\n",
      "         5.03550091e-02,  -9.71292905e-03,  -2.61815008e-02,\n",
      "         8.01791550e-02,  -1.93627843e-01,  -2.64868791e-01,\n",
      "        -6.09305821e-02,  -5.39859361e-03,   6.29264376e-02,\n",
      "        -6.35522063e-02,  -4.68879471e-01,  -1.78354611e-01,\n",
      "        -7.92540288e-02,  -7.92540288e-02,  -2.52520383e-01,\n",
      "         8.89035528e-02,  -6.09116802e-02,  -3.75265365e-02,\n",
      "        -2.71616946e-03,   4.53571565e-01,  -6.33780907e-03,\n",
      "         4.43731999e-02,  -4.95747023e-02,  -2.25560674e-02,\n",
      "         3.25178040e-02,   2.48874859e-01,   6.29172614e-03,\n",
      "        -6.19707793e-03,   8.94278757e-02,   4.54086033e-03,\n",
      "         6.53416531e-02,   1.03632052e-02,  -1.17856307e-01,\n",
      "         8.13878464e-03,  -7.38956983e-02,   1.48434227e-01,\n",
      "         3.97542668e-02,   2.91616160e-02,  -1.77423259e-01,\n",
      "         4.06330922e-02,   4.17831406e-07,  -2.27519047e-01,\n",
      "        -4.53963157e-03,  -4.03134567e-01,  -1.92909160e-02,\n",
      "         3.18303428e-01,   6.59173567e-02,  -1.00657611e-01,\n",
      "        -1.11061256e-01,  -5.56429537e-03,   4.17831406e-07,\n",
      "        -2.19843363e-02,  -3.48986965e-01,  -5.20309243e-02,\n",
      "        -1.33245529e-01,  -1.91237667e-01,  -1.40677293e-01,\n",
      "         4.66918722e-03,   1.06277921e-01,  -4.36028052e-02,\n",
      "        -1.62216714e-01,   6.53975952e-02,   2.90279418e-02,\n",
      "        -2.73135256e-02,   2.57511748e-04,  -9.32634582e-02,\n",
      "        -2.95416809e-02,  -8.02726157e-02,   1.19924884e-01,\n",
      "        -1.13742655e-01,  -2.12633525e-02,  -5.46069027e-03,\n",
      "        -3.65045822e-02,  -1.19908570e-01,  -1.46083237e-01,\n",
      "         1.78081209e-02,   1.10934880e-01,   1.86269552e-01,\n",
      "        -4.16791426e-02,  -1.62438754e-02,  -7.84404213e-02,\n",
      "         2.88315707e-02,  -2.22042251e-02,  -1.31410668e-01,\n",
      "        -1.52064048e-02,   2.07647188e-01,  -9.54123884e-02,\n",
      "         4.52253443e-02,  -6.29426610e-02,  -1.98376846e-03,\n",
      "        -1.61558349e-02,  -5.24313318e-02,   4.17831406e-07,\n",
      "         4.57411000e-02,  -2.53205067e-01,  -7.53099942e-03,\n",
      "        -2.55082948e-02,   8.11430519e-02,   5.99001730e-02,\n",
      "        -2.71616946e-03,   7.66328358e-02,  -1.13664691e-01,\n",
      "         4.48283361e-02,   3.71431906e-02,  -5.33179872e-02,\n",
      "        -6.06913023e-03,  -1.44536612e-02,  -4.29422850e-02,\n",
      "        -6.65006355e-02,  -2.98563575e-02,  -4.53810918e-02,\n",
      "        -1.65748279e-02,  -7.07128805e-02,   6.65381093e-02,\n",
      "         1.81677550e-01,   1.02416309e-01,   4.17831406e-07,\n",
      "         3.58780556e-02,  -3.28318520e-02,   1.35940190e-02,\n",
      "         9.51753212e-02,  -1.00066685e-02,  -3.43363796e-02,\n",
      "         4.07960503e-02,   9.21700879e-02,  -5.46069027e-03,\n",
      "        -1.52963198e-02,   9.18431681e-02,  -2.07122600e-02,\n",
      "         4.17831406e-07,  -6.60090669e-04,  -1.58917292e-02,\n",
      "         1.79298656e-02,  -3.88682155e-02,  -2.25278241e-02,\n",
      "        -3.35852271e-02,  -4.46864922e-02,  -2.03939289e-01,\n",
      "         4.46291688e-02,  -1.07976312e-01,  -1.20064053e-01,\n",
      "        -5.15964279e-02,  -5.52656969e-03,   1.58773905e-02,\n",
      "        -1.24751778e-01,   6.13608508e-02,  -4.16410128e-02,\n",
      "        -3.42442469e-02,   2.11704655e-01,  -2.21211667e-01,\n",
      "        -8.07170462e-02,   8.86703340e-02,   9.12685107e-02,\n",
      "        -1.59793292e-02,  -9.79762177e-02,  -5.82473819e-02,\n",
      "         1.27442527e-01,  -5.24082295e-02,   2.43948078e-02,\n",
      "        -1.35417132e-02,  -4.79082509e-02,  -1.35417132e-02,\n",
      "        -2.82497586e-02,   4.17831406e-07,   6.62661644e-02,\n",
      "        -3.35327625e-02,  -1.24439346e-01,  -1.17779972e-02,\n",
      "         4.17831406e-07,   6.17981314e-03,  -3.82987238e-02,\n",
      "        -3.90335164e-02,  -9.60881128e-02,  -2.24719483e-02,\n",
      "        -1.09006366e-01,  -1.67667478e-01,  -1.35929240e-01,\n",
      "        -3.44048335e-02,  -2.46683755e-02,   6.15193733e-02,\n",
      "        -1.02222170e-01,  -1.46413863e-01,  -7.19690317e-03,\n",
      "         8.83498181e-01,   5.93788832e-02,  -1.22166615e-01,\n",
      "         1.01822562e-01,  -4.33716588e-01,  -6.98359795e-02,\n",
      "        -5.08634845e-01,  -7.32716767e-02,  -4.35296787e-01,\n",
      "         1.40196699e-01,  -5.00176318e-02,   2.27114110e-01,\n",
      "        -1.48484339e-01,  -1.48464815e-01,  -9.62887525e-03,\n",
      "        -1.83725625e-01,  -6.85141362e-02,  -5.25538750e-02,\n",
      "        -9.70644920e-02,  -8.07522521e-03,   1.66912636e-01,\n",
      "         1.16663708e-01,  -4.25175678e-02,  -1.74260043e-01,\n",
      "         2.33517082e-02,  -2.44019657e-01,  -1.31784068e-01,\n",
      "         9.99880443e-02,  -8.86729949e-02,  -1.14358739e-01,\n",
      "        -7.79624247e-03,  -2.53804366e-02,   1.00947059e-02,\n",
      "        -1.17381706e-01,   1.28418374e-02,  -8.45396423e-02,\n",
      "        -1.69031519e-02,  -2.84771301e-01,  -1.53324052e-02,\n",
      "        -2.29134622e-02,  -1.88117673e-02,  -2.25278241e-02,\n",
      "        -2.25278241e-02,   2.15488453e-02,  -4.33745310e-03,\n",
      "         9.99880443e-02,   1.25998856e-01,  -1.59545024e-02,\n",
      "         3.63528293e-01,   1.00490277e-01,   3.44161305e-02,\n",
      "        -1.25857885e-01,   9.15065636e-03,   7.11637831e-02,\n",
      "        -1.35682156e-01,   1.41324130e-01,   2.89980902e-02,\n",
      "         1.64776627e-01,  -1.82563860e-01,   5.40641752e-02,\n",
      "         4.17831406e-07,  -6.76061260e-02,  -9.84294305e-02,\n",
      "         2.29063493e-01,  -2.64240183e-03,   1.26888633e-01,\n",
      "        -5.99690209e-02,  -1.11759488e-02,  -1.34734049e-01,\n",
      "         9.65358408e-03,   1.42332192e-01,   5.75555317e-02,\n",
      "        -2.19781111e-03,  -3.04468339e-02,  -7.46348334e-02,\n",
      "        -1.55156104e-02,  -4.65338196e-02,   3.31267325e-02,\n",
      "        -4.13127946e-02,   3.12866834e-02,  -2.05972022e-02,\n",
      "         2.35601233e-02,  -6.25366906e-02,  -9.37672458e-02,\n",
      "         9.22639589e-02,   2.19542034e-02,  -2.72356559e-02,\n",
      "        -3.37467386e-02,   4.69714390e-02,  -5.76923907e-02,\n",
      "        -5.57236923e-02,   4.17831406e-07,  -2.35042183e-01,\n",
      "         1.48585488e-02,  -2.03693400e-02,  -9.06992031e-03,\n",
      "        -3.10990154e-02,  -6.67449517e-02,   6.16483472e-02,\n",
      "        -5.34567798e-02,   2.52207256e-02,  -6.18660731e-02,\n",
      "        -3.30113287e-02,   1.23500587e-01,   2.40658283e-02,\n",
      "        -9.13271888e-03,   4.69714390e-02,  -1.73146856e-01,\n",
      "        -5.16487738e-02,   5.10536748e-02,  -1.67379701e-02,\n",
      "        -3.16219966e-02,   9.26279116e-03,  -9.61110910e-02,\n",
      "        -1.39242532e-01,   7.87527456e-02,   4.17831406e-07,\n",
      "        -2.05090831e-01,  -7.03311060e-02,  -4.72997560e-02,\n",
      "        -1.17162957e-01,  -2.22665973e-01,  -3.50022127e-02,\n",
      "        -8.65794938e-03,  -1.57188616e-02,   3.05024890e-02,\n",
      "         4.02710539e-02,  -1.34354618e-01,  -2.78056332e-02,\n",
      "         7.80824189e-02,   2.37237592e-01,   1.66663890e-01,\n",
      "        -1.06416950e-02,  -2.48358739e-01,   4.65778569e-02,\n",
      "        -8.49261318e-02,  -8.72252546e-02,  -1.84599093e-01,\n",
      "         5.83409801e-02,   1.64665278e-01,   7.23740438e-02,\n",
      "        -4.50190388e-02,  -7.98520703e-03,  -6.02066777e-03,\n",
      "        -1.19693898e-01,   3.40325455e-02,  -6.66845895e-02,\n",
      "        -1.09078297e-01,   7.47913698e-04,  -6.29104962e-02,\n",
      "        -9.38721487e-02,  -4.74820292e-02,  -5.63441170e-02,\n",
      "         1.58773905e-02,  -9.95947058e-03,   5.06527127e-01,\n",
      "        -1.15308018e-01,   2.61097389e-02,   9.85984902e-02,\n",
      "         1.03014822e-02,  -7.45679360e-03,   1.04824310e-01,\n",
      "         4.49929219e-02,  -6.86513446e-02,  -3.27413337e-02,\n",
      "        -8.92748060e-03,  -5.83227666e-03,  -1.14296116e-02,\n",
      "         7.70984562e-02,  -7.50654195e-02,  -1.21920813e-02,\n",
      "        -1.14775471e-01,  -3.88245046e-02,  -6.05700616e-02,\n",
      "        -3.73651512e-02,  -4.00501220e-03,  -9.92195453e-02,\n",
      "         7.77635460e-02,   1.31437215e-01,  -1.88772357e-02,\n",
      "         9.85194786e-02,   8.45401900e-02,   4.17831406e-07,\n",
      "         3.05024890e-02,   3.33942136e-01,  -1.60439794e-02,\n",
      "         9.52014037e-02,   4.17831406e-07,  -3.22913564e-02,\n",
      "         4.17831406e-07,   1.45601141e-01,  -1.36556463e-02,\n",
      "        -1.17472846e-01,  -2.43744682e-02,   1.96482836e-01,\n",
      "         6.56410877e-02,  -9.72088049e-02,   1.05556346e-01,\n",
      "        -1.47517113e-01,  -1.25921242e-01,   4.17831406e-07,\n",
      "        -6.28466094e-03,   4.91603063e-01,  -4.91460790e-02,\n",
      "        -5.58324141e-02,  -2.50586544e-02,  -6.55132435e-02,\n",
      "         1.56075486e-02,  -2.82935829e-02,   4.62472453e-02,\n",
      "         4.17831406e-07,   2.96562051e-02,   4.17831406e-07,\n",
      "        -8.83109953e-02,   4.19769631e-02,   5.63496862e-03,\n",
      "        -9.39109245e-02,  -2.96591849e-02,   1.09385999e-01,\n",
      "        -2.48046674e-02,   2.34737797e-01,  -3.42699995e-02,\n",
      "        -9.11956691e-02,  -3.55912661e-02,  -1.14778137e-01,\n",
      "        -1.61508871e-01,  -1.23951522e-01,  -1.42966927e-01,\n",
      "         3.80794937e-02,   9.20259107e-03,   1.24966864e-01,\n",
      "        -2.13151278e-03,   4.17831406e-07,  -5.46069027e-03,\n",
      "        -6.88169708e-02,  -5.26978135e-02,   7.21876688e-02,\n",
      "         1.78230759e-02,  -6.97748880e-02,  -3.89399076e-02,\n",
      "        -5.46324897e-02,  -5.40608874e-02,   2.86025729e-02,\n",
      "         8.87358029e-02,  -3.37185375e-02,   2.34974392e-02,\n",
      "        -1.66847956e-02,   1.55982330e-01,  -3.48619129e-02,\n",
      "        -2.30421880e-02,  -3.39680203e-02,   3.41990687e-02,\n",
      "        -1.05201463e-02,   4.17831406e-07,   1.35394110e-02,\n",
      "        -2.72079550e-02,   4.17831406e-07,   3.18100675e-01,\n",
      "         4.17831406e-07,  -4.77100614e-03,  -1.23520178e-02,\n",
      "        -8.22334487e-02,  -5.59038799e-02,   6.02124128e-02,\n",
      "        -2.20951156e-02,  -2.66821962e-02,   1.25372253e-01,\n",
      "        -7.38490130e-02,  -6.98471800e-02,  -8.90822135e-02,\n",
      "        -1.42955366e-01,  -3.90697149e-02,  -1.37490666e-01,\n",
      "        -8.43835951e-02,  -4.77100614e-03,  -7.42830590e-02,\n",
      "        -7.39850587e-02,  -1.15484649e-01,  -3.25708894e-03,\n",
      "         5.28598566e-02,  -5.93535887e-02,  -2.23657330e-01,\n",
      "        -9.51697249e-02,   1.01635568e-01,   2.89980902e-02,\n",
      "        -2.35570054e-02,  -4.60852473e-02,  -6.45499670e-02,\n",
      "        -4.74243144e-02,  -1.27093452e-01,  -3.28078691e-01,\n",
      "         3.70853585e-03,  -6.83195606e-02,  -7.82376389e-02,\n",
      "        -8.46164943e-03,   2.22277824e-02,   1.87353059e-02,\n",
      "         5.66806361e-03,  -4.22763582e-02,   1.00323281e-01,\n",
      "        -4.46567876e-03,   9.16166110e-03,   2.29222672e-02,\n",
      "        -2.09580379e-02,   1.70453472e-02,   4.04198830e-02,\n",
      "         9.90572372e-03,  -1.23447017e-01,  -4.41409680e-02,\n",
      "         2.15220425e-02,  -4.68299113e-02,  -4.40447658e-02,\n",
      "        -3.89583120e-02,   2.34974392e-02,  -7.52789572e-02,\n",
      "        -3.25228536e-02,  -7.54460626e-02,  -1.16236818e-02,\n",
      "         7.89004242e-02,  -3.23198151e-02,  -5.58828403e-03,\n",
      "        -1.02445449e-02,  -5.39376011e-02,   1.29105748e-01,\n",
      "         1.99516990e-02,  -4.03762727e-02,   1.42416315e-01,\n",
      "        -3.94770955e-02,   1.16436806e-02,  -6.46860044e-03,\n",
      "         3.78017625e-02,  -3.43759352e-02,  -8.09623744e-02,\n",
      "        -2.13118628e-02,  -3.60669416e-02,   1.19060055e-02,\n",
      "        -5.35170600e-03,  -2.37728832e-02,  -7.40376472e-03,\n",
      "        -6.55078235e-02,  -7.02856800e-02,   2.04886212e-01,\n",
      "         3.62412690e-02,  -4.38236597e-02,  -4.38236597e-02,\n",
      "         6.17981314e-03,   1.02711575e-01,  -3.01427365e-03,\n",
      "        -5.52198652e-03,   4.17831406e-07,   4.17831406e-07,\n",
      "         4.69714390e-02,  -2.47253482e-04,  -5.58068776e-03,\n",
      "        -9.31221115e-02,  -4.66395653e-02,  -3.52354062e-03,\n",
      "        -4.28887166e-02,  -1.25306551e-02,  -1.16989447e-02,\n",
      "        -7.85558327e-02,   1.43975978e-01,   1.43975978e-01,\n",
      "         1.57091336e-01,   1.66531976e-02,   3.39756969e-02,\n",
      "        -6.41742562e-03,  -9.22418386e-03,  -3.67303756e-02,\n",
      "         5.04493885e-02,   1.41932430e-01,   6.72728526e-02,\n",
      "        -1.22698352e-02,  -5.18838851e-03,  -2.66782030e-02,\n",
      "        -2.13118628e-02,  -1.04599596e-01,   3.27426312e-02,\n",
      "         6.44116922e-02,  -4.41753177e-02,   5.49648558e-02,\n",
      "        -8.94650935e-03,   4.17831406e-07,  -1.02624123e-01,\n",
      "        -8.43835951e-02,  -1.38225718e-02,  -1.45352112e-02,\n",
      "         4.17831406e-07,  -2.31653891e-02,  -2.46298807e-02,\n",
      "         6.09837998e-02,   3.38750620e-02,  -1.59793292e-02,\n",
      "        -2.49268600e-03,   5.60798136e-01,  -2.05323235e-02,\n",
      "        -1.47957361e-02,   4.17831406e-07,   2.59179220e-02,\n",
      "         1.24897931e-01,   1.58773905e-02,  -5.23686173e-03,\n",
      "        -1.49518724e-01,   2.52943879e-01,  -2.31653891e-02,\n",
      "         7.69764145e-02,  -1.15526261e-02,   5.17560331e-04,\n",
      "         5.47038290e-02,   2.46705944e-01,  -5.92247493e-03,\n",
      "        -8.32288026e-02,  -2.04184776e-01,  -2.09237981e-02,\n",
      "         1.04513838e-01,  -3.42785028e-03,   1.77786924e-01,\n",
      "        -1.80735690e-01,   4.17831406e-07,  -8.96462786e-03,\n",
      "        -6.55213702e-02,  -1.05092219e-01,  -7.96274232e-02,\n",
      "        -2.22349113e-02,   7.21876688e-02,  -1.57518699e-02,\n",
      "         1.53651100e-04,  -8.35260850e-03,  -6.33326670e-03,\n",
      "         4.17831406e-07,  -4.89996049e-02,  -1.12347754e-02,\n",
      "        -7.08916941e-03,   1.88562804e-02,   1.79087374e-02,\n",
      "        -1.62691729e-01,   4.17831406e-07,  -9.13271888e-03,\n",
      "        -4.12605604e-02,  -2.47543938e-02,   6.44948780e-02,\n",
      "         7.12277223e-02,  -1.77160892e-02,   4.17831406e-07,\n",
      "         4.17831406e-07,  -6.33732324e-03,   4.17831406e-07,\n",
      "        -2.36578311e-02,   6.39290779e-02,  -2.13102694e-03,\n",
      "        -5.37900255e-02,  -9.88035880e-02,  -2.15893529e-02,\n",
      "         2.75763119e-02,   4.17831406e-07,  -2.05326063e-02,\n",
      "        -1.44435597e-02,   1.62599660e-02,  -2.84145304e-02,\n",
      "         6.99153403e-03,  -2.35224857e-02,   9.19650087e-02,\n",
      "        -3.87631750e-03,   4.17831406e-07,   6.68191040e-02,\n",
      "         2.04826106e-01,  -4.43619019e-02,  -1.04125448e-02])\n",
      "\n",
      "Training model on the binary version of the wine data using HingeLoss function \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 0.9959514170040485, test accuracy 0.9520295202952029\n",
      "w=array([ -8.30616352e-01,   4.39266225e-01,  -3.13353046e+00,\n",
      "         5.16897984e-02,  -3.90305469e-01,  -2.35818203e-01,\n",
      "        -3.54319104e-01,   2.91446765e-01,  -3.86189315e-01,\n",
      "        -1.44488979e+00,  -4.24458751e+00,  -1.07971853e+00,\n",
      "        -6.94578681e-02,  -1.88233920e-01,  -2.35818203e-01,\n",
      "        -1.50759492e+00,  -1.83607770e-01,   5.69969012e-01,\n",
      "         7.09368848e-01,  -3.44842555e-01,  -4.90850964e-01,\n",
      "        -5.04590808e+00,  -1.61236312e-01,  -5.95974009e-01,\n",
      "        -6.08081288e-01,  -1.41005726e+00,  -1.06361816e+00,\n",
      "        -1.53245188e+00,  -4.94102492e-01,  -6.01261309e-01,\n",
      "        -8.75070519e-02,  -6.00312008e-01,  -1.26611658e+00,\n",
      "        -3.17096435e-01,  -3.03633259e-01,   4.34795986e-01,\n",
      "        -9.75195645e-01,  -2.28284267e-01,  -5.14158268e-01,\n",
      "        -6.13127997e-01,  -4.67187562e-01,  -4.94649690e-01,\n",
      "        -2.35818203e-01,   1.61766189e+00,   4.43343485e-01,\n",
      "        -4.75152739e-02,   1.26200583e-01,  -8.22249992e-02,\n",
      "         1.45510803e+00,   2.28461706e-01,  -1.51632229e+00,\n",
      "        -4.36084189e-01,   2.37758362e-01,   1.65574987e-01,\n",
      "         8.32406477e-01,  -1.33037653e-01,   1.18296255e-01,\n",
      "        -1.40550440e+00,   2.40874615e-01,  -2.33371542e+00,\n",
      "        -3.20737729e-01,  -3.93218451e-01,  -4.71633064e-02,\n",
      "        -1.79246556e-01,  -2.21398870e+00,  -6.03034579e-01,\n",
      "         4.95639573e-02,   2.01583017e-02,  -9.43270306e-02,\n",
      "        -3.32830046e-01,  -5.89066016e-01,  -8.53469977e-01,\n",
      "        -1.88654479e-01,  -4.71633064e-02,  -8.19351858e-01,\n",
      "         1.72012789e+00,   6.59284513e-01,   4.75119880e-01,\n",
      "        -1.16830277e-01,   8.81432479e-01,   2.55213152e+00,\n",
      "        -3.64044748e-01,   2.27472394e+00,   1.96509607e+00,\n",
      "         2.85188059e+00,   8.68909464e-01,  -1.58486463e-01,\n",
      "        -4.71633064e-02,  -3.07162463e+00,  -3.72262667e-01,\n",
      "        -1.60672232e+00,  -8.21083751e-02,   4.17831406e-07,\n",
      "        -4.11472958e-01,  -1.14885422e+00,  -1.21182511e+00,\n",
      "        -7.70979848e-01,  -3.98996505e-01,   1.27821996e-01,\n",
      "        -5.37119124e-01,   4.06560804e-01,   1.94571905e-01,\n",
      "        -2.35818203e-01,  -7.49572461e-01,  -1.41490755e-01,\n",
      "        -1.05818982e+00,  -9.46859181e-01,   4.57766428e-02,\n",
      "         1.72451664e-01,  -8.05494114e-01,  -3.20052233e-01,\n",
      "        -4.21165971e-02,  -9.18040520e-01,  -2.88280792e-02,\n",
      "        -2.29463506e+00,   4.68331060e-01,  -4.45005320e-01,\n",
      "        -5.62267722e-01,  -5.14926045e-01,  -2.77935218e-01,\n",
      "        -3.84100061e-01,   5.22507964e-03,  -2.73535053e-01,\n",
      "        -1.36444046e-01,  -3.77164319e-02,   5.22507964e-03,\n",
      "         1.06383820e+00,   6.76457821e-01,   1.92957500e+00,\n",
      "        -8.68805061e-02,   2.71082628e+00,  -2.33418388e-01,\n",
      "         1.57388748e+00,   5.28324622e-01,   2.78853512e-01,\n",
      "         3.18474481e+00,  -3.53555683e-02,   9.57217929e-03,\n",
      "        -1.06806955e+00,   5.41911728e-02,  -3.74335906e-02,\n",
      "         3.31205012e-01,  -3.13291204e-01,  -2.71482306e-01,\n",
      "        -6.93000813e-01,  -3.38441383e-01,  -6.50240086e-01,\n",
      "        -8.33806490e-01,  -2.30813277e-01,   2.22205768e-01,\n",
      "        -3.25098942e-01,  -1.73458147e-01,  -1.23390634e-01,\n",
      "        -4.71633064e-02,  -2.66168962e-02,   2.25167358e-01,\n",
      "        -1.61064419e-01,  -1.74199847e-01,  -4.22332211e-02,\n",
      "        -6.15625388e-02,  -1.83607770e-01,  -1.41490755e-01,\n",
      "        -1.08476524e+00,  -1.41490755e-01,  -3.60454929e-01,\n",
      "        -1.88654479e-01,  -1.41490755e-01,  -1.55270593e+00,\n",
      "         8.67937613e-01,  -9.43270306e-02,  -1.41490755e-01,\n",
      "        -1.18581217e-03,  -6.55842458e-01,  -1.16395256e+00,\n",
      "        -2.35818203e-01,  -1.80549410e-01,   3.03100847e-01,\n",
      "        -8.71550844e-02,  -2.25055911e+00,  -8.38087544e-01,\n",
      "        -7.01531704e-01,  -7.01531704e-01,  -9.69051368e-01,\n",
      "        -1.96279806e-01,  -2.73574005e-01,   1.29152464e-01,\n",
      "        -4.71633064e-02,   2.28846833e+00,  -9.43270306e-02,\n",
      "        -4.91520732e-01,  -3.25098942e-01,  -8.08440011e-04,\n",
      "        -9.03247592e-02,   6.39956543e-01,  -4.68523484e-01,\n",
      "        -9.43270306e-02,   1.52313841e-01,   2.62612864e-01,\n",
      "         2.82934239e-01,   1.41847496e-01,  -7.86212822e-01,\n",
      "         4.09312028e-02,  -3.22973706e-01,   7.28923596e-01,\n",
      "         3.33000835e-01,   5.88848799e-03,  -9.64369120e-01,\n",
      "         1.64069691e-01,   4.17831406e-07,  -6.45837089e-01,\n",
      "        -9.43270306e-02,  -2.02106064e+00,  -4.88403958e-01,\n",
      "         1.42269207e+00,   2.52676494e-01,  -3.13172987e-02,\n",
      "        -2.26410280e-01,  -9.43270306e-02,   4.17831406e-07,\n",
      "        -4.71633064e-02,  -1.49436682e+00,  -2.35818203e-01,\n",
      "        -3.97461744e-01,  -5.51509641e-01,  -4.48178698e-01,\n",
      "        -4.75677353e-02,   1.62068131e-01,  -5.16573204e-01,\n",
      "        -2.11100778e-01,   2.50741237e-01,   2.50309661e-01,\n",
      "        -4.61662157e-01,  -2.22774137e-01,  -1.32082832e-01,\n",
      "        -1.81304580e-01,  -5.55870854e-01,   2.46954879e-01,\n",
      "        -7.39479042e-01,  -1.88654479e-01,  -4.71633064e-02,\n",
      "        -1.88654479e-01,  -5.07104140e-01,  -5.81789726e-01,\n",
      "         9.39758988e-02,   1.29576743e-01,   1.34102399e-01,\n",
      "        -3.19926742e-01,  -2.77935218e-01,  -3.15691020e-01,\n",
      "         1.06599846e-01,  -1.18911659e-01,  -5.14703253e-01,\n",
      "        -9.41490781e-02,   6.74689816e-01,  -1.51899713e-01,\n",
      "         1.06777799e-01,  -1.20118969e-01,  -4.21165971e-02,\n",
      "        -4.71633064e-02,  -1.83607770e-01,   4.17831406e-07,\n",
      "         5.06177092e-02,  -6.16639182e-01,  -9.43270306e-02,\n",
      "        -9.43270306e-02,   2.97146756e-01,  -4.41448300e-01,\n",
      "        -4.71633064e-02,   5.55665786e-02,  -5.61321993e-01,\n",
      "         2.59262147e-01,  -1.24569685e-01,  -2.30771494e-01,\n",
      "        -1.36444046e-01,  -9.43270306e-02,  -4.75152739e-02,\n",
      "        -8.57571448e-02,  -8.99268654e-02,  -3.79380094e-02,\n",
      "        -1.41490755e-01,  -2.69982697e-01,   1.09412699e-01,\n",
      "         8.18604405e-01,   4.34696443e-01,   4.17831406e-07,\n",
      "         1.54703877e-01,  -8.63188039e-02,   1.44069358e-01,\n",
      "         3.39850117e-01,   2.72927434e-03,  -1.31857061e-01,\n",
      "        -8.43502361e-02,   2.00757346e-01,  -4.71633064e-02,\n",
      "        -1.70722523e-01,   3.91460091e-01,  -1.32726132e-01,\n",
      "         4.17831406e-07,  -1.33039221e-01,  -4.71633064e-02,\n",
      "        -4.68074013e-02,  -4.71633064e-02,  -9.43270306e-02,\n",
      "        -2.77935218e-01,  -1.88654479e-01,  -9.26944313e-01,\n",
      "         1.01657767e-01,  -4.71636824e-01,  -3.03472364e-01,\n",
      "        -3.77309376e-01,  -9.43270306e-02,   1.41087162e-01,\n",
      "        -1.74715016e-01,  -2.99257671e-03,  -1.41490755e-01,\n",
      "        -1.41490755e-01,   9.31861119e-01,  -6.93598260e-01,\n",
      "        -1.84205216e-01,   1.95932754e-01,   2.87985878e-01,\n",
      "        -4.71633064e-02,  -1.32082832e-01,  -2.30771494e-01,\n",
      "         5.50228428e-01,  -1.84254314e-01,   1.78370366e-04,\n",
      "        -1.19855309e-01,  -3.90503109e-02,  -1.19855309e-01,\n",
      "        -1.41490755e-01,   4.17831406e-07,   2.35592563e-01,\n",
      "        -2.25724785e-01,  -2.22996967e-01,  -4.71633064e-02,\n",
      "         4.17831406e-07,   1.01499812e-01,  -1.41490755e-01,\n",
      "        -1.88654479e-01,  -1.85014074e-01,  -7.26615007e-02,\n",
      "        -3.72262667e-01,  -3.46736384e-01,  -2.57455644e-01,\n",
      "        -2.67841800e-01,  -1.41490755e-01,   2.42586556e-01,\n",
      "        -1.20268605e-01,  -3.57934835e-01,  -3.57724062e-03,\n",
      "         3.76986843e+00,   2.03177160e-01,  -5.18800549e-01,\n",
      "         4.41631544e-01,  -1.92677497e+00,   1.67441255e-02,\n",
      "        -1.54381795e+00,  -3.77309376e-01,  -1.30765125e+00,\n",
      "         3.85360681e-01,  -1.41490755e-01,   1.00371800e+00,\n",
      "        -7.86684549e-01,  -3.11329806e-01,   1.34279636e-02,\n",
      "        -5.94224102e-01,  -1.88654479e-01,  -2.30771494e-01,\n",
      "        -3.52761325e-01,   1.92893166e-01,   4.99484023e-01,\n",
      "         4.37321956e-01,   1.43430839e-02,  -3.16288466e-01,\n",
      "         5.17261825e-03,  -1.00917832e+00,  -5.03660421e-01,\n",
      "         1.92173084e-01,  -3.68253421e-01,  -4.57182192e-01,\n",
      "        -9.43270306e-02,  -9.43270306e-02,   1.25908975e-04,\n",
      "        -1.76319313e-01,   9.45058188e-02,  -8.93916982e-02,\n",
      "        -1.31219384e-01,  -1.04896207e+00,  -4.25210260e-02,\n",
      "        -9.43270306e-02,  -9.43270306e-02,  -9.43270306e-02,\n",
      "        -9.43270306e-02,   7.33031834e-03,  -4.21165971e-02,\n",
      "         1.92173084e-01,   1.74529838e-01,  -1.41490755e-01,\n",
      "         1.29830135e+00,  -2.74998657e-02,  -2.08803715e-01,\n",
      "        -3.24731070e-01,  -2.14535920e-01,   1.06024274e-01,\n",
      "        -4.49958375e-01,   5.59638108e-01,   1.41669543e-01,\n",
      "         3.73602579e-01,  -6.02887982e-01,   5.81220292e-02,\n",
      "         4.17831406e-07,  -2.26410280e-01,  -2.31368940e-01,\n",
      "         3.23161725e-01,  -4.71633064e-02,   5.26863814e-01,\n",
      "         1.16216269e-02,   4.40058301e-03,  -5.61515010e-01,\n",
      "         9.39758988e-02,   1.93769621e-01,   1.53763571e-01,\n",
      "        -4.71633064e-02,  -2.25724785e-01,   1.69816225e-02,\n",
      "        -9.43270306e-02,  -1.23035488e-01,   2.93009057e-01,\n",
      "        -9.16506355e-02,   1.96005213e-01,  -1.88654479e-01,\n",
      "         9.39758988e-02,  -8.11131446e-02,  -2.35818203e-01,\n",
      "         5.25924807e-01,   1.06247879e-01,  -3.15005524e-01,\n",
      "        -1.31918993e-01,   1.41087162e-01,  -3.25098942e-01,\n",
      "         2.60892369e-01,   4.17831406e-07,  -8.71562292e-01,\n",
      "         2.53689845e-02,  -1.41490755e-01,  -9.43270306e-02,\n",
      "        -1.88654479e-01,  -2.56533521e-02,   9.95000667e-02,\n",
      "        -7.77471616e-02,   4.93050308e-03,  -4.61543406e-01,\n",
      "         6.71941126e-03,   7.17256099e-01,   3.49068498e-01,\n",
      "        -4.71633064e-02,   1.41087162e-01,  -5.18072591e-01,\n",
      "        -1.41490755e-01,   9.44533574e-02,   9.58629320e-03,\n",
      "        -8.49191078e-02,   1.44940016e-02,  -2.26896350e-01,\n",
      "        -5.83744747e-01,   9.78456359e-02,   4.17831406e-07,\n",
      "        -5.89265442e-01,  -1.23318209e-01,  -2.30771494e-01,\n",
      "        -2.73574005e-01,  -9.28819435e-01,  -2.73116510e-01,\n",
      "        -3.77164319e-02,  -1.41842722e-01,   1.90703162e-01,\n",
      "         2.43168938e-01,  -3.20737729e-01,  -9.43270306e-02,\n",
      "         3.84255681e-01,   9.89447910e-01,   9.13688808e-01,\n",
      "        -9.43270306e-02,  -4.10817897e-01,   1.50821841e-01,\n",
      "        -2.82981928e-01,  -2.26896350e-01,  -4.67187562e-01,\n",
      "         1.53763571e-01,   5.93884212e-01,   1.11824508e-01,\n",
      "        -3.17652418e-01,   5.89433055e-02,   5.15643072e-02,\n",
      "        -8.38853200e-01,   4.93050308e-03,  -4.24473100e-01,\n",
      "        -4.66590115e-01,   1.78370366e-04,  -2.28943548e-01,\n",
      "        -3.77309376e-01,  -3.25098942e-01,  -4.24473100e-01,\n",
      "         1.41087162e-01,   8.76504097e-03,   2.05297354e+00,\n",
      "        -2.69476709e-01,   1.72573306e-01,  -1.61979327e-01,\n",
      "         9.35190084e-02,  -3.91962739e-02,   2.48101432e-01,\n",
      "         3.96311000e-01,  -1.88654479e-01,  -1.41490755e-01,\n",
      "        -9.43270306e-02,  -4.71633064e-02,  -3.34351000e-01,\n",
      "         5.20942273e-02,  -2.35818203e-01,  -2.35818203e-01,\n",
      "        -6.03720074e-01,  -3.23270997e-01,  -4.19426391e-01,\n",
      "        -3.80635005e-02,  -4.71633064e-02,  -4.57235992e-01,\n",
      "         3.35508708e-02,   3.33744330e-01,  -8.75070519e-02,\n",
      "         1.52097510e-01,   4.11377853e-01,   4.17831406e-07,\n",
      "         1.90703162e-01,   1.70158116e+00,  -1.41490755e-01,\n",
      "         1.46421676e-01,   4.17831406e-07,  -1.41490755e-01,\n",
      "         4.17831406e-07,   2.36176171e-01,  -8.93969454e-02,\n",
      "        -6.42573826e-01,  -8.92803213e-02,   6.04271632e-01,\n",
      "         5.39813242e-01,  -3.20649680e-01,   3.72109596e-01,\n",
      "        -7.89343271e-01,  -1.04736465e-01,   4.17831406e-07,\n",
      "        -4.71633064e-02,   1.02196216e+00,  -1.88654479e-01,\n",
      "        -1.41490755e-01,  -9.43270306e-02,   8.84984592e-03,\n",
      "         2.35010181e-01,  -1.88654479e-01,   5.61017459e-02,\n",
      "         4.17831406e-07,   1.52706771e-01,   4.17831406e-07,\n",
      "        -6.55361636e-01,   1.88428838e-01,   2.40023309e-03,\n",
      "        -2.73574005e-01,  -1.36444046e-01,   3.63344973e-01,\n",
      "        -9.43270306e-02,   4.96806850e-01,  -9.43270306e-02,\n",
      "        -2.35818203e-01,  -1.41312802e-01,  -2.71360718e-01,\n",
      "        -2.73574005e-01,  -1.63128196e-01,  -2.75096024e-01,\n",
      "         9.13205619e-02,   1.40682733e-01,   3.70446367e-01,\n",
      "        -4.71633064e-02,   4.17831406e-07,  -4.71633064e-02,\n",
      "        -2.77935218e-01,  -3.77309376e-01,   1.46369214e-01,\n",
      "         1.51017648e-01,  -1.88476527e-01,   1.22864866e-02,\n",
      "        -3.72262667e-01,  -3.72262667e-01,  -3.77839582e-02,\n",
      "         1.90594637e-01,  -1.41490755e-01,   1.41087162e-01,\n",
      "        -1.36266093e-01,   5.99932999e-01,  -2.82981928e-01,\n",
      "        -4.71633064e-02,  -1.88654479e-01,   1.41795034e-01,\n",
      "        -8.42336121e-02,   4.17831406e-07,  -5.33962457e-02,\n",
      "        -9.43270306e-02,   4.17831406e-07,   9.17975931e-01,\n",
      "         4.17831406e-07,  -4.71633064e-02,  -4.71633064e-02,\n",
      "        -2.82981928e-01,  -8.54051773e-02,   9.92579515e-02,\n",
      "        -4.71633064e-02,  -2.77977001e-01,   2.76345395e-01,\n",
      "        -2.21849641e-01,  -2.26322231e-01,  -1.32390949e-01,\n",
      "        -4.12137934e-01,  -1.41490755e-01,   1.67098200e-01,\n",
      "        -1.37041492e-01,  -4.71633064e-02,  -1.80768575e-01,\n",
      "        -2.82981928e-01,  -2.27932300e-01,  -4.71633064e-02,\n",
      "         2.35645024e-01,  -1.33604851e-01,  -2.24949706e-01,\n",
      "        -2.35818203e-01,   3.84255681e-01,   1.41669543e-01,\n",
      "        -4.71633064e-02,  -1.41490755e-01,  -3.25098942e-01,\n",
      "        -1.88654479e-01,  -3.77309376e-01,  -1.24382538e+00,\n",
      "         4.68121745e-02,  -5.03660421e-01,  -2.17838881e-01,\n",
      "        -1.31865928e-01,   1.41669543e-01,   1.03270442e-01,\n",
      "        -1.86715156e-01,  -1.41490755e-01,  -1.01195169e-01,\n",
      "        -8.92803213e-02,   1.70713596e-01,   6.26111058e-02,\n",
      "        -3.99913602e-02,   4.91662859e-02,   2.35188134e-01,\n",
      "         9.87280315e-02,  -1.83719147e-01,  -2.72024040e-01,\n",
      "         5.74355131e-02,  -2.35818203e-01,  -4.27631412e-02,\n",
      "         9.86583549e-03,   1.41087162e-01,  -1.84205216e-01,\n",
      "         5.70521895e-02,  -3.72262667e-01,  -9.43270306e-02,\n",
      "         3.35508708e-02,  -3.77309376e-01,  -4.71633064e-02,\n",
      "        -1.31397336e-01,  -1.41490755e-01,   1.29576743e-01,\n",
      "         9.39758988e-02,  -2.30771494e-01,   4.35787728e-01,\n",
      "        -9.43270306e-02,   4.95182534e-02,  -9.43270306e-02,\n",
      "         9.41013899e-02,   1.81021786e-01,  -3.30145652e-01,\n",
      "        -4.71633064e-02,   4.73420946e-02,   1.01499812e-01,\n",
      "        -3.83986833e-02,  -4.69853539e-02,  -9.43270306e-02,\n",
      "        -1.32568902e-01,  -1.34690774e-01,   3.85936019e-01,\n",
      "         9.45058188e-02,  -1.83724394e-01,  -1.83724394e-01,\n",
      "         1.01499812e-01,   3.83851253e-01,   9.44533574e-02,\n",
      "         7.17236403e-03,   4.17831406e-07,   4.17831406e-07,\n",
      "         1.41087162e-01,   4.40058301e-03,   2.40023309e-03,\n",
      "        -2.55711307e-01,  -1.88654479e-01,  -4.71633064e-02,\n",
      "        -8.71550844e-02,  -1.83607770e-01,  -1.78561061e-01,\n",
      "        -1.84205216e-01,   5.81096685e-01,   5.81096685e-01,\n",
      "         4.94463099e-01,   6.44828315e-02,   2.40344695e-01,\n",
      "         5.15643072e-02,  -4.71633064e-02,  -4.17143200e-01,\n",
      "         1.97822647e-01,   3.63292511e-01,   5.20417659e-02,\n",
      "        -4.71633064e-02,  -9.43270306e-02,  -4.71633064e-02,\n",
      "        -4.71633064e-02,  -3.25696389e-01,   1.90650701e-01,\n",
      "         4.84063769e-01,  -4.71633064e-02,   1.93763353e-01,\n",
      "        -4.71633064e-02,   4.17831406e-07,  -1.83719147e-01,\n",
      "        -1.37041492e-01,  -9.43270306e-02,  -9.43270306e-02,\n",
      "         4.17831406e-07,  -4.71633064e-02,  -1.41490755e-01,\n",
      "         2.00927295e-01,   9.89059840e-02,  -4.71633064e-02,\n",
      "        -4.71633064e-02,   2.36968467e+00,  -1.88654479e-01,\n",
      "        -4.71633064e-02,   4.17831406e-07,   1.01677765e-01,\n",
      "         2.94118603e-01,   1.41087162e-01,  -8.92803213e-02,\n",
      "        -4.04971759e-01,   1.20522079e+00,  -4.71633064e-02,\n",
      "         4.94552162e-01,  -4.23636759e-02,  -8.21083751e-02,\n",
      "         1.06599846e-01,   1.06981785e+00,  -9.43270306e-02,\n",
      "        -1.88654479e-01,  -9.62014596e-01,  -1.69796437e-01,\n",
      "         1.45493862e-01,  -4.71633064e-02,   8.68602236e-01,\n",
      "        -1.90004637e-01,   4.17831406e-07,  -4.71633064e-02,\n",
      "        -2.82981928e-01,  -2.68527295e-01,  -1.37041492e-01,\n",
      "        -1.41490755e-01,   1.46369214e-01,  -4.71633064e-02,\n",
      "         4.40058301e-03,  -9.43270306e-02,  -1.41490755e-01,\n",
      "         4.17831406e-07,  -3.30145652e-01,  -9.43270306e-02,\n",
      "        -4.71633064e-02,   1.89011220e-01,   9.39234374e-02,\n",
      "        -3.46736384e-01,   4.17831406e-07,  -4.71633064e-02,\n",
      "        -1.41490755e-01,  -9.43270306e-02,   2.00757346e-01,\n",
      "         9.78456359e-02,  -4.75677353e-02,   4.17831406e-07,\n",
      "         4.17831406e-07,  -9.43270306e-02,   4.17831406e-07,\n",
      "        -9.43270306e-02,   2.53853972e-01,  -4.71633064e-02,\n",
      "        -1.79246556e-01,  -8.08742738e-02,  -1.41490755e-01,\n",
      "         1.95475293e-01,   4.17831406e-07,  -1.36444046e-01,\n",
      "        -1.41490755e-01,   1.41087162e-01,  -1.41490755e-01,\n",
      "         5.02449661e-02,  -1.41490755e-01,   1.45009360e-01,\n",
      "        -3.50612749e-02,   4.17831406e-07,  -1.36128534e-02,\n",
      "         8.07970290e-01,  -1.41490755e-01,  -4.71633064e-02])\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(linear)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training model on the binary version of the wine data using SquaredLoss function \")\n",
    "print(\"\")\n",
    "f = linear.LinearClassifier({'lossFunction': linear.SquaredLoss(), 'lambda': 1, 'numIter': 100, 'stepSize': 0.5})\n",
    "runClassifier.trainTestSet(f, datasets.WineDataBinary)\n",
    "print(f)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training model on the binary version of the wine data using LogisticLoss function \")\n",
    "print(\"\")\n",
    "f = linear.LinearClassifier({'lossFunction': linear.LogisticLoss(), 'lambda': 1, 'numIter': 100, 'stepSize': 0.5})\n",
    "runClassifier.trainTestSet(f, datasets.WineDataBinary)\n",
    "print(f)\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training model on the binary version of the wine data using HingeLoss function \")\n",
    "print(\"\")\n",
    "f = linear.LinearClassifier({'lossFunction': linear.HingeLoss(), 'lambda': 1, 'numIter': 100, 'stepSize': 0.5})\n",
    "runClassifier.trainTestSet(f, datasets.WineDataBinary)\n",
    "print(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 0.9959514170040485, test accuracy 0.974169741697417\n",
      "w=array([ -1.98266249e-01,   1.31616904e-01,  -6.84741830e-01,\n",
      "         1.00691389e-02,   1.78779194e-02,  -3.05188612e-02,\n",
      "        -1.89976924e-02,   9.00183167e-02,  -3.39901236e-02,\n",
      "        -3.33016936e-01,  -7.72612095e-01,  -2.26272888e-01,\n",
      "        -1.27385579e-02,  -1.34272230e-01,  -3.19749239e-02,\n",
      "        -3.01115740e-01,  -2.89261751e-02,   2.26508880e-01,\n",
      "         9.46066625e-02,  -6.30563105e-02,  -1.79127039e-01,\n",
      "        -1.17217518e+00,   1.34817537e-01,  -2.17478723e-01,\n",
      "        -9.71414088e-02,  -1.61448810e-01,  -2.63977047e-01,\n",
      "        -3.96837791e-01,  -1.10080950e-01,  -1.15079371e-01,\n",
      "         4.00516151e-03,  -1.01758948e-01,  -2.10447399e-01,\n",
      "        -1.17453720e-01,  -3.80541147e-02,   1.30637262e-01,\n",
      "        -1.73913057e-01,  -1.30428096e-01,  -1.34036968e-01,\n",
      "        -1.11851824e-01,  -1.20101346e-01,  -1.20086805e-01,\n",
      "        -8.69852461e-02,   1.95872206e-01,   6.14042701e-02,\n",
      "        -2.69871023e-02,   8.72206423e-02,  -5.75638368e-04,\n",
      "         3.51500339e-01,   2.06948378e-02,  -3.67236687e-01,\n",
      "        -1.33132497e-01,   9.06207414e-02,   6.25255394e-03,\n",
      "         2.69179504e-01,  -1.28055655e-02,   1.00711301e-01,\n",
      "        -1.80672743e-01,   7.03535151e-02,  -4.74164003e-01,\n",
      "        -9.23745585e-02,  -5.01406641e-02,  -2.71616946e-03,\n",
      "        -7.18417375e-02,  -3.47789437e-01,  -1.08823345e-01,\n",
      "         4.26612909e-03,  -1.15673761e-02,  -1.37442816e-02,\n",
      "        -1.46197255e-01,  -1.54904489e-01,  -2.26201585e-01,\n",
      "        -6.73011211e-02,  -3.30462195e-02,  -2.01051707e-01,\n",
      "         4.79837059e-01,   1.69093088e-01,   9.78850212e-02,\n",
      "        -1.89719266e-02,   2.92131642e-01,   6.07572878e-01,\n",
      "        -1.55875431e-01,   4.32066617e-01,   5.85715194e-01,\n",
      "         6.92389173e-01,   2.61892395e-01,  -1.85406448e-02,\n",
      "        -1.60452617e-02,  -6.34521636e-01,  -4.28948173e-02,\n",
      "        -4.15215658e-01,  -2.20207340e-02,   4.17831406e-07,\n",
      "        -8.32942389e-02,  -2.03837689e-01,  -2.63022105e-01,\n",
      "        -1.89597827e-01,  -5.47568612e-03,   1.74633272e-03,\n",
      "        -4.66621916e-02,   1.44388911e-01,   9.66496956e-03,\n",
      "        -6.04256927e-02,  -1.58396843e-01,  -5.21121719e-02,\n",
      "        -1.69897299e-01,  -2.45907186e-01,   8.65604505e-02,\n",
      "         3.76899636e-02,  -3.29088525e-02,  -4.31478323e-02,\n",
      "        -4.21966079e-03,  -1.27675074e-01,  -3.97988857e-02,\n",
      "        -5.32339755e-01,   1.18681162e-01,  -1.05828236e-01,\n",
      "        -2.33818689e-01,  -2.25744047e-01,  -4.56059333e-02,\n",
      "        -7.57373209e-02,   2.85229150e-02,  -6.79522263e-02,\n",
      "        -2.42234920e-02,   2.90767580e-03,  -2.31264185e-03,\n",
      "         3.12203756e-01,   1.11189694e-01,   5.12128039e-01,\n",
      "        -2.49130105e-02,   7.08736400e-01,  -5.97674456e-02,\n",
      "         3.29256872e-01,   8.66717888e-02,   2.41517596e-01,\n",
      "         7.69741623e-01,  -3.28623633e-02,  -1.34619458e-02,\n",
      "        -2.45277397e-01,  -4.21646747e-02,   4.38698461e-02,\n",
      "         1.48963913e-01,  -6.44203312e-02,  -7.25962208e-02,\n",
      "        -1.30035654e-01,   2.93280504e-02,  -1.18298948e-01,\n",
      "        -1.98834166e-01,  -2.31138565e-02,   9.49614087e-02,\n",
      "        -5.81090265e-02,  -1.87783536e-02,  -1.52349828e-01,\n",
      "        -3.52354062e-03,  -6.55304619e-02,   5.47491738e-02,\n",
      "         2.37526884e-02,  -5.61916370e-02,   4.79579066e-03,\n",
      "        -5.54062104e-03,  -2.96926619e-02,  -1.16440679e-02,\n",
      "        -1.61740110e-01,  -2.39714065e-02,  -9.57069158e-02,\n",
      "        -2.60439593e-02,  -1.46190546e-02,  -3.62962797e-01,\n",
      "         5.03550091e-02,  -9.71292905e-03,  -2.61815008e-02,\n",
      "         8.01791550e-02,  -1.93627843e-01,  -2.64868791e-01,\n",
      "        -6.09305821e-02,  -5.39859361e-03,   6.29264376e-02,\n",
      "        -6.35522063e-02,  -4.68879471e-01,  -1.78354611e-01,\n",
      "        -7.92540288e-02,  -7.92540288e-02,  -2.52520383e-01,\n",
      "         8.89035528e-02,  -6.09116802e-02,  -3.75265365e-02,\n",
      "        -2.71616946e-03,   4.53571565e-01,  -6.33780907e-03,\n",
      "         4.43731999e-02,  -4.95747023e-02,  -2.25560674e-02,\n",
      "         3.25178040e-02,   2.48874859e-01,   6.29172614e-03,\n",
      "        -6.19707793e-03,   8.94278757e-02,   4.54086033e-03,\n",
      "         6.53416531e-02,   1.03632052e-02,  -1.17856307e-01,\n",
      "         8.13878464e-03,  -7.38956983e-02,   1.48434227e-01,\n",
      "         3.97542668e-02,   2.91616160e-02,  -1.77423259e-01,\n",
      "         4.06330922e-02,   4.17831406e-07,  -2.27519047e-01,\n",
      "        -4.53963157e-03,  -4.03134567e-01,  -1.92909160e-02,\n",
      "         3.18303428e-01,   6.59173567e-02,  -1.00657611e-01,\n",
      "        -1.11061256e-01,  -5.56429537e-03,   4.17831406e-07,\n",
      "        -2.19843363e-02,  -3.48986965e-01,  -5.20309243e-02,\n",
      "        -1.33245529e-01,  -1.91237667e-01,  -1.40677293e-01,\n",
      "         4.66918722e-03,   1.06277921e-01,  -4.36028052e-02,\n",
      "        -1.62216714e-01,   6.53975952e-02,   2.90279418e-02,\n",
      "        -2.73135256e-02,   2.57511748e-04,  -9.32634582e-02,\n",
      "        -2.95416809e-02,  -8.02726157e-02,   1.19924884e-01,\n",
      "        -1.13742655e-01,  -2.12633525e-02,  -5.46069027e-03,\n",
      "        -3.65045822e-02,  -1.19908570e-01,  -1.46083237e-01,\n",
      "         1.78081209e-02,   1.10934880e-01,   1.86269552e-01,\n",
      "        -4.16791426e-02,  -1.62438754e-02,  -7.84404213e-02,\n",
      "         2.88315707e-02,  -2.22042251e-02,  -1.31410668e-01,\n",
      "        -1.52064048e-02,   2.07647188e-01,  -9.54123884e-02,\n",
      "         4.52253443e-02,  -6.29426610e-02,  -1.98376846e-03,\n",
      "        -1.61558349e-02,  -5.24313318e-02,   4.17831406e-07,\n",
      "         4.57411000e-02,  -2.53205067e-01,  -7.53099942e-03,\n",
      "        -2.55082948e-02,   8.11430519e-02,   5.99001730e-02,\n",
      "        -2.71616946e-03,   7.66328358e-02,  -1.13664691e-01,\n",
      "         4.48283361e-02,   3.71431906e-02,  -5.33179872e-02,\n",
      "        -6.06913023e-03,  -1.44536612e-02,  -4.29422850e-02,\n",
      "        -6.65006355e-02,  -2.98563575e-02,  -4.53810918e-02,\n",
      "        -1.65748279e-02,  -7.07128805e-02,   6.65381093e-02,\n",
      "         1.81677550e-01,   1.02416309e-01,   4.17831406e-07,\n",
      "         3.58780556e-02,  -3.28318520e-02,   1.35940190e-02,\n",
      "         9.51753212e-02,  -1.00066685e-02,  -3.43363796e-02,\n",
      "         4.07960503e-02,   9.21700879e-02,  -5.46069027e-03,\n",
      "        -1.52963198e-02,   9.18431681e-02,  -2.07122600e-02,\n",
      "         4.17831406e-07,  -6.60090669e-04,  -1.58917292e-02,\n",
      "         1.79298656e-02,  -3.88682155e-02,  -2.25278241e-02,\n",
      "        -3.35852271e-02,  -4.46864922e-02,  -2.03939289e-01,\n",
      "         4.46291688e-02,  -1.07976312e-01,  -1.20064053e-01,\n",
      "        -5.15964279e-02,  -5.52656969e-03,   1.58773905e-02,\n",
      "        -1.24751778e-01,   6.13608508e-02,  -4.16410128e-02,\n",
      "        -3.42442469e-02,   2.11704655e-01,  -2.21211667e-01,\n",
      "        -8.07170462e-02,   8.86703340e-02,   9.12685107e-02,\n",
      "        -1.59793292e-02,  -9.79762177e-02,  -5.82473819e-02,\n",
      "         1.27442527e-01,  -5.24082295e-02,   2.43948078e-02,\n",
      "        -1.35417132e-02,  -4.79082509e-02,  -1.35417132e-02,\n",
      "        -2.82497586e-02,   4.17831406e-07,   6.62661644e-02,\n",
      "        -3.35327625e-02,  -1.24439346e-01,  -1.17779972e-02,\n",
      "         4.17831406e-07,   6.17981314e-03,  -3.82987238e-02,\n",
      "        -3.90335164e-02,  -9.60881128e-02,  -2.24719483e-02,\n",
      "        -1.09006366e-01,  -1.67667478e-01,  -1.35929240e-01,\n",
      "        -3.44048335e-02,  -2.46683755e-02,   6.15193733e-02,\n",
      "        -1.02222170e-01,  -1.46413863e-01,  -7.19690317e-03,\n",
      "         8.83498181e-01,   5.93788832e-02,  -1.22166615e-01,\n",
      "         1.01822562e-01,  -4.33716588e-01,  -6.98359795e-02,\n",
      "        -5.08634845e-01,  -7.32716767e-02,  -4.35296787e-01,\n",
      "         1.40196699e-01,  -5.00176318e-02,   2.27114110e-01,\n",
      "        -1.48484339e-01,  -1.48464815e-01,  -9.62887525e-03,\n",
      "        -1.83725625e-01,  -6.85141362e-02,  -5.25538750e-02,\n",
      "        -9.70644920e-02,  -8.07522521e-03,   1.66912636e-01,\n",
      "         1.16663708e-01,  -4.25175678e-02,  -1.74260043e-01,\n",
      "         2.33517082e-02,  -2.44019657e-01,  -1.31784068e-01,\n",
      "         9.99880443e-02,  -8.86729949e-02,  -1.14358739e-01,\n",
      "        -7.79624247e-03,  -2.53804366e-02,   1.00947059e-02,\n",
      "        -1.17381706e-01,   1.28418374e-02,  -8.45396423e-02,\n",
      "        -1.69031519e-02,  -2.84771301e-01,  -1.53324052e-02,\n",
      "        -2.29134622e-02,  -1.88117673e-02,  -2.25278241e-02,\n",
      "        -2.25278241e-02,   2.15488453e-02,  -4.33745310e-03,\n",
      "         9.99880443e-02,   1.25998856e-01,  -1.59545024e-02,\n",
      "         3.63528293e-01,   1.00490277e-01,   3.44161305e-02,\n",
      "        -1.25857885e-01,   9.15065636e-03,   7.11637831e-02,\n",
      "        -1.35682156e-01,   1.41324130e-01,   2.89980902e-02,\n",
      "         1.64776627e-01,  -1.82563860e-01,   5.40641752e-02,\n",
      "         4.17831406e-07,  -6.76061260e-02,  -9.84294305e-02,\n",
      "         2.29063493e-01,  -2.64240183e-03,   1.26888633e-01,\n",
      "        -5.99690209e-02,  -1.11759488e-02,  -1.34734049e-01,\n",
      "         9.65358408e-03,   1.42332192e-01,   5.75555317e-02,\n",
      "        -2.19781111e-03,  -3.04468339e-02,  -7.46348334e-02,\n",
      "        -1.55156104e-02,  -4.65338196e-02,   3.31267325e-02,\n",
      "        -4.13127946e-02,   3.12866834e-02,  -2.05972022e-02,\n",
      "         2.35601233e-02,  -6.25366906e-02,  -9.37672458e-02,\n",
      "         9.22639589e-02,   2.19542034e-02,  -2.72356559e-02,\n",
      "        -3.37467386e-02,   4.69714390e-02,  -5.76923907e-02,\n",
      "        -5.57236923e-02,   4.17831406e-07,  -2.35042183e-01,\n",
      "         1.48585488e-02,  -2.03693400e-02,  -9.06992031e-03,\n",
      "        -3.10990154e-02,  -6.67449517e-02,   6.16483472e-02,\n",
      "        -5.34567798e-02,   2.52207256e-02,  -6.18660731e-02,\n",
      "        -3.30113287e-02,   1.23500587e-01,   2.40658283e-02,\n",
      "        -9.13271888e-03,   4.69714390e-02,  -1.73146856e-01,\n",
      "        -5.16487738e-02,   5.10536748e-02,  -1.67379701e-02,\n",
      "        -3.16219966e-02,   9.26279116e-03,  -9.61110910e-02,\n",
      "        -1.39242532e-01,   7.87527456e-02,   4.17831406e-07,\n",
      "        -2.05090831e-01,  -7.03311060e-02,  -4.72997560e-02,\n",
      "        -1.17162957e-01,  -2.22665973e-01,  -3.50022127e-02,\n",
      "        -8.65794938e-03,  -1.57188616e-02,   3.05024890e-02,\n",
      "         4.02710539e-02,  -1.34354618e-01,  -2.78056332e-02,\n",
      "         7.80824189e-02,   2.37237592e-01,   1.66663890e-01,\n",
      "        -1.06416950e-02,  -2.48358739e-01,   4.65778569e-02,\n",
      "        -8.49261318e-02,  -8.72252546e-02,  -1.84599093e-01,\n",
      "         5.83409801e-02,   1.64665278e-01,   7.23740438e-02,\n",
      "        -4.50190388e-02,  -7.98520703e-03,  -6.02066777e-03,\n",
      "        -1.19693898e-01,   3.40325455e-02,  -6.66845895e-02,\n",
      "        -1.09078297e-01,   7.47913698e-04,  -6.29104962e-02,\n",
      "        -9.38721487e-02,  -4.74820292e-02,  -5.63441170e-02,\n",
      "         1.58773905e-02,  -9.95947058e-03,   5.06527127e-01,\n",
      "        -1.15308018e-01,   2.61097389e-02,   9.85984902e-02,\n",
      "         1.03014822e-02,  -7.45679360e-03,   1.04824310e-01,\n",
      "         4.49929219e-02,  -6.86513446e-02,  -3.27413337e-02,\n",
      "        -8.92748060e-03,  -5.83227666e-03,  -1.14296116e-02,\n",
      "         7.70984562e-02,  -7.50654195e-02,  -1.21920813e-02,\n",
      "        -1.14775471e-01,  -3.88245046e-02,  -6.05700616e-02,\n",
      "        -3.73651512e-02,  -4.00501220e-03,  -9.92195453e-02,\n",
      "         7.77635460e-02,   1.31437215e-01,  -1.88772357e-02,\n",
      "         9.85194786e-02,   8.45401900e-02,   4.17831406e-07,\n",
      "         3.05024890e-02,   3.33942136e-01,  -1.60439794e-02,\n",
      "         9.52014037e-02,   4.17831406e-07,  -3.22913564e-02,\n",
      "         4.17831406e-07,   1.45601141e-01,  -1.36556463e-02,\n",
      "        -1.17472846e-01,  -2.43744682e-02,   1.96482836e-01,\n",
      "         6.56410877e-02,  -9.72088049e-02,   1.05556346e-01,\n",
      "        -1.47517113e-01,  -1.25921242e-01,   4.17831406e-07,\n",
      "        -6.28466094e-03,   4.91603063e-01,  -4.91460790e-02,\n",
      "        -5.58324141e-02,  -2.50586544e-02,  -6.55132435e-02,\n",
      "         1.56075486e-02,  -2.82935829e-02,   4.62472453e-02,\n",
      "         4.17831406e-07,   2.96562051e-02,   4.17831406e-07,\n",
      "        -8.83109953e-02,   4.19769631e-02,   5.63496862e-03,\n",
      "        -9.39109245e-02,  -2.96591849e-02,   1.09385999e-01,\n",
      "        -2.48046674e-02,   2.34737797e-01,  -3.42699995e-02,\n",
      "        -9.11956691e-02,  -3.55912661e-02,  -1.14778137e-01,\n",
      "        -1.61508871e-01,  -1.23951522e-01,  -1.42966927e-01,\n",
      "         3.80794937e-02,   9.20259107e-03,   1.24966864e-01,\n",
      "        -2.13151278e-03,   4.17831406e-07,  -5.46069027e-03,\n",
      "        -6.88169708e-02,  -5.26978135e-02,   7.21876688e-02,\n",
      "         1.78230759e-02,  -6.97748880e-02,  -3.89399076e-02,\n",
      "        -5.46324897e-02,  -5.40608874e-02,   2.86025729e-02,\n",
      "         8.87358029e-02,  -3.37185375e-02,   2.34974392e-02,\n",
      "        -1.66847956e-02,   1.55982330e-01,  -3.48619129e-02,\n",
      "        -2.30421880e-02,  -3.39680203e-02,   3.41990687e-02,\n",
      "        -1.05201463e-02,   4.17831406e-07,   1.35394110e-02,\n",
      "        -2.72079550e-02,   4.17831406e-07,   3.18100675e-01,\n",
      "         4.17831406e-07,  -4.77100614e-03,  -1.23520178e-02,\n",
      "        -8.22334487e-02,  -5.59038799e-02,   6.02124128e-02,\n",
      "        -2.20951156e-02,  -2.66821962e-02,   1.25372253e-01,\n",
      "        -7.38490130e-02,  -6.98471800e-02,  -8.90822135e-02,\n",
      "        -1.42955366e-01,  -3.90697149e-02,  -1.37490666e-01,\n",
      "        -8.43835951e-02,  -4.77100614e-03,  -7.42830590e-02,\n",
      "        -7.39850587e-02,  -1.15484649e-01,  -3.25708894e-03,\n",
      "         5.28598566e-02,  -5.93535887e-02,  -2.23657330e-01,\n",
      "        -9.51697249e-02,   1.01635568e-01,   2.89980902e-02,\n",
      "        -2.35570054e-02,  -4.60852473e-02,  -6.45499670e-02,\n",
      "        -4.74243144e-02,  -1.27093452e-01,  -3.28078691e-01,\n",
      "         3.70853585e-03,  -6.83195606e-02,  -7.82376389e-02,\n",
      "        -8.46164943e-03,   2.22277824e-02,   1.87353059e-02,\n",
      "         5.66806361e-03,  -4.22763582e-02,   1.00323281e-01,\n",
      "        -4.46567876e-03,   9.16166110e-03,   2.29222672e-02,\n",
      "        -2.09580379e-02,   1.70453472e-02,   4.04198830e-02,\n",
      "         9.90572372e-03,  -1.23447017e-01,  -4.41409680e-02,\n",
      "         2.15220425e-02,  -4.68299113e-02,  -4.40447658e-02,\n",
      "        -3.89583120e-02,   2.34974392e-02,  -7.52789572e-02,\n",
      "        -3.25228536e-02,  -7.54460626e-02,  -1.16236818e-02,\n",
      "         7.89004242e-02,  -3.23198151e-02,  -5.58828403e-03,\n",
      "        -1.02445449e-02,  -5.39376011e-02,   1.29105748e-01,\n",
      "         1.99516990e-02,  -4.03762727e-02,   1.42416315e-01,\n",
      "        -3.94770955e-02,   1.16436806e-02,  -6.46860044e-03,\n",
      "         3.78017625e-02,  -3.43759352e-02,  -8.09623744e-02,\n",
      "        -2.13118628e-02,  -3.60669416e-02,   1.19060055e-02,\n",
      "        -5.35170600e-03,  -2.37728832e-02,  -7.40376472e-03,\n",
      "        -6.55078235e-02,  -7.02856800e-02,   2.04886212e-01,\n",
      "         3.62412690e-02,  -4.38236597e-02,  -4.38236597e-02,\n",
      "         6.17981314e-03,   1.02711575e-01,  -3.01427365e-03,\n",
      "        -5.52198652e-03,   4.17831406e-07,   4.17831406e-07,\n",
      "         4.69714390e-02,  -2.47253482e-04,  -5.58068776e-03,\n",
      "        -9.31221115e-02,  -4.66395653e-02,  -3.52354062e-03,\n",
      "        -4.28887166e-02,  -1.25306551e-02,  -1.16989447e-02,\n",
      "        -7.85558327e-02,   1.43975978e-01,   1.43975978e-01,\n",
      "         1.57091336e-01,   1.66531976e-02,   3.39756969e-02,\n",
      "        -6.41742562e-03,  -9.22418386e-03,  -3.67303756e-02,\n",
      "         5.04493885e-02,   1.41932430e-01,   6.72728526e-02,\n",
      "        -1.22698352e-02,  -5.18838851e-03,  -2.66782030e-02,\n",
      "        -2.13118628e-02,  -1.04599596e-01,   3.27426312e-02,\n",
      "         6.44116922e-02,  -4.41753177e-02,   5.49648558e-02,\n",
      "        -8.94650935e-03,   4.17831406e-07,  -1.02624123e-01,\n",
      "        -8.43835951e-02,  -1.38225718e-02,  -1.45352112e-02,\n",
      "         4.17831406e-07,  -2.31653891e-02,  -2.46298807e-02,\n",
      "         6.09837998e-02,   3.38750620e-02,  -1.59793292e-02,\n",
      "        -2.49268600e-03,   5.60798136e-01,  -2.05323235e-02,\n",
      "        -1.47957361e-02,   4.17831406e-07,   2.59179220e-02,\n",
      "         1.24897931e-01,   1.58773905e-02,  -5.23686173e-03,\n",
      "        -1.49518724e-01,   2.52943879e-01,  -2.31653891e-02,\n",
      "         7.69764145e-02,  -1.15526261e-02,   5.17560331e-04,\n",
      "         5.47038290e-02,   2.46705944e-01,  -5.92247493e-03,\n",
      "        -8.32288026e-02,  -2.04184776e-01,  -2.09237981e-02,\n",
      "         1.04513838e-01,  -3.42785028e-03,   1.77786924e-01,\n",
      "        -1.80735690e-01,   4.17831406e-07,  -8.96462786e-03,\n",
      "        -6.55213702e-02,  -1.05092219e-01,  -7.96274232e-02,\n",
      "        -2.22349113e-02,   7.21876688e-02,  -1.57518699e-02,\n",
      "         1.53651100e-04,  -8.35260850e-03,  -6.33326670e-03,\n",
      "         4.17831406e-07,  -4.89996049e-02,  -1.12347754e-02,\n",
      "        -7.08916941e-03,   1.88562804e-02,   1.79087374e-02,\n",
      "        -1.62691729e-01,   4.17831406e-07,  -9.13271888e-03,\n",
      "        -4.12605604e-02,  -2.47543938e-02,   6.44948780e-02,\n",
      "         7.12277223e-02,  -1.77160892e-02,   4.17831406e-07,\n",
      "         4.17831406e-07,  -6.33732324e-03,   4.17831406e-07,\n",
      "        -2.36578311e-02,   6.39290779e-02,  -2.13102694e-03,\n",
      "        -5.37900255e-02,  -9.88035880e-02,  -2.15893529e-02,\n",
      "         2.75763119e-02,   4.17831406e-07,  -2.05326063e-02,\n",
      "        -1.44435597e-02,   1.62599660e-02,  -2.84145304e-02,\n",
      "         6.99153403e-03,  -2.35224857e-02,   9.19650087e-02,\n",
      "        -3.87631750e-03,   4.17831406e-07,   6.68191040e-02,\n",
      "         2.04826106e-01,  -4.43619019e-02,  -1.04125448e-02])\n",
      "***********************************\n",
      "top_positive_w:  [ 80  84 130 135 351]\n",
      "top_negative_w:  [ 21  10   2  88 114]\n",
      "top_positive_weights_words:  ['tropical', 'acidity', 'lime', 'crisp', 'citrus']\n",
      "top_negative_weights_words:  ['tannins', 'black', 'dark', 'cherry', 'blackberry']\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(linear)\n",
    "\n",
    "f = linear.LinearClassifier({'lossFunction': linear.LogisticLoss(), 'lambda': 1, 'numIter': 100, 'stepSize': 0.5})\n",
    "runClassifier.trainTestSet(f, datasets.WineDataBinary)\n",
    "print(f)\n",
    "\n",
    "weights = f.getRepresentation()\n",
    "print(\"***********************************\")\n",
    "#print(weights)\n",
    "\n",
    "sorted_weights = argsort(weights)\n",
    "#print(\"sorted_weights.shape: \",sorted_weights.shape)\n",
    "top_positive_w = sorted_weights[-5:]\n",
    "print(\"top_positive_w: \",top_positive_w)\n",
    "#print(\"top_positive_w shape: \",top_positive_w.shape)\n",
    "\n",
    "top_negative_w = sorted_weights[:5]\n",
    "print(\"top_negative_w: \",top_negative_w)\n",
    "#print(\"top_negative_w shape: \",top_negative_w.shape)\n",
    "\n",
    "top_positive_weights_words = [datasets.WineDataBinary.words[i] for i in top_positive_w]\n",
    "print(\"top_positive_weights_words: \",top_positive_weights_words)\n",
    "\n",
    "top_negative_weights_words = [datasets.WineDataBinary.words[i] for i in top_negative_w]\n",
    "print(\"top_negative_weights_words: \",top_negative_weights_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Classification with Many Classes *[0% -- up to 15% extra credit]*\n",
    "\n",
    "Finally, we'll do multiclass classification using Scikit-learn functionality. You can find the documentation here: http://scikit-learn.org/stable/modules/multiclass.html.\n",
    "\n",
    "Quiz bowl is a game in which two teams compete head-to-head to answer questions from different areas of knowledge. It lets players interrupt the reading of a question when they know the answer. The goal here is to see how well a classifier performs in predicting the `Answer` of a question when a different portion of the question is revealed.\n",
    "\n",
    "Here's an example question from the development data:\n",
    "\n",
    "    206824,dev,History,Alan Turing,\"This man and Donald Bayley created a secure voice communications machine called \"\"Delilah\"\". ||| The Chinese Room Experiment was developed by John Searle in response to one of this man's namesake tests. ||| He showed that the halting problem was undecidable. ||| He devised a bombe with Gordon Welchman that found the settings of an Enigma machine. ||| One of this man's eponymous machines which can perform any computing task is his namesake \"\"complete.\"\" Name this man, whose eponymous test is used to determine if a machine can exhibit behavior indistinguishable from that of a human.\" \n",
    "\n",
    "The more of the question you get, the easier the problem becomes.\n",
    "\n",
    "The default code below just runs OVA and AVA on top of a linear SVM (it might take a few seconds):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Quizbowl dataset..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total labels: 2370\n",
      "unique features: 8416\n",
      "total training examples: 8845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading QuizbowlSmall dataset..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total labels: 31\n",
      "unique features: 8416\n",
      "total training examples: 361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading QuizbowlHard dataset..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total labels: 2370\n",
      "unique features: 4132\n",
      "total training examples: 8845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading QuizbowlHardSmall dataset..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total labels: 31\n",
      "unique features: 4132\n",
      "total training examples: 361\n",
      "\n",
      "\n",
      "RUNNING ON EASY DATA\n",
      "\n",
      "training ova\n",
      "predicting ova\n",
      "error = 0.2934131736526946\n",
      "training ava\n",
      "predicting ava\n",
      "error = 0.218562874251497\n",
      "\n",
      "\n",
      "RUNNING ON HARD DATA\n",
      "\n",
      "training ova\n",
      "predicting ova\n",
      "error = 0.5958083832335329\n",
      "training ava\n",
      "predicting ava\n",
      "error = 0.5538922155688623\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from numpy import *\n",
    "import datasets\n",
    "import importlib\n",
    "\n",
    "importlib.reload(datasets)\n",
    "\n",
    "if not datasets.Quizbowl.loaded:\n",
    "    datasets.loadQuizbowl()\n",
    "\n",
    "print('\\n\\nRUNNING ON EASY DATA\\n')\n",
    "    \n",
    "print('training ova')\n",
    "X = datasets.QuizbowlSmall.X\n",
    "Y = datasets.QuizbowlSmall.Y\n",
    "ova = OneVsOneClassifier(LinearSVC(random_state=0)).fit(X, Y)\n",
    "print('predicting ova')\n",
    "ovaDevPred = ova.predict(datasets.QuizbowlSmall.Xde)\n",
    "print('error = {0}'.format(mean(ovaDevPred != datasets.QuizbowlSmall.Yde)))\n",
    "\n",
    "print('training ava')\n",
    "ava = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X, Y)\n",
    "print('predicting ava')\n",
    "avaDevPred = ava.predict(datasets.QuizbowlSmall.Xde)\n",
    "print('error = {0}'.format(mean(avaDevPred != datasets.QuizbowlSmall.Yde)))\n",
    "\n",
    "print('\\n\\nRUNNING ON HARD DATA\\n')\n",
    "    \n",
    "print('training ova')\n",
    "X = datasets.QuizbowlHardSmall.X\n",
    "Y = datasets.QuizbowlHardSmall.Y\n",
    "ova = OneVsOneClassifier(LinearSVC(random_state=0)).fit(X, Y)\n",
    "print('predicting ova')\n",
    "ovaDevPred = ova.predict(datasets.QuizbowlHardSmall.Xde)\n",
    "print('error = {0}'.format(mean(ovaDevPred != datasets.QuizbowlHardSmall.Yde)))\n",
    "\n",
    "print('training ava')\n",
    "ava = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X, Y)\n",
    "print('predicting ava')\n",
    "avaDevPred = ava.predict(datasets.QuizbowlHardSmall.Xde)\n",
    "print('error = {0}'.format(mean(avaDevPred != datasets.QuizbowlHardSmall.Yde)))\n",
    "\n",
    "savetxt('predictions.txt', avaDevPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Quizbowl dataset..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total labels: 2370\n",
      "unique features: 8416\n",
      "total training examples: 8845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading QuizbowlSmall dataset..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total labels: 31\n",
      "unique features: 8416\n",
      "total training examples: 361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading QuizbowlHard dataset..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total labels: 2370\n",
      "unique features: 4132\n",
      "total training examples: 8845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading QuizbowlHardSmall dataset..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total labels: 31\n",
      "unique features: 4132\n",
      "total training examples: 361\n",
      "\n",
      "\n",
      "RUNNING ON EASY DATA\n",
      "\n",
      "training ova using classifier: 0\n",
      "predicting ova using classifier: 0\n",
      "error = 0.47604790419161674\n",
      "training ava using classifier: 0\n",
      "predicting ava using classifier: 0\n",
      "error = 0.5\n",
      "training ova using classifier: 1\n",
      "predicting ova using classifier: 1\n",
      "error = 0.32934131736526945\n",
      "training ava using classifier: 1\n",
      "predicting ava using classifier: 1\n",
      "error = 0.23652694610778444\n",
      "training ova using classifier: 2\n",
      "predicting ova using classifier: 2\n",
      "error = 0.9670658682634731\n",
      "training ava using classifier: 2\n",
      "predicting ava using classifier: 2\n",
      "error = 0.9610778443113772\n",
      "training ova using classifier: 3\n",
      "predicting ova using classifier: 3\n",
      "error = 0.4341317365269461\n",
      "training ava using classifier: 3\n",
      "predicting ava using classifier: 3\n",
      "error = 0.3263473053892216\n",
      "training ova using classifier: 4\n",
      "predicting ova using classifier: 4\n",
      "error = 0.4221556886227545\n",
      "training ava using classifier: 4\n",
      "predicting ava using classifier: 4\n",
      "error = 0.36826347305389223\n",
      "training ova using classifier: 5\n",
      "predicting ova using classifier: 5\n",
      "error = 0.8293413173652695\n",
      "training ava using classifier: 5\n",
      "predicting ava using classifier: 5\n",
      "error = 0.8862275449101796\n",
      "training ova using classifier: 6\n",
      "predicting ova using classifier: 6\n",
      "error = 0.2874251497005988\n",
      "training ava using classifier: 6\n",
      "predicting ava using classifier: 6\n",
      "error = 0.20059880239520958\n",
      "training ova using classifier: 7\n",
      "predicting ova using classifier: 7\n",
      "error = 0.3473053892215569\n",
      "training ava using classifier: 7\n",
      "predicting ava using classifier: 7\n",
      "error = 0.3203592814371258\n",
      "training ova using classifier: 8\n",
      "predicting ova using classifier: 8\n",
      "error = 0.281437125748503\n",
      "training ava using classifier: 8\n",
      "predicting ava using classifier: 8\n",
      "error = 0.8592814371257484\n",
      "training ova using classifier: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting ova using classifier: 9\n",
      "error = 0.9311377245508982\n",
      "training ava using classifier: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\mayur\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting ava using classifier: 9\n",
      "error = 0.9491017964071856\n"
     ]
    }
   ],
   "source": [
    "#Testing error with different scikit-learn binary classifiers for both OVA and AVA on easy dataset.\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from numpy import *\n",
    "import datasets\n",
    "import importlib\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "importlib.reload(datasets)\n",
    "\n",
    "if not datasets.Quizbowl.loaded:\n",
    "    datasets.loadQuizbowl()\n",
    "\n",
    "print('\\n\\nRUNNING ON EASY DATA\\n')\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=10),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]       \n",
    "\n",
    "X = datasets.QuizbowlSmall.X\n",
    "Y = datasets.QuizbowlSmall.Y\n",
    "\n",
    "for i, cls in enumerate(classifiers):\n",
    "\n",
    "    print('training ova using classifier:',i)\n",
    "    ova = OneVsOneClassifier(cls).fit(X, Y)\n",
    "    print('predicting ova using classifier:',i)\n",
    "    ovaDevPred = ova.predict(datasets.QuizbowlSmall.Xde)\n",
    "    print('error = {0}'.format(mean(ovaDevPred != datasets.QuizbowlSmall.Yde)))\n",
    "\n",
    "    print('training ava using classifier:',i)\n",
    "    ava = OneVsRestClassifier(cls).fit(X, Y)\n",
    "    print('predicting ava using classifier:',i)\n",
    "    avaDevPred = ava.predict(datasets.QuizbowlSmall.Xde)\n",
    "    print('error = {0}'.format(mean(avaDevPred != datasets.QuizbowlSmall.Yde)))\n",
    "\n",
    "#savetxt('predictions.txt', avaDevPred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Quizbowl dataset..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total labels: 2370\n",
      "unique features: 8416\n",
      "total training examples: 8845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading QuizbowlSmall dataset..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total labels: 31\n",
      "unique features: 8416\n",
      "total training examples: 361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading QuizbowlHard dataset..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total labels: 2370\n",
      "unique features: 4132\n",
      "total training examples: 8845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading QuizbowlHardSmall dataset..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total labels: 31\n",
      "unique features: 4132\n",
      "total training examples: 361\n",
      "\n",
      "\n",
      "RUNNING ON SMALL DATA\n",
      "\n",
      "training ava\n",
      "predicting ava\n",
      "error = 0.19760479041916168\n"
     ]
    }
   ],
   "source": [
    "# Using MLPClassifier in OneVsRestClassifier for training on dev data which gives least error \n",
    "# as compared to others as found in above testing.\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from numpy import *\n",
    "import datasets\n",
    "import importlib\n",
    "importlib.reload(datasets)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "if not datasets.Quizbowl.loaded:\n",
    "    datasets.loadQuizbowl()\n",
    "\n",
    "print('\\n\\nRUNNING ON SMALL DATA\\n')\n",
    "    \n",
    "print('training ava')\n",
    "X = datasets.QuizbowlSmall.X\n",
    "Y = datasets.QuizbowlSmall.Y\n",
    "ava = OneVsRestClassifier(MLPClassifier(alpha=1)).fit(X, Y)\n",
    "print('predicting ava')\n",
    "avaDevPred = ava.predict(datasets.QuizbowlSmall.Xde)\n",
    "print('error = {0}'.format(mean(avaDevPred != datasets.QuizbowlSmall.Yde)))\n",
    "\n",
    "savetxt('predictions.txt', avaDevPred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run the code above, you should see some statistics of the loaded datasets and the following error rates on two of the datasets `QuizbowlSmall` and `QuizbowlHardSmall` using OVA and AVA:\n",
    "\n",
    "```\n",
    "RUNNING ON EASY DATA\n",
    "\n",
    "training ova\n",
    "predicting ova\n",
    "error = 0.293413\n",
    "training ava\n",
    "predicting ava\n",
    "error = 0.218563\n",
    "\n",
    "\n",
    "RUNNING ON HARD DATA\n",
    "\n",
    "training ova\n",
    "predicting ova\n",
    "error = 0.595808\n",
    "training ava\n",
    "predicting ava\n",
    "error = 0.553892\n",
    "```\n",
    "\n",
    "This is running on a shrunken version of the data (that only contains answers that occur at least 20 times in the data).\n",
    "\n",
    "The first (\"easy\") version is when you get to see the entire question. The second (\"hard\") version is when you only get to use the first two sentences. It's clearly significantly harder to answer!\n",
    "\n",
    "Your task is to achieve the lowest possible error on the development set for `QuizbowlSmall` and `QuizbowlHardSmall`. You will get 5% extra credit for getting lower error (by at least absolute 1%) on *either* dataset than the errors presented above (21.86% for `QuizbowlSmall` and 55.39% for `QuizbowlHardSmall`). \n",
    "\n",
    "You're free to use the training data in any way you want, but you must include your code in `quizbowl.py`, submit your predictions file(s), and a writeup here that says what you did, in order to receive the extra credit. The script `quizbowl.py` includes a command in the last line that saves predictions to a text file `predictions.txt`. You need to edit this line to rename the file to either `predictionsQuizbowlSmall.txt` or `predictionsQuizbowlHardSmall.txt` dependent on the dataset: that's what you upload for the EC. \n",
    "\n",
    "## WU-EC2 (5%):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[YOUR WU-EC2 WRITEUP HERE] :\n",
    "\n",
    "To reduce the error for the above quizbowl problem, I tried with different scikit learn classifiers mentioned below as binary classifier for OVA and AVA. I used these classifiers as list, iterated it one by one and used is as binary classifier and noted their errors. With this experiment, I found that MLPClassifier does pretty well on easy data and I got the error reduced by more than 1 % with this classifier.\n",
    "\n",
    "classifiers used to check error = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=10),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()] \n",
    "    \n",
    "Best classifier found with OneVsRestClassifier: MLPClassifier\n",
    "\n",
    "Below is the error that I got when used MLPClassifier in OneVsRestClassifier\n",
    "\n",
    "RUNNING ON SMALL DATA\n",
    "\n",
    "training ava\n",
    "predicting ava\n",
    "error = 0.19760479041916168\n",
    "\n",
    "Error with above model is less than 1% as compared to error found with LinearSVC classifier used earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you can get extra credit for providing the lowest-error solution on the full versions of the easy and hard problems, `Quizbowl` and `QuizbowlHard` in comparison to your classmates' solutions. There will be two separate (hidden) leaderboards for each of these two datasets. You will receive 5% if your solution is the best for the respective dataset (first place), 3% for second place and 1% for third. We will reveal the top three scores for each dataset after the submission period is over, and you are welcome to compete in both. Note that this problem is much harder due to the larger number of class labels. A simple majority label classifier has an error of 99.89%.\n",
    "\n",
    "You're free to use the training data in any way you want, but you must include your code in `quizbowl.py`, submit your predictions file(s) (`predictionsQuizbowl.txt` and/or `predictionsQuizbowlHard.txt`), and a writeup here that says what you did, in order to receive the extra credit.\n",
    "\n",
    "<img src=\"data/kitten.jpeg\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br>\n",
    "## WU-EC3 (up to 10%):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[YOUR WU-EC3 WRITEUP HERE] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
